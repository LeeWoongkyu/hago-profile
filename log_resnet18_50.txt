Script started on 2021-03-15 17:56:32+0900
(base) ]0;woongkyu@watson: ~/hago/tvm/tests/python/nightly/quantizationwoongkyu@watson:~/hago/tvm/tests/python/nightly/quantization$ conda activaet t[K[K[K[Kte tvm-build
(tvm-build) ]0;woongkyu@watson: ~/hago/tvm/tests/python/nightly/quantizationwoongkyu@watson:~/hago/tvm/tests/python/nightly/quantization$ python3 learning_based_quantize.py 
[17:56:42] src/io/iter_image_recordio_2.cc:178: ImageRecordIOParser2: /home/woongkyu/imagenet/val.rec, use 4 threads for decoding..
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
DEBUG:root:original
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %4 = nn.conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %5 = add(%4, meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %6 = nn.relu(%5) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %7 = nn.conv2d(%6, meta[relay.Constant][4] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %8 = add(%7, meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %9 = add(%3, %8) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.relu(%9) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.conv2d(%10, meta[relay.Constant][6] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %12 = add(%11, meta[relay.Constant][7] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %13 = nn.relu(%12) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, meta[relay.Constant][8] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %15 = add(%14, meta[relay.Constant][9] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %16 = add(%10, %15) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %17 = nn.relu(%16) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %18 = nn.conv2d(%17, meta[relay.Constant][10] /* ty=Tensor[(128, 64, 1, 1), float32] */ /* ty=Tensor[(128, 64, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %19 = add(%18, meta[relay.Constant][11] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %20 = nn.conv2d(%17, meta[relay.Constant][12] /* ty=Tensor[(128, 64, 3, 3), float32] */ /* ty=Tensor[(128, 64, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %21 = add(%20, meta[relay.Constant][13] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %22 = nn.relu(%21) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %23 = nn.conv2d(%22, meta[relay.Constant][14] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %24 = add(%23, meta[relay.Constant][15] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %25 = add(%19, %24) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %26 = nn.relu(%25) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %27 = nn.conv2d(%26, meta[relay.Constant][16] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %28 = add(%27, meta[relay.Constant][17] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %29 = nn.relu(%28) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %30 = nn.conv2d(%29, meta[relay.Constant][18] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %31 = add(%30, meta[relay.Constant][19] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %32 = add(%26, %31) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%33, meta[relay.Constant][20] /* ty=Tensor[(256, 128, 1, 1), float32] */ /* ty=Tensor[(256, 128, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %35 = add(%34, meta[relay.Constant][21] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %36 = nn.conv2d(%33, meta[relay.Constant][22] /* ty=Tensor[(256, 128, 3, 3), float32] */ /* ty=Tensor[(256, 128, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %37 = add(%36, meta[relay.Constant][23] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %38 = nn.relu(%37) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %39 = nn.conv2d(%38, meta[relay.Constant][24] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %40 = add(%39, meta[relay.Constant][25] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %41 = add(%35, %40) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %42 = nn.relu(%41) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %43 = nn.conv2d(%42, meta[relay.Constant][26] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %44 = add(%43, meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %45 = nn.relu(%44) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %46 = nn.conv2d(%45, meta[relay.Constant][28] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %47 = add(%46, meta[relay.Constant][29] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %48 = add(%42, %47) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %49 = nn.relu(%48) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %50 = nn.conv2d(%49, meta[relay.Constant][30] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %51 = add(%50, meta[relay.Constant][31] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %52 = nn.conv2d(%49, meta[relay.Constant][32] /* ty=Tensor[(512, 256, 3, 3), float32] */ /* ty=Tensor[(512, 256, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %53 = add(%52, meta[relay.Constant][33] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %54 = nn.relu(%53) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %55 = nn.conv2d(%54, meta[relay.Constant][34] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %56 = add(%55, meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %57 = add(%51, %56) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %59 = nn.conv2d(%58, meta[relay.Constant][36] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %60 = add(%59, meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %61 = nn.relu(%60) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %62 = nn.conv2d(%61, meta[relay.Constant][38] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %63 = add(%62, meta[relay.Constant][39] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %64 = add(%58, %63) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %65 = nn.relu(%64) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %66 = nn.global_avg_pool2d(%65) /* ty=Tensor[(32, 512, 1, 1), float32] */;
  %67 = nn.batch_flatten(%66) /* ty=Tensor[(32, 512), float32] */;
  %68 = nn.dense(%67, meta[relay.Constant][40] /* ty=Tensor[(1000, 512), float32] */ /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%68, meta[relay.Constant][41] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:autotvm:Finish loading 688 records
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 3, 224, 224), 'float32'), ('TENSOR', (64, 3, 7, 7), 'float32'), (2, 2), (3, 3, 3, 3), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 64, 56, 56), 'float32'), ('TENSOR', (64, 64, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 64, 56, 56), 'float32'), ('TENSOR', (128, 64, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 128, 28, 28), 'float32'), ('TENSOR', (128, 128, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 128, 28, 28), 'float32'), ('TENSOR', (256, 128, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 256, 14, 14), 'float32'), ('TENSOR', (256, 256, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 256, 14, 14), 'float32'), ('TENSOR', (512, 256, 3, 3), 'float32'), (2, 2), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 512, 7, 7), 'float32'), ('TENSOR', (512, 512, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('dense_tensorcore.cuda', ('TENSOR', (32, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.
INFO:compile_engine:Use implementation dense_tensorcore.cuda for op nn.dense
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.batch_flatten
INFO:compile_engine:Use implementation adaptive_pool.cuda for op nn.global_avg_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
learning_based_quantize.py:100: DeprecationWarning: legacy graph runtime behaviour of producing json / lib / params will be removed in the next release 
  graph, lib, params = relay.build(mod, target)
INFO:root:[3200 samples] validation: acc-top1=0.710625 acc-top5=0.910937
INFO:root:[6400 samples] validation: acc-top1=0.710938 acc-top5=0.905625
INFO:root:[9600 samples] validation: acc-top1=0.711771 acc-top5=0.903333
INFO:root:[12800 samples] validation: acc-top1=0.709063 acc-top5=0.899687
INFO:root:[16000 samples] validation: acc-top1=0.709125 acc-top5=0.900000
INFO:root:[19200 samples] validation: acc-top1=0.708333 acc-top5=0.899167
INFO:root:[22400 samples] validation: acc-top1=0.708750 acc-top5=0.898705
INFO:root:[25600 samples] validation: acc-top1=0.708633 acc-top5=0.897852
INFO:root:[28800 samples] validation: acc-top1=0.708472 acc-top5=0.897813
INFO:root:[32000 samples] validation: acc-top1=0.707969 acc-top5=0.897375
INFO:root:[35200 samples] validation: acc-top1=0.708608 acc-top5=0.897557
INFO:root:[38400 samples] validation: acc-top1=0.708594 acc-top5=0.897995
INFO:root:[41600 samples] validation: acc-top1=0.707476 acc-top5=0.898077
INFO:root:[44800 samples] validation: acc-top1=0.706964 acc-top5=0.898170
INFO:root:[48000 samples] validation: acc-top1=0.707750 acc-top5=0.898458
INFO:root:[final] validation: acc-top1=0.707750 acc-top5=0.898458
Final accuracy fp32 0.70775
DEBUG:root:original
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %4 = nn.conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %5 = add(%4, meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %6 = nn.relu(%5) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %7 = nn.conv2d(%6, meta[relay.Constant][4] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %8 = add(%7, meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %9 = add(%3, %8) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.relu(%9) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.conv2d(%10, meta[relay.Constant][6] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %12 = add(%11, meta[relay.Constant][7] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %13 = nn.relu(%12) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, meta[relay.Constant][8] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %15 = add(%14, meta[relay.Constant][9] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %16 = add(%10, %15) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %17 = nn.relu(%16) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %18 = nn.conv2d(%17, meta[relay.Constant][10] /* ty=Tensor[(128, 64, 1, 1), float32] */ /* ty=Tensor[(128, 64, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %19 = add(%18, meta[relay.Constant][11] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %20 = nn.conv2d(%17, meta[relay.Constant][12] /* ty=Tensor[(128, 64, 3, 3), float32] */ /* ty=Tensor[(128, 64, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %21 = add(%20, meta[relay.Constant][13] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %22 = nn.relu(%21) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %23 = nn.conv2d(%22, meta[relay.Constant][14] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %24 = add(%23, meta[relay.Constant][15] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %25 = add(%19, %24) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %26 = nn.relu(%25) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %27 = nn.conv2d(%26, meta[relay.Constant][16] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %28 = add(%27, meta[relay.Constant][17] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %29 = nn.relu(%28) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %30 = nn.conv2d(%29, meta[relay.Constant][18] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %31 = add(%30, meta[relay.Constant][19] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %32 = add(%26, %31) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%33, meta[relay.Constant][20] /* ty=Tensor[(256, 128, 1, 1), float32] */ /* ty=Tensor[(256, 128, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %35 = add(%34, meta[relay.Constant][21] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %36 = nn.conv2d(%33, meta[relay.Constant][22] /* ty=Tensor[(256, 128, 3, 3), float32] */ /* ty=Tensor[(256, 128, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %37 = add(%36, meta[relay.Constant][23] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %38 = nn.relu(%37) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %39 = nn.conv2d(%38, meta[relay.Constant][24] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %40 = add(%39, meta[relay.Constant][25] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %41 = add(%35, %40) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %42 = nn.relu(%41) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %43 = nn.conv2d(%42, meta[relay.Constant][26] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %44 = add(%43, meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %45 = nn.relu(%44) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %46 = nn.conv2d(%45, meta[relay.Constant][28] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %47 = add(%46, meta[relay.Constant][29] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %48 = add(%42, %47) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %49 = nn.relu(%48) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %50 = nn.conv2d(%49, meta[relay.Constant][30] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %51 = add(%50, meta[relay.Constant][31] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %52 = nn.conv2d(%49, meta[relay.Constant][32] /* ty=Tensor[(512, 256, 3, 3), float32] */ /* ty=Tensor[(512, 256, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %53 = add(%52, meta[relay.Constant][33] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %54 = nn.relu(%53) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %55 = nn.conv2d(%54, meta[relay.Constant][34] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %56 = add(%55, meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %57 = add(%51, %56) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %59 = nn.conv2d(%58, meta[relay.Constant][36] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %60 = add(%59, meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %61 = nn.relu(%60) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %62 = nn.conv2d(%61, meta[relay.Constant][38] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %63 = add(%62, meta[relay.Constant][39] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %64 = add(%58, %63) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %65 = nn.relu(%64) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %66 = nn.global_avg_pool2d(%65) /* ty=Tensor[(32, 512, 1, 1), float32] */;
  %67 = nn.batch_flatten(%66) /* ty=Tensor[(32, 512), float32] */;
  %68 = nn.dense(%67, meta[relay.Constant][40] /* ty=Tensor[(1000, 512), float32] */ /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%68, meta[relay.Constant][41] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:root:current quantize config
DEBUG:root:<tvm.hago.base.QConfig object at 0x7fa89e7e3b50>
data
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
nn.global_avg_pool2d
nn.batch_flatten
constant
nn.dense
constant
add
data -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.max_pool2d -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.global_avg_pool2d
nn.global_avg_pool2d -> nn.batch_flatten
nn.batch_flatten -> nn.dense
constant -> nn.dense
nn.dense -> add
constant -> add
analyzed condition
node_conds: [False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, False, False, False, False, False]
edge_conds: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]
bit limit
[8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32]
--------
nn.conv2d[%2]: [32]
  data[%0] -> nn.conv2d[%2] : 8
  constant[%1] -> nn.conv2d[%2] : 8
--------
add[%4]: [32]
  nn.conv2d[%2] -> add[%4] : 32
  constant[%3] -> add[%4] : 32
--------
nn.relu[%5]: [32]
  add[%4] -> nn.relu[%5] : 32
--------
nn.max_pool2d[%6]: [8, 32]
  nn.relu[%5] -> nn.max_pool2d[%6] : 32
--------
nn.conv2d[%8]: [32]
  nn.max_pool2d[%6] -> nn.conv2d[%8] : 8
  constant[%7] -> nn.conv2d[%8] : 8
--------
add[%10]: [32]
  nn.conv2d[%8] -> add[%10] : 32
  constant[%9] -> add[%10] : 32
--------
nn.relu[%11]: [8]
  add[%10] -> nn.relu[%11] : 32
--------
nn.conv2d[%13]: [32]
  nn.relu[%11] -> nn.conv2d[%13] : 8
  constant[%12] -> nn.conv2d[%13] : 8
--------
add[%15]: [32]
  nn.conv2d[%13] -> add[%15] : 32
  constant[%14] -> add[%15] : 32
--------
add[%16]: [32]
  nn.max_pool2d[%6] -> add[%16] : 32
  add[%15] -> add[%16] : 32
--------
nn.relu[%17]: [8, 32]
  add[%16] -> nn.relu[%17] : 32
--------
nn.conv2d[%19]: [32]
  nn.relu[%17] -> nn.conv2d[%19] : 8
  constant[%18] -> nn.conv2d[%19] : 8
--------
add[%21]: [32]
  nn.conv2d[%19] -> add[%21] : 32
  constant[%20] -> add[%21] : 32
--------
nn.relu[%22]: [8]
  add[%21] -> nn.relu[%22] : 32
--------
nn.conv2d[%24]: [32]
  nn.relu[%22] -> nn.conv2d[%24] : 8
  constant[%23] -> nn.conv2d[%24] : 8
--------
add[%26]: [32]
  nn.conv2d[%24] -> add[%26] : 32
  constant[%25] -> add[%26] : 32
--------
add[%27]: [32]
  nn.relu[%17] -> add[%27] : 32
  add[%26] -> add[%27] : 32
--------
nn.relu[%28]: [8, 8]
  add[%27] -> nn.relu[%28] : 32
--------
nn.conv2d[%30]: [32]
  nn.relu[%28] -> nn.conv2d[%30] : 8
  constant[%29] -> nn.conv2d[%30] : 8
--------
add[%32]: [32]
  nn.conv2d[%30] -> add[%32] : 32
  constant[%31] -> add[%32] : 32
--------
nn.conv2d[%34]: [32]
  nn.relu[%28] -> nn.conv2d[%34] : 8
  constant[%33] -> nn.conv2d[%34] : 8
--------
add[%36]: [32]
  nn.conv2d[%34] -> add[%36] : 32
  constant[%35] -> add[%36] : 32
--------
nn.relu[%37]: [8]
  add[%36] -> nn.relu[%37] : 32
--------
nn.conv2d[%39]: [32]
  nn.relu[%37] -> nn.conv2d[%39] : 8
  constant[%38] -> nn.conv2d[%39] : 8
--------
add[%41]: [32]
  nn.conv2d[%39] -> add[%41] : 32
  constant[%40] -> add[%41] : 32
--------
add[%42]: [32]
  add[%32] -> add[%42] : 32
  add[%41] -> add[%42] : 32
--------
nn.relu[%43]: [8, 32]
  add[%42] -> nn.relu[%43] : 32
--------
nn.conv2d[%45]: [32]
  nn.relu[%43] -> nn.conv2d[%45] : 8
  constant[%44] -> nn.conv2d[%45] : 8
--------
add[%47]: [32]
  nn.conv2d[%45] -> add[%47] : 32
  constant[%46] -> add[%47] : 32
--------
nn.relu[%48]: [8]
  add[%47] -> nn.relu[%48] : 32
--------
nn.conv2d[%50]: [32]
  nn.relu[%48] -> nn.conv2d[%50] : 8
  constant[%49] -> nn.conv2d[%50] : 8
--------
add[%52]: [32]
  nn.conv2d[%50] -> add[%52] : 32
  constant[%51] -> add[%52] : 32
--------
add[%53]: [32]
  nn.relu[%43] -> add[%53] : 32
  add[%52] -> add[%53] : 32
--------
nn.relu[%54]: [8, 8]
  add[%53] -> nn.relu[%54] : 32
--------
nn.conv2d[%56]: [32]
  nn.relu[%54] -> nn.conv2d[%56] : 8
  constant[%55] -> nn.conv2d[%56] : 8
--------
add[%58]: [32]
  nn.conv2d[%56] -> add[%58] : 32
  constant[%57] -> add[%58] : 32
--------
nn.conv2d[%60]: [32]
  nn.relu[%54] -> nn.conv2d[%60] : 8
  constant[%59] -> nn.conv2d[%60] : 8
--------
add[%62]: [32]
  nn.conv2d[%60] -> add[%62] : 32
  constant[%61] -> add[%62] : 32
--------
nn.relu[%63]: [8]
  add[%62] -> nn.relu[%63] : 32
--------
nn.conv2d[%65]: [32]
  nn.relu[%63] -> nn.conv2d[%65] : 8
  constant[%64] -> nn.conv2d[%65] : 8
--------
add[%67]: [32]
  nn.conv2d[%65] -> add[%67] : 32
  constant[%66] -> add[%67] : 32
--------
add[%68]: [32]
  add[%58] -> add[%68] : 32
  add[%67] -> add[%68] : 32
--------
nn.relu[%69]: [8, 32]
  add[%68] -> nn.relu[%69] : 32
--------
nn.conv2d[%71]: [32]
  nn.relu[%69] -> nn.conv2d[%71] : 8
  constant[%70] -> nn.conv2d[%71] : 8
--------
add[%73]: [32]
  nn.conv2d[%71] -> add[%73] : 32
  constant[%72] -> add[%73] : 32
--------
nn.relu[%74]: [8]
  add[%73] -> nn.relu[%74] : 32
--------
nn.conv2d[%76]: [32]
  nn.relu[%74] -> nn.conv2d[%76] : 8
  constant[%75] -> nn.conv2d[%76] : 8
--------
add[%78]: [32]
  nn.conv2d[%76] -> add[%78] : 32
  constant[%77] -> add[%78] : 32
--------
add[%79]: [32]
  nn.relu[%69] -> add[%79] : 32
  add[%78] -> add[%79] : 32
--------
nn.relu[%80]: [8, 8]
  add[%79] -> nn.relu[%80] : 32
--------
nn.conv2d[%82]: [32]
  nn.relu[%80] -> nn.conv2d[%82] : 8
  constant[%81] -> nn.conv2d[%82] : 8
--------
add[%84]: [32]
  nn.conv2d[%82] -> add[%84] : 32
  constant[%83] -> add[%84] : 32
--------
nn.conv2d[%86]: [32]
  nn.relu[%80] -> nn.conv2d[%86] : 8
  constant[%85] -> nn.conv2d[%86] : 8
--------
add[%88]: [32]
  nn.conv2d[%86] -> add[%88] : 32
  constant[%87] -> add[%88] : 32
--------
nn.relu[%89]: [8]
  add[%88] -> nn.relu[%89] : 32
--------
nn.conv2d[%91]: [32]
  nn.relu[%89] -> nn.conv2d[%91] : 8
  constant[%90] -> nn.conv2d[%91] : 8
--------
add[%93]: [32]
  nn.conv2d[%91] -> add[%93] : 32
  constant[%92] -> add[%93] : 32
--------
add[%94]: [32]
  add[%84] -> add[%94] : 32
  add[%93] -> add[%94] : 32
--------
nn.relu[%95]: [8, 32]
  add[%94] -> nn.relu[%95] : 32
--------
nn.conv2d[%97]: [32]
  nn.relu[%95] -> nn.conv2d[%97] : 8
  constant[%96] -> nn.conv2d[%97] : 8
--------
add[%99]: [32]
  nn.conv2d[%97] -> add[%99] : 32
  constant[%98] -> add[%99] : 32
--------
nn.relu[%100]: [8]
  add[%99] -> nn.relu[%100] : 32
--------
nn.conv2d[%102]: [32]
  nn.relu[%100] -> nn.conv2d[%102] : 8
  constant[%101] -> nn.conv2d[%102] : 8
--------
add[%104]: [32]
  nn.conv2d[%102] -> add[%104] : 32
  constant[%103] -> add[%104] : 32
--------
add[%105]: [32]
  nn.relu[%95] -> add[%105] : 32
  add[%104] -> add[%105] : 32
--------
nn.relu[%106]: [None]
  add[%105] -> nn.relu[%106] : 32
--------
nn.global_avg_pool2d[%107]: [None]
  nn.relu[%106] -> nn.global_avg_pool2d[%107] : None
--------
nn.batch_flatten[%108]: [None]
  nn.global_avg_pool2d[%107] -> nn.batch_flatten[%108] : None
--------
nn.dense[%110]: [None]
  nn.batch_flatten[%108] -> nn.dense[%110] : None
  constant[%109] -> nn.dense[%110] : None
--------
add[%112]: []
  nn.dense[%110] -> add[%112] : None
  constant[%111] -> add[%112] : None
ops in graph:
{'nn.conv2d', 'nn.dense', 'nn.relu', 'nn.max_pool2d', 'nn.global_avg_pool2d', 'nn.batch_flatten', 'add'}
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
original acc: 0.765625
data
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
nn.global_avg_pool2d
nn.batch_flatten
constant
nn.dense
constant
add
data -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.max_pool2d -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.global_avg_pool2d
nn.global_avg_pool2d -> nn.batch_flatten
nn.batch_flatten -> nn.dense
constant -> nn.dense
nn.dense -> add
constant -> add
analyzed condition
node_conds: [False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, False, False, False, False, False]
edge_conds: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]
INFO:root:collecting statistics for calibration...
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation dense_tensorcore.cuda for op nn.dense
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:root:statistics collected

select descriptor
---------
nn.conv2d[%2]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%4]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%5]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%6]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%8]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%10]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%11]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%13]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%15]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%16]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%17]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%19]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%21]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%22]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%24]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%26]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%27]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%28]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%30]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%32]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%34]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%36]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%37]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%39]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%41]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%42]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%43]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%45]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%47]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%48]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%50]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%52]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%53]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%54]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%56]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%58]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%60]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%62]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%63]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%65]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%67]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%68]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%69]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%71]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%73]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%74]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%76]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%78]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%79]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%80]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%82]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%84]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%86]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%88]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%89]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%91]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%93]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%94]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%95]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%97]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%99]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%100]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%102]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%104]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%105]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%106]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %4 = nn.conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %5 = add(%4, meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %6 = nn.relu(%5) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %7 = nn.conv2d(%6, meta[relay.Constant][4] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %8 = add(%7, meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %9 = add(%3, %8) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.relu(%9) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.conv2d(%10, meta[relay.Constant][6] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %12 = add(%11, meta[relay.Constant][7] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %13 = nn.relu(%12) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, meta[relay.Constant][8] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %15 = add(%14, meta[relay.Constant][9] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %16 = add(%10, %15) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %17 = nn.relu(%16) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %18 = nn.conv2d(%17, meta[relay.Constant][10] /* ty=Tensor[(128, 64, 1, 1), float32] */ /* ty=Tensor[(128, 64, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %19 = add(%18, meta[relay.Constant][11] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %20 = nn.conv2d(%17, meta[relay.Constant][12] /* ty=Tensor[(128, 64, 3, 3), float32] */ /* ty=Tensor[(128, 64, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %21 = add(%20, meta[relay.Constant][13] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %22 = nn.relu(%21) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %23 = nn.conv2d(%22, meta[relay.Constant][14] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %24 = add(%23, meta[relay.Constant][15] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %25 = add(%19, %24) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %26 = nn.relu(%25) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %27 = nn.conv2d(%26, meta[relay.Constant][16] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %28 = add(%27, meta[relay.Constant][17] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %29 = nn.relu(%28) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %30 = nn.conv2d(%29, meta[relay.Constant][18] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %31 = add(%30, meta[relay.Constant][19] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %32 = add(%26, %31) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%33, meta[relay.Constant][20] /* ty=Tensor[(256, 128, 1, 1), float32] */ /* ty=Tensor[(256, 128, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %35 = add(%34, meta[relay.Constant][21] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %36 = nn.conv2d(%33, meta[relay.Constant][22] /* ty=Tensor[(256, 128, 3, 3), float32] */ /* ty=Tensor[(256, 128, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %37 = add(%36, meta[relay.Constant][23] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %38 = nn.relu(%37) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %39 = nn.conv2d(%38, meta[relay.Constant][24] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %40 = add(%39, meta[relay.Constant][25] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %41 = add(%35, %40) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %42 = nn.relu(%41) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %43 = nn.conv2d(%42, meta[relay.Constant][26] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %44 = add(%43, meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %45 = nn.relu(%44) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %46 = nn.conv2d(%45, meta[relay.Constant][28] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %47 = add(%46, meta[relay.Constant][29] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %48 = add(%42, %47) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %49 = nn.relu(%48) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %50 = nn.conv2d(%49, meta[relay.Constant][30] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %51 = add(%50, meta[relay.Constant][31] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %52 = nn.conv2d(%49, meta[relay.Constant][32] /* ty=Tensor[(512, 256, 3, 3), float32] */ /* ty=Tensor[(512, 256, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %53 = add(%52, meta[relay.Constant][33] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %54 = nn.relu(%53) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %55 = nn.conv2d(%54, meta[relay.Constant][34] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %56 = add(%55, meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %57 = add(%51, %56) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %59 = nn.conv2d(%58, meta[relay.Constant][36] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %60 = add(%59, meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %61 = nn.relu(%60) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %62 = nn.conv2d(%61, meta[relay.Constant][38] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %63 = add(%62, meta[relay.Constant][39] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %64 = add(%58, %63) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %65 = nn.relu(%64) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %66 = nn.global_avg_pool2d(%65) /* ty=Tensor[(32, 512, 1, 1), float32] */;
  %67 = nn.batch_flatten(%66) /* ty=Tensor[(32, 512), float32] */;
  %68 = nn.dense(%67, meta[relay.Constant][40] /* ty=Tensor[(1000, 512), float32] */ /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%68, meta[relay.Constant][41] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
Simulated graph fn (%data: Tensor[(32, 3, 224, 224), float32], %in_scale0: float32, %out_scale0: float32, %clip_min0: float32, %clip_max0: float32, %in_scale1: float32, %out_scale1: float32, %clip_min1: float32, %clip_max1: float32, %in_scale2: float32, %out_scale2: float32, %clip_min2: float32, %clip_max2: float32, %in_scale3: float32, %out_scale3: float32, %clip_min3: float32, %clip_max3: float32, %in_scale4: float32, %out_scale4: float32, %clip_min4: float32, %clip_max4: float32, %in_scale5: float32, %out_scale5: float32, %clip_min5: float32, %clip_max5: float32, %in_scale15: float32, %out_scale15: float32, %clip_min15: float32, %clip_max15: float32, %in_scale6: float32, %out_scale6: float32, %clip_min6: float32, %clip_max6: float32, %in_scale7: float32, %out_scale7: float32, %clip_min7: float32, %clip_max7: float32, %in_scale8: float32, %out_scale8: float32, %clip_min8: float32, %clip_max8: float32, %in_scale9: float32, %out_scale9: float32, %clip_min9: float32, %clip_max9: float32, %in_scale10: float32, %out_scale10: float32, %clip_min10: float32, %clip_max10: float32, %in_scale11: float32, %out_scale11: float32, %clip_min11: float32, %clip_max11: float32, %in_scale12: float32, %out_scale12: float32, %clip_min12: float32, %clip_max12: float32, %in_scale13: float32, %out_scale13: float32, %clip_min13: float32, %clip_max13: float32, %in_scale14: float32, %out_scale14: float32, %clip_min14: float32, %clip_max14: float32, %in_scale16: float32, %out_scale16: float32, %clip_min16: float32, %clip_max16: float32, %in_scale17: float32, %out_scale17: float32, %clip_min17: float32, %clip_max17: float32, %in_scale27: float32, %out_scale27: float32, %clip_min27: float32, %clip_max27: float32, %in_scale18: float32, %out_scale18: float32, %clip_min18: float32, %clip_max18: float32, %in_scale19: float32, %out_scale19: float32, %clip_min19: float32, %clip_max19: float32, %in_scale20: float32, %out_scale20: float32, %clip_min20: float32, %clip_max20: float32, %in_scale21: float32, %out_scale21: float32, %clip_min21: float32, %clip_max21: float32, %in_scale22: float32, %out_scale22: float32, %clip_min22: float32, %clip_max22: float32, %in_scale23: float32, %out_scale23: float32, %clip_min23: float32, %clip_max23: float32, %in_scale24: float32, %out_scale24: float32, %clip_min24: float32, %clip_max24: float32, %in_scale25: float32, %out_scale25: float32, %clip_min25: float32, %clip_max25: float32, %in_scale26: float32, %out_scale26: float32, %clip_min26: float32, %clip_max26: float32, %in_scale28: float32, %out_scale28: float32, %clip_min28: float32, %clip_max28: float32, %in_scale29: float32, %out_scale29: float32, %clip_min29: float32, %clip_max29: float32, %in_scale30: float32, %out_scale30: float32, %clip_min30: float32, %clip_max30: float32, %in_scale31: float32, %out_scale31: float32, %clip_min31: float32, %clip_max31: float32, %in_scale32: float32, %out_scale32: float32, %clip_min32: float32, %clip_max32: float32, %in_scale33: float32, %out_scale33: float32, %clip_min33: float32, %clip_max33: float32, %in_scale43: float32, %out_scale43: float32, %clip_min43: float32, %clip_max43: float32, %in_scale34: float32, %out_scale34: float32, %clip_min34: float32, %clip_max34: float32, %in_scale35: float32, %out_scale35: float32, %clip_min35: float32, %clip_max35: float32, %in_scale36: float32, %out_scale36: float32, %clip_min36: float32, %clip_max36: float32, %in_scale37: float32, %out_scale37: float32, %clip_min37: float32, %clip_max37: float32, %in_scale38: float32, %out_scale38: float32, %clip_min38: float32, %clip_max38: float32, %in_scale39: float32, %out_scale39: float32, %clip_min39: float32, %clip_max39: float32, %in_scale40: float32, %out_scale40: float32, %clip_min40: float32, %clip_max40: float32, %in_scale41: float32, %out_scale41: float32, %clip_min41: float32, %clip_max41: float32, %in_scale42: float32, %out_scale42: float32, %clip_min42: float32, %clip_max42: float32, %in_scale44: float32, %out_scale44: float32, %clip_min44: float32, %clip_max44: float32, %in_scale45: float32, %out_scale45: float32, %clip_min45: float32, %clip_max45: float32, %in_scale55: float32, %out_scale55: float32, %clip_min55: float32, %clip_max55: float32, %in_scale46: float32, %out_scale46: float32, %clip_min46: float32, %clip_max46: float32, %in_scale47: float32, %out_scale47: float32, %clip_min47: float32, %clip_max47: float32, %in_scale48: float32, %out_scale48: float32, %clip_min48: float32, %clip_max48: float32, %in_scale49: float32, %out_scale49: float32, %clip_min49: float32, %clip_max49: float32, %in_scale50: float32, %out_scale50: float32, %clip_min50: float32, %clip_max50: float32, %in_scale51: float32, %out_scale51: float32, %clip_min51: float32, %clip_max51: float32, %in_scale52: float32, %out_scale52: float32, %clip_min52: float32, %clip_max52: float32, %in_scale53: float32, %out_scale53: float32, %clip_min53: float32, %clip_max53: float32, %in_scale54: float32, %out_scale54: float32, %clip_min54: float32, %clip_max54: float32, %in_scale56: float32, %out_scale56: float32, %clip_min56: float32, %clip_max56: float32, %in_scale57: float32, %out_scale57: float32, %clip_min57: float32, %clip_max57: float32, %in_scale58: float32, %out_scale58: float32, %clip_min58: float32, %clip_max58: float32, %in_scale59: float32, %out_scale59: float32, %clip_min59: float32, %clip_max59: float32, %in_scale60: float32, %out_scale60: float32, %clip_min60: float32, %clip_max60: float32, %in_scale61: float32, %out_scale61: float32, %clip_min61: float32, %clip_max61: float32, %in_scale71: float32, %out_scale71: float32, %clip_min71: float32, %clip_max71: float32, %in_scale62: float32, %out_scale62: float32, %clip_min62: float32, %clip_max62: float32, %in_scale63: float32, %out_scale63: float32, %clip_min63: float32, %clip_max63: float32, %in_scale64: float32, %out_scale64: float32, %clip_min64: float32, %clip_max64: float32, %in_scale65: float32, %out_scale65: float32, %clip_min65: float32, %clip_max65: float32, %in_scale66: float32, %out_scale66: float32, %clip_min66: float32, %clip_max66: float32, %in_scale67: float32, %out_scale67: float32, %clip_min67: float32, %clip_max67: float32, %in_scale68: float32, %out_scale68: float32, %clip_min68: float32, %clip_max68: float32, %in_scale69: float32, %out_scale69: float32, %clip_min69: float32, %clip_max69: float32, %in_scale70: float32, %out_scale70: float32, %clip_min70: float32, %clip_max70: float32, %in_scale72: float32, %out_scale72: float32, %clip_min72: float32, %clip_max72: float32, %in_scale73: float32, %out_scale73: float32, %clip_min73: float32, %clip_max73: float32, %in_scale83: float32, %out_scale83: float32, %clip_min83: float32, %clip_max83: float32, %in_scale74: float32, %out_scale74: float32, %clip_min74: float32, %clip_max74: float32, %in_scale75: float32, %out_scale75: float32, %clip_min75: float32, %clip_max75: float32, %in_scale76: float32, %out_scale76: float32, %clip_min76: float32, %clip_max76: float32, %in_scale77: float32, %out_scale77: float32, %clip_min77: float32, %clip_max77: float32, %in_scale78: float32, %out_scale78: float32, %clip_min78: float32, %clip_max78: float32, %in_scale79: float32, %out_scale79: float32, %clip_min79: float32, %clip_max79: float32, %in_scale80: float32, %out_scale80: float32, %clip_min80: float32, %clip_max80: float32, %in_scale81: float32, %out_scale81: float32, %clip_min81: float32, %clip_max81: float32, %in_scale82: float32, %out_scale82: float32, %clip_min82: float32, %clip_max82: float32, %in_scale84: float32, %out_scale84: float32, %clip_min84: float32, %clip_max84: float32, %in_scale85: float32, %out_scale85: float32, %clip_min85: float32, %clip_max85: float32, %in_scale86: float32, %out_scale86: float32, %clip_min86: float32, %clip_max86: float32, %in_scale87: float32, %out_scale87: float32, %clip_min87: float32, %clip_max87: float32, %in_scale88: float32, %out_scale88: float32, %clip_min88: float32, %clip_max88: float32, %in_scale89: float32, %out_scale89: float32, %clip_min89: float32, %clip_max89: float32, %in_scale99: float32, %out_scale99: float32, %clip_min99: float32, %clip_max99: float32, %in_scale90: float32, %out_scale90: float32, %clip_min90: float32, %clip_max90: float32, %in_scale91: float32, %out_scale91: float32, %clip_min91: float32, %clip_max91: float32, %in_scale92: float32, %out_scale92: float32, %clip_min92: float32, %clip_max92: float32, %in_scale93: float32, %out_scale93: float32, %clip_min93: float32, %clip_max93: float32, %in_scale94: float32, %out_scale94: float32, %clip_min94: float32, %clip_max94: float32, %in_scale95: float32, %out_scale95: float32, %clip_min95: float32, %clip_max95: float32, %in_scale96: float32, %out_scale96: float32, %clip_min96: float32, %clip_max96: float32, %in_scale97: float32, %out_scale97: float32, %clip_min97: float32, %clip_max97: float32, %in_scale98: float32, %out_scale98: float32, %clip_min98: float32, %clip_max98: float32, %in_scale100: float32, %out_scale100: float32, %clip_min100: float32, %clip_max100: float32, %in_scale101: float32, %out_scale101: float32, %clip_min101: float32, %clip_max101: float32, %in_scale111: float32, %out_scale111: float32, %clip_min111: float32, %clip_max111: float32, %in_scale102: float32, %out_scale102: float32, %clip_min102: float32, %clip_max102: float32, %in_scale103: float32, %out_scale103: float32, %clip_min103: float32, %clip_max103: float32, %in_scale104: float32, %out_scale104: float32, %clip_min104: float32, %clip_max104: float32, %in_scale105: float32, %out_scale105: float32, %clip_min105: float32, %clip_max105: float32, %in_scale106: float32, %out_scale106: float32, %clip_min106: float32, %clip_max106: float32, %in_scale107: float32, %out_scale107: float32, %clip_min107: float32, %clip_max107: float32, %in_scale108: float32, %out_scale108: float32, %clip_min108: float32, %clip_max108: float32, %in_scale109: float32, %out_scale109: float32, %clip_min109: float32, %clip_max109: float32, %in_scale110: float32, %out_scale110: float32, %clip_min110: float32, %clip_max110: float32, %in_scale112: float32, %out_scale112: float32, %clip_min112: float32, %clip_max112: float32, %in_scale113: float32, %out_scale113: float32, %clip_min113: float32, %clip_max113: float32, %in_scale114: float32, %out_scale114: float32, %clip_min114: float32, %clip_max114: float32, %in_scale115: float32, %out_scale115: float32, %clip_min115: float32, %clip_max115: float32, %in_scale116: float32, %out_scale116: float32, %clip_min116: float32, %clip_max116: float32, %in_scale117: float32, %out_scale117: float32, %clip_min117: float32, %clip_max117: float32, %in_scale118: float32, %out_scale118: float32, %clip_min118: float32, %clip_max118: float32, %in_scale119: float32, %out_scale119: float32, %clip_min119: float32, %clip_max119: float32, %in_scale120: float32, %out_scale120: float32, %clip_min120: float32, %clip_max120: float32) -> Tensor[(32, 1000), float32] {
  %0 = nn.simulated_quantize(%data, %in_scale0, %out_scale0, %clip_min0, %clip_max0, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 3, 224, 224), float32] */;
  %1 = nn.simulated_quantize(meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, %in_scale1, %out_scale1, %clip_min1, %clip_max1, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %2 = nn.conv2d(%0, %1, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.simulated_quantize(%2, %in_scale2, %out_scale2, %clip_min2, %clip_max2, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %4 = nn.simulated_quantize(meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale3, %out_scale3, %clip_min3, %clip_max3, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %5 = add(%3, %4) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %6 = nn.simulated_quantize(%5, %in_scale4, %out_scale4, %clip_min4, %clip_max4, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %8 = nn.simulated_quantize(%7, %in_scale5, %out_scale5, %clip_min5, %clip_max5, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.simulated_quantize(%9, %in_scale15, %out_scale15, %clip_min15, %clip_max15, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.simulated_quantize(%9, %in_scale6, %out_scale6, %clip_min6, %clip_max6, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %12 = nn.simulated_quantize(meta[relay.Constant][2] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, %in_scale7, %out_scale7, %clip_min7, %clip_max7, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %13 = nn.conv2d(%11, %12, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %14 = nn.simulated_quantize(%13, %in_scale8, %out_scale8, %clip_min8, %clip_max8, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %15 = nn.simulated_quantize(meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale9, %out_scale9, %clip_min9, %clip_max9, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %17 = nn.simulated_quantize(%16, %in_scale10, %out_scale10, %clip_min10, %clip_max10, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %18 = nn.relu(%17) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %19 = nn.simulated_quantize(%18, %in_scale11, %out_scale11, %clip_min11, %clip_max11, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %20 = nn.simulated_quantize(meta[relay.Constant][4] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, %in_scale12, %out_scale12, %clip_min12, %clip_max12, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %21 = nn.conv2d(%19, %20, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %22 = nn.simulated_quantize(%21, %in_scale13, %out_scale13, %clip_min13, %clip_max13, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %23 = nn.simulated_quantize(meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale14, %out_scale14, %clip_min14, %clip_max14, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %24 = add(%22, %23) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %25 = nn.simulated_quantize(%24, %in_scale16, %out_scale16, %clip_min16, %clip_max16, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %26 = add(%10, %25) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %27 = nn.simulated_quantize(%26, %in_scale17, %out_scale17, %clip_min17, %clip_max17, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %29 = nn.simulated_quantize(%28, %in_scale27, %out_scale27, %clip_min27, %clip_max27, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %30 = nn.simulated_quantize(%28, %in_scale18, %out_scale18, %clip_min18, %clip_max18, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %31 = nn.simulated_quantize(meta[relay.Constant][6] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, %in_scale19, %out_scale19, %clip_min19, %clip_max19, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %32 = nn.conv2d(%30, %31, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %33 = nn.simulated_quantize(%32, %in_scale20, %out_scale20, %clip_min20, %clip_max20, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %34 = nn.simulated_quantize(meta[relay.Constant][7] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale21, %out_scale21, %clip_min21, %clip_max21, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %36 = nn.simulated_quantize(%35, %in_scale22, %out_scale22, %clip_min22, %clip_max22, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %37 = nn.relu(%36) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %38 = nn.simulated_quantize(%37, %in_scale23, %out_scale23, %clip_min23, %clip_max23, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %39 = nn.simulated_quantize(meta[relay.Constant][8] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, %in_scale24, %out_scale24, %clip_min24, %clip_max24, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %40 = nn.conv2d(%38, %39, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %41 = nn.simulated_quantize(%40, %in_scale25, %out_scale25, %clip_min25, %clip_max25, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %42 = nn.simulated_quantize(meta[relay.Constant][9] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale26, %out_scale26, %clip_min26, %clip_max26, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %43 = add(%41, %42) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %44 = nn.simulated_quantize(%43, %in_scale28, %out_scale28, %clip_min28, %clip_max28, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %45 = add(%29, %44) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %46 = nn.simulated_quantize(%45, %in_scale29, %out_scale29, %clip_min29, %clip_max29, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %47 = nn.relu(%46) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %48 = nn.simulated_quantize(%47, %in_scale30, %out_scale30, %clip_min30, %clip_max30, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %49 = nn.simulated_quantize(meta[relay.Constant][10] /* ty=Tensor[(128, 64, 1, 1), float32] */ /* ty=Tensor[(128, 64, 1, 1), float32] */, %in_scale31, %out_scale31, %clip_min31, %clip_max31, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %50 = nn.conv2d(%48, %49, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %51 = nn.simulated_quantize(%50, %in_scale32, %out_scale32, %clip_min32, %clip_max32, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %52 = nn.simulated_quantize(meta[relay.Constant][11] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale33, %out_scale33, %clip_min33, %clip_max33, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %53 = add(%51, %52) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %54 = nn.simulated_quantize(%53, %in_scale43, %out_scale43, %clip_min43, %clip_max43, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %55 = nn.simulated_quantize(%47, %in_scale34, %out_scale34, %clip_min34, %clip_max34, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %56 = nn.simulated_quantize(meta[relay.Constant][12] /* ty=Tensor[(128, 64, 3, 3), float32] */ /* ty=Tensor[(128, 64, 3, 3), float32] */, %in_scale35, %out_scale35, %clip_min35, %clip_max35, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %57 = nn.conv2d(%55, %56, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %58 = nn.simulated_quantize(%57, %in_scale36, %out_scale36, %clip_min36, %clip_max36, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %59 = nn.simulated_quantize(meta[relay.Constant][13] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale37, %out_scale37, %clip_min37, %clip_max37, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %60 = add(%58, %59) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %61 = nn.simulated_quantize(%60, %in_scale38, %out_scale38, %clip_min38, %clip_max38, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %63 = nn.simulated_quantize(%62, %in_scale39, %out_scale39, %clip_min39, %clip_max39, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %64 = nn.simulated_quantize(meta[relay.Constant][14] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, %in_scale40, %out_scale40, %clip_min40, %clip_max40, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %65 = nn.conv2d(%63, %64, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %66 = nn.simulated_quantize(%65, %in_scale41, %out_scale41, %clip_min41, %clip_max41, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %67 = nn.simulated_quantize(meta[relay.Constant][15] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale42, %out_scale42, %clip_min42, %clip_max42, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %68 = add(%66, %67) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %69 = nn.simulated_quantize(%68, %in_scale44, %out_scale44, %clip_min44, %clip_max44, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %70 = add(%54, %69) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %71 = nn.simulated_quantize(%70, %in_scale45, %out_scale45, %clip_min45, %clip_max45, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %72 = nn.relu(%71) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %73 = nn.simulated_quantize(%72, %in_scale55, %out_scale55, %clip_min55, %clip_max55, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %74 = nn.simulated_quantize(%72, %in_scale46, %out_scale46, %clip_min46, %clip_max46, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %75 = nn.simulated_quantize(meta[relay.Constant][16] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, %in_scale47, %out_scale47, %clip_min47, %clip_max47, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %76 = nn.conv2d(%74, %75, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %77 = nn.simulated_quantize(%76, %in_scale48, %out_scale48, %clip_min48, %clip_max48, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %78 = nn.simulated_quantize(meta[relay.Constant][17] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale49, %out_scale49, %clip_min49, %clip_max49, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %79 = add(%77, %78) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %80 = nn.simulated_quantize(%79, %in_scale50, %out_scale50, %clip_min50, %clip_max50, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %81 = nn.relu(%80) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %82 = nn.simulated_quantize(%81, %in_scale51, %out_scale51, %clip_min51, %clip_max51, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %83 = nn.simulated_quantize(meta[relay.Constant][18] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, %in_scale52, %out_scale52, %clip_min52, %clip_max52, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %84 = nn.conv2d(%82, %83, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %85 = nn.simulated_quantize(%84, %in_scale53, %out_scale53, %clip_min53, %clip_max53, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %86 = nn.simulated_quantize(meta[relay.Constant][19] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale54, %out_scale54, %clip_min54, %clip_max54, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %87 = add(%85, %86) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %88 = nn.simulated_quantize(%87, %in_scale56, %out_scale56, %clip_min56, %clip_max56, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %89 = add(%73, %88) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %90 = nn.simulated_quantize(%89, %in_scale57, %out_scale57, %clip_min57, %clip_max57, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %92 = nn.simulated_quantize(%91, %in_scale58, %out_scale58, %clip_min58, %clip_max58, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %93 = nn.simulated_quantize(meta[relay.Constant][20] /* ty=Tensor[(256, 128, 1, 1), float32] */ /* ty=Tensor[(256, 128, 1, 1), float32] */, %in_scale59, %out_scale59, %clip_min59, %clip_max59, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %94 = nn.conv2d(%92, %93, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %95 = nn.simulated_quantize(%94, %in_scale60, %out_scale60, %clip_min60, %clip_max60, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %96 = nn.simulated_quantize(meta[relay.Constant][21] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale61, %out_scale61, %clip_min61, %clip_max61, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %97 = add(%95, %96) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %98 = nn.simulated_quantize(%97, %in_scale71, %out_scale71, %clip_min71, %clip_max71, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %99 = nn.simulated_quantize(%91, %in_scale62, %out_scale62, %clip_min62, %clip_max62, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %100 = nn.simulated_quantize(meta[relay.Constant][22] /* ty=Tensor[(256, 128, 3, 3), float32] */ /* ty=Tensor[(256, 128, 3, 3), float32] */, %in_scale63, %out_scale63, %clip_min63, %clip_max63, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %101 = nn.conv2d(%99, %100, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %102 = nn.simulated_quantize(%101, %in_scale64, %out_scale64, %clip_min64, %clip_max64, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %103 = nn.simulated_quantize(meta[relay.Constant][23] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale65, %out_scale65, %clip_min65, %clip_max65, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %104 = add(%102, %103) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %105 = nn.simulated_quantize(%104, %in_scale66, %out_scale66, %clip_min66, %clip_max66, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %106 = nn.relu(%105) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %107 = nn.simulated_quantize(%106, %in_scale67, %out_scale67, %clip_min67, %clip_max67, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %108 = nn.simulated_quantize(meta[relay.Constant][24] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, %in_scale68, %out_scale68, %clip_min68, %clip_max68, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %109 = nn.conv2d(%107, %108, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %110 = nn.simulated_quantize(%109, %in_scale69, %out_scale69, %clip_min69, %clip_max69, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %111 = nn.simulated_quantize(meta[relay.Constant][25] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale70, %out_scale70, %clip_min70, %clip_max70, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %112 = add(%110, %111) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %113 = nn.simulated_quantize(%112, %in_scale72, %out_scale72, %clip_min72, %clip_max72, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %114 = add(%98, %113) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %115 = nn.simulated_quantize(%114, %in_scale73, %out_scale73, %clip_min73, %clip_max73, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %116 = nn.relu(%115) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %117 = nn.simulated_quantize(%116, %in_scale83, %out_scale83, %clip_min83, %clip_max83, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %118 = nn.simulated_quantize(%116, %in_scale74, %out_scale74, %clip_min74, %clip_max74, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %119 = nn.simulated_quantize(meta[relay.Constant][26] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, %in_scale75, %out_scale75, %clip_min75, %clip_max75, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %120 = nn.conv2d(%118, %119, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %121 = nn.simulated_quantize(%120, %in_scale76, %out_scale76, %clip_min76, %clip_max76, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %122 = nn.simulated_quantize(meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale77, %out_scale77, %clip_min77, %clip_max77, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %123 = add(%121, %122) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %124 = nn.simulated_quantize(%123, %in_scale78, %out_scale78, %clip_min78, %clip_max78, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %125 = nn.relu(%124) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %126 = nn.simulated_quantize(%125, %in_scale79, %out_scale79, %clip_min79, %clip_max79, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %127 = nn.simulated_quantize(meta[relay.Constant][28] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, %in_scale80, %out_scale80, %clip_min80, %clip_max80, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %129 = nn.simulated_quantize(%128, %in_scale81, %out_scale81, %clip_min81, %clip_max81, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %130 = nn.simulated_quantize(meta[relay.Constant][29] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale82, %out_scale82, %clip_min82, %clip_max82, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %131 = add(%129, %130) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %132 = nn.simulated_quantize(%131, %in_scale84, %out_scale84, %clip_min84, %clip_max84, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %133 = add(%117, %132) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %134 = nn.simulated_quantize(%133, %in_scale85, %out_scale85, %clip_min85, %clip_max85, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %135 = nn.relu(%134) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %136 = nn.simulated_quantize(%135, %in_scale86, %out_scale86, %clip_min86, %clip_max86, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %137 = nn.simulated_quantize(meta[relay.Constant][30] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, %in_scale87, %out_scale87, %clip_min87, %clip_max87, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %138 = nn.conv2d(%136, %137, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %139 = nn.simulated_quantize(%138, %in_scale88, %out_scale88, %clip_min88, %clip_max88, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %140 = nn.simulated_quantize(meta[relay.Constant][31] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale89, %out_scale89, %clip_min89, %clip_max89, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %141 = add(%139, %140) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %142 = nn.simulated_quantize(%141, %in_scale99, %out_scale99, %clip_min99, %clip_max99, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %143 = nn.simulated_quantize(%135, %in_scale90, %out_scale90, %clip_min90, %clip_max90, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %144 = nn.simulated_quantize(meta[relay.Constant][32] /* ty=Tensor[(512, 256, 3, 3), float32] */ /* ty=Tensor[(512, 256, 3, 3), float32] */, %in_scale91, %out_scale91, %clip_min91, %clip_max91, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %145 = nn.conv2d(%143, %144, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %146 = nn.simulated_quantize(%145, %in_scale92, %out_scale92, %clip_min92, %clip_max92, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %147 = nn.simulated_quantize(meta[relay.Constant][33] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale93, %out_scale93, %clip_min93, %clip_max93, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %148 = add(%146, %147) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %149 = nn.simulated_quantize(%148, %in_scale94, %out_scale94, %clip_min94, %clip_max94, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %150 = nn.relu(%149) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %151 = nn.simulated_quantize(%150, %in_scale95, %out_scale95, %clip_min95, %clip_max95, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %152 = nn.simulated_quantize(meta[relay.Constant][34] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, %in_scale96, %out_scale96, %clip_min96, %clip_max96, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %153 = nn.conv2d(%151, %152, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %154 = nn.simulated_quantize(%153, %in_scale97, %out_scale97, %clip_min97, %clip_max97, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %155 = nn.simulated_quantize(meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale98, %out_scale98, %clip_min98, %clip_max98, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %156 = add(%154, %155) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %157 = nn.simulated_quantize(%156, %in_scale100, %out_scale100, %clip_min100, %clip_max100, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %158 = add(%142, %157) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %159 = nn.simulated_quantize(%158, %in_scale101, %out_scale101, %clip_min101, %clip_max101, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %161 = nn.simulated_quantize(%160, %in_scale111, %out_scale111, %clip_min111, %clip_max111, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %162 = nn.simulated_quantize(%160, %in_scale102, %out_scale102, %clip_min102, %clip_max102, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %163 = nn.simulated_quantize(meta[relay.Constant][36] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, %in_scale103, %out_scale103, %clip_min103, %clip_max103, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %164 = nn.conv2d(%162, %163, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %165 = nn.simulated_quantize(%164, %in_scale104, %out_scale104, %clip_min104, %clip_max104, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %166 = nn.simulated_quantize(meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale105, %out_scale105, %clip_min105, %clip_max105, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %167 = add(%165, %166) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %168 = nn.simulated_quantize(%167, %in_scale106, %out_scale106, %clip_min106, %clip_max106, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %169 = nn.relu(%168) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %170 = nn.simulated_quantize(%169, %in_scale107, %out_scale107, %clip_min107, %clip_max107, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %171 = nn.simulated_quantize(meta[relay.Constant][38] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, %in_scale108, %out_scale108, %clip_min108, %clip_max108, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %172 = nn.conv2d(%170, %171, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %173 = nn.simulated_quantize(%172, %in_scale109, %out_scale109, %clip_min109, %clip_max109, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %174 = nn.simulated_quantize(meta[relay.Constant][39] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale110, %out_scale110, %clip_min110, %clip_max110, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %175 = add(%173, %174) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %176 = nn.simulated_quantize(%175, %in_scale112, %out_scale112, %clip_min112, %clip_max112, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %177 = add(%161, %176) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %178 = nn.simulated_quantize(%177, %in_scale113, %out_scale113, %clip_min113, %clip_max113, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %180 = nn.simulated_quantize(%179, %in_scale114, %out_scale114, %clip_min114, %clip_max114, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %181 = nn.global_avg_pool2d(%180) /* ty=Tensor[(32, 512, 1, 1), float32] */;
  %182 = nn.simulated_quantize(%181, %in_scale115, %out_scale115, %clip_min115, %clip_max115, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 512, 1, 1), float32] */;
  %183 = nn.batch_flatten(%182) /* ty=Tensor[(32, 512), float32] */;
  %184 = nn.simulated_quantize(%183, %in_scale116, %out_scale116, %clip_min116, %clip_max116, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 512), float32] */;
  %185 = nn.simulated_quantize(meta[relay.Constant][40] /* ty=Tensor[(1000, 512), float32] */ /* ty=Tensor[(1000, 512), float32] */, %in_scale117, %out_scale117, %clip_min117, %clip_max117, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000, 512), float32] */;
  %186 = nn.dense(%184, %185, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  %187 = nn.simulated_quantize(%186, %in_scale118, %out_scale118, %clip_min118, %clip_max118, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */;
  %188 = nn.simulated_quantize(meta[relay.Constant][41] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */, %in_scale119, %out_scale119, %clip_min119, %clip_max119, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000), float32] */;
  %189 = add(%187, %188) /* ty=Tensor[(32, 1000), float32] */;
  nn.simulated_quantize(%189, %in_scale120, %out_scale120, %clip_min120, %clip_max120, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
calculating threshold...
threshold method:
avg_range
thresholds: [2.6328714, 0.2220482, 4.3506937, 0.6874881, 4.432858, 4.432858, 4.432858, 0.54553044, 9.525923, 1.522466, 8.400149, 3.7981372, 1.0774889, 11.1619, 1.2661262, 10.535105, 10.328388, 5.7362657, 0.24305855, 5.013894, 1.179964, 4.2310896, 2.5727034, 1.0661017, 5.6858826, 0.95773375, 5.1894836, 6.987096, 6.987096, 0.6479256, 3.022561, 0.9377952, 2.7795398, 0.19011457, 3.4856453, 0.8193389, 3.3992183, 3.0592165, 0.7278937, 4.6621428, 1.0132959, 4.439095, 4.732796, 4.732796, 0.4957537, 3.533898, 1.0434468, 3.6343296, 3.4409904, 0.87852496, 5.323593, 0.91657436, 5.5100746, 5.7487316, 5.7487316, 0.33109924, 1.7465111, 0.5326773, 1.7214947, 0.22134402, 3.572783, 0.7108252, 3.5181856, 3.063507, 0.71010673, 4.7471104, 0.7395471, 4.8800535, 5.2819414, 5.2819414, 0.3446066, 3.99934, 0.7690893, 3.3673244, 3.3673244, 0.7020171, 5.627665, 1.0121639, 5.4015403, 6.3052955, 6.3052955, 1.0782837, 2.1701648, 0.63687545, 2.1979291, 0.3463119, 3.4846873, 0.63679266, 3.3883557, 3.3883557, 1.3083792, 5.1740994, 1.3365514, 5.372775, 6.5063295, 6.5063295, 0.25089654, 3.6266098, 0.43459564, 3.3141809, 2.3603878, 4.8275437, 32.737225, 4.275579, 33.225784, 35.743122, 35.743122, 11.637708, 11.637708, 0.8083785, 26.637497, 0.04346545, 26.631872]

calculate parameters
---------
data[%0] -> nn.conv2d[%2]
  bit=8, threshold=2.632871389389038
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.02056930772960186, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%1] -> nn.conv2d[%2]
  bit=8, threshold=0.22204819321632385
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00173475150950253, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%2] -> add[%4]
  bit=32, threshold=0.6874880790710449
  SimulatedQuantizeParams(in_scale=3.568264e-05, out_scale=2.0259496302799107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%3] -> add[%4]
  bit=32, threshold=0.6874880790710449
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.0259496302799107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%4] -> nn.relu[%5]
  bit=32, threshold=4.432857990264893
  SimulatedQuantizeParams(in_scale=2.0259496e-09, out_scale=2.0259496302799107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%5] -> nn.max_pool2d[%6]
  bit=32, threshold=4.432857990264893
  SimulatedQuantizeParams(in_scale=2.0259496e-09, out_scale=2.0642103581991478e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%6] -> nn.conv2d[%8]
  bit=8, threshold=4.432857990264893
  SimulatedQuantizeParams(in_scale=2.0642104e-09, out_scale=0.03463170304894447, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%7] -> nn.conv2d[%8]
  bit=8, threshold=0.5455304384231567
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004261956550180912, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%8] -> add[%10]
  bit=32, threshold=1.5224659442901611
  SimulatedQuantizeParams(in_scale=0.00014759881, out_scale=4.435853462325667e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%9] -> add[%10]
  bit=32, threshold=1.5224659442901611
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.435853462325667e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%10] -> nn.relu[%11]
  bit=32, threshold=8.40014934539795
  SimulatedQuantizeParams(in_scale=4.4358535e-09, out_scale=4.435853462325667e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%11] -> nn.conv2d[%13]
  bit=8, threshold=3.7981371879577637
  SimulatedQuantizeParams(in_scale=4.4358535e-09, out_scale=0.02967294678092003, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%12] -> nn.conv2d[%13]
  bit=8, threshold=1.077488899230957
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008417882025241852, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%13] -> add[%15]
  bit=32, threshold=1.2661261558532715
  SimulatedQuantizeParams(in_scale=0.00024978336, out_scale=5.197664521006118e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%14] -> add[%15]
  bit=32, threshold=1.2661261558532715
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.197664521006118e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.max_pool2d[%6] -> add[%16]
  bit=32, threshold=10.535104751586914
  SimulatedQuantizeParams(in_scale=2.0642104e-09, out_scale=4.905790440545843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%15] -> add[%16]
  bit=32, threshold=10.535104751586914
  SimulatedQuantizeParams(in_scale=5.1976645e-09, out_scale=4.905790440545843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%16] -> nn.relu[%17]
  bit=32, threshold=10.328388214111328
  SimulatedQuantizeParams(in_scale=4.9057904e-09, out_scale=4.905790440545843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%17] -> nn.conv2d[%19]
  bit=8, threshold=5.736265659332275
  SimulatedQuantizeParams(in_scale=4.9057904e-09, out_scale=0.0448145754635334, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%18] -> nn.conv2d[%19]
  bit=8, threshold=0.24305854737758636
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0018988949013873935, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%19] -> add[%21]
  bit=32, threshold=1.1799639463424683
  SimulatedQuantizeParams(in_scale=8.509817e-05, out_scale=2.334776372237002e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%20] -> add[%21]
  bit=32, threshold=1.1799639463424683
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.334776372237002e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%21] -> nn.relu[%22]
  bit=32, threshold=4.2310895919799805
  SimulatedQuantizeParams(in_scale=2.3347764e-09, out_scale=2.334776372237002e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%22] -> nn.conv2d[%24]
  bit=8, threshold=2.5727033615112305
  SimulatedQuantizeParams(in_scale=2.3347764e-09, out_scale=0.020099245011806488, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%23] -> nn.conv2d[%24]
  bit=8, threshold=1.0661016702651978
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008328919298946857, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%24] -> add[%26]
  bit=32, threshold=0.9577337503433228
  SimulatedQuantizeParams(in_scale=0.00016740499, out_scale=2.6476953962628613e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%25] -> add[%26]
  bit=32, threshold=0.9577337503433228
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.6476953962628613e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%17] -> add[%27]
  bit=32, threshold=5.189483642578125
  SimulatedQuantizeParams(in_scale=4.9057904e-09, out_scale=2.671156851263845e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%26] -> add[%27]
  bit=32, threshold=5.189483642578125
  SimulatedQuantizeParams(in_scale=2.6476954e-09, out_scale=2.671156851263845e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%27] -> nn.relu[%28]
  bit=32, threshold=6.987095832824707
  SimulatedQuantizeParams(in_scale=2.6711569e-09, out_scale=2.671156851263845e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%28] -> nn.conv2d[%30]
  bit=8, threshold=6.987095832824707
  SimulatedQuantizeParams(in_scale=2.6711569e-09, out_scale=0.054586686193943024, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%29] -> nn.conv2d[%30]
  bit=8, threshold=0.647925615310669
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005061918869614601, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%30] -> add[%32]
  bit=32, threshold=0.9377952218055725
  SimulatedQuantizeParams(in_scale=0.00027631337, out_scale=1.4074896803606407e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%31] -> add[%32]
  bit=32, threshold=0.9377952218055725
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4074896803606407e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%28] -> nn.conv2d[%34]
  bit=8, threshold=6.987095832824707
  SimulatedQuantizeParams(in_scale=2.6711569e-09, out_scale=0.054586686193943024, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%33] -> nn.conv2d[%34]
  bit=8, threshold=0.1901145726442337
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0014852700987830758, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%34] -> add[%36]
  bit=32, threshold=0.8193389177322388
  SimulatedQuantizeParams(in_scale=8.1075974e-05, out_scale=1.6231300747904243e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%35] -> add[%36]
  bit=32, threshold=0.8193389177322388
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6231300747904243e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%36] -> nn.relu[%37]
  bit=32, threshold=3.3992183208465576
  SimulatedQuantizeParams(in_scale=1.6231301e-09, out_scale=1.6231300747904243e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%37] -> nn.conv2d[%39]
  bit=8, threshold=3.0592164993286133
  SimulatedQuantizeParams(in_scale=1.6231301e-09, out_scale=0.02390012890100479, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%38] -> nn.conv2d[%39]
  bit=8, threshold=0.7278937101364136
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005686669610440731, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%39] -> add[%41]
  bit=32, threshold=1.0132958889007568
  SimulatedQuantizeParams(in_scale=0.00013591214, out_scale=2.1709793962543245e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%40] -> add[%41]
  bit=32, threshold=1.0132958889007568
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.1709793962543245e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%32] -> add[%42]
  bit=32, threshold=4.4390950202941895
  SimulatedQuantizeParams(in_scale=1.4074897e-09, out_scale=2.067114701631567e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%41] -> add[%42]
  bit=32, threshold=4.4390950202941895
  SimulatedQuantizeParams(in_scale=2.1709794e-09, out_scale=2.067114701631567e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%42] -> nn.relu[%43]
  bit=32, threshold=4.7327961921691895
  SimulatedQuantizeParams(in_scale=2.0671147e-09, out_scale=2.067114701631567e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%43] -> nn.conv2d[%45]
  bit=8, threshold=4.7327961921691895
  SimulatedQuantizeParams(in_scale=2.0671147e-09, out_scale=0.03697497025132179, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%44] -> nn.conv2d[%45]
  bit=8, threshold=0.4957537055015564
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0038730758242309093, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%45] -> add[%47]
  bit=32, threshold=1.0434467792510986
  SimulatedQuantizeParams(in_scale=0.00014320687, out_scale=1.6455995455189054e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%46] -> add[%47]
  bit=32, threshold=1.0434467792510986
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6455995455189054e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%47] -> nn.relu[%48]
  bit=32, threshold=3.6343295574188232
  SimulatedQuantizeParams(in_scale=1.6455995e-09, out_scale=1.6455995455189054e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%48] -> nn.conv2d[%50]
  bit=8, threshold=3.440990447998047
  SimulatedQuantizeParams(in_scale=1.6455995e-09, out_scale=0.02688273787498474, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%49] -> nn.conv2d[%50]
  bit=8, threshold=0.8785249590873718
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006863476242870092, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%50] -> add[%52]
  bit=32, threshold=0.9165743589401245
  SimulatedQuantizeParams(in_scale=0.00018450903, out_scale=2.4789912345113407e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%51] -> add[%52]
  bit=32, threshold=0.9165743589401245
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.4789912345113407e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%43] -> add[%53]
  bit=32, threshold=5.510074615478516
  SimulatedQuantizeParams(in_scale=2.0671147e-09, out_scale=2.565828438605422e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%52] -> add[%53]
  bit=32, threshold=5.510074615478516
  SimulatedQuantizeParams(in_scale=2.4789912e-09, out_scale=2.565828438605422e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%53] -> nn.relu[%54]
  bit=32, threshold=5.74873161315918
  SimulatedQuantizeParams(in_scale=2.5658284e-09, out_scale=2.565828438605422e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%54] -> nn.conv2d[%56]
  bit=8, threshold=5.74873161315918
  SimulatedQuantizeParams(in_scale=2.5658284e-09, out_scale=0.04491196572780609, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%55] -> nn.conv2d[%56]
  bit=8, threshold=0.3310992419719696
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0025867128279060125, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%56] -> add[%58]
  bit=32, threshold=0.5326772928237915
  SimulatedQuantizeParams(in_scale=0.00011617436, out_scale=8.132826079254585e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%57] -> add[%58]
  bit=32, threshold=0.5326772928237915
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.132826079254585e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%54] -> nn.conv2d[%60]
  bit=8, threshold=5.74873161315918
  SimulatedQuantizeParams(in_scale=2.5658284e-09, out_scale=0.04491196572780609, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%59] -> nn.conv2d[%60]
  bit=8, threshold=0.2213440239429474
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0017292501870542765, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%60] -> add[%62]
  bit=32, threshold=0.7108252048492432
  SimulatedQuantizeParams(in_scale=7.766402e-05, out_scale=1.6637067279390294e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%61] -> add[%62]
  bit=32, threshold=0.7108252048492432
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6637067279390294e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%62] -> nn.relu[%63]
  bit=32, threshold=3.518185615539551
  SimulatedQuantizeParams(in_scale=1.6637067e-09, out_scale=1.6637067279390294e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%63] -> nn.conv2d[%65]
  bit=8, threshold=3.063507080078125
  SimulatedQuantizeParams(in_scale=1.6637067e-09, out_scale=0.02393364906311035, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%64] -> nn.conv2d[%65]
  bit=8, threshold=0.7101067304611206
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005547708831727505, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%65] -> add[%67]
  bit=32, threshold=0.739547073841095
  SimulatedQuantizeParams(in_scale=0.00013277691, out_scale=2.2105455244059158e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%66] -> add[%67]
  bit=32, threshold=0.739547073841095
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.2105455244059158e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%58] -> add[%68]
  bit=32, threshold=4.880053520202637
  SimulatedQuantizeParams(in_scale=8.132826e-10, out_scale=2.2724520043482244e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%67] -> add[%68]
  bit=32, threshold=4.880053520202637
  SimulatedQuantizeParams(in_scale=2.2105455e-09, out_scale=2.2724520043482244e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%68] -> nn.relu[%69]
  bit=32, threshold=5.2819414138793945
  SimulatedQuantizeParams(in_scale=2.272452e-09, out_scale=2.2724520043482244e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%69] -> nn.conv2d[%71]
  bit=8, threshold=5.2819414138793945
  SimulatedQuantizeParams(in_scale=2.272452e-09, out_scale=0.04126516729593277, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%70] -> nn.conv2d[%71]
  bit=8, threshold=0.3446066081523895
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.002692239126190543, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%71] -> add[%73]
  bit=32, threshold=0.7690892815589905
  SimulatedQuantizeParams(in_scale=0.000111095695, out_scale=1.8623378394977408e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%72] -> add[%73]
  bit=32, threshold=0.7690892815589905
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.8623378394977408e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%73] -> nn.relu[%74]
  bit=32, threshold=3.3673243522644043
  SimulatedQuantizeParams(in_scale=1.8623378e-09, out_scale=1.8623378394977408e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%74] -> nn.conv2d[%76]
  bit=8, threshold=3.3673243522644043
  SimulatedQuantizeParams(in_scale=1.8623378e-09, out_scale=0.02630722150206566, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%75] -> nn.conv2d[%76]
  bit=8, threshold=0.7020171284675598
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005484508816152811, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%76] -> add[%78]
  bit=32, threshold=1.0121638774871826
  SimulatedQuantizeParams(in_scale=0.00014428218, out_scale=2.6205857484029593e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%77] -> add[%78]
  bit=32, threshold=1.0121638774871826
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.6205857484029593e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%69] -> add[%79]
  bit=32, threshold=5.401540279388428
  SimulatedQuantizeParams(in_scale=2.272452e-09, out_scale=2.515288199944621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%78] -> add[%79]
  bit=32, threshold=5.401540279388428
  SimulatedQuantizeParams(in_scale=2.6205857e-09, out_scale=2.515288199944621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%79] -> nn.relu[%80]
  bit=32, threshold=6.305295467376709
  SimulatedQuantizeParams(in_scale=2.5152882e-09, out_scale=2.515288199944621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%80] -> nn.conv2d[%82]
  bit=8, threshold=6.305295467376709
  SimulatedQuantizeParams(in_scale=2.5152882e-09, out_scale=0.04926012083888054, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%81] -> nn.conv2d[%82]
  bit=8, threshold=1.078283667564392
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008424091152846813, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%82] -> add[%84]
  bit=32, threshold=0.6368754506111145
  SimulatedQuantizeParams(in_scale=0.00041497176, out_scale=1.0105617453959326e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%83] -> add[%84]
  bit=32, threshold=0.6368754506111145
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0105617453959326e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%80] -> nn.conv2d[%86]
  bit=8, threshold=6.305295467376709
  SimulatedQuantizeParams(in_scale=2.5152882e-09, out_scale=0.04926012083888054, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%85] -> nn.conv2d[%86]
  bit=8, threshold=0.34631189703941345
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0027055616956204176, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%86] -> add[%88]
  bit=32, threshold=0.6367926597595215
  SimulatedQuantizeParams(in_scale=0.00013327629, out_scale=1.6226839871791299e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%87] -> add[%88]
  bit=32, threshold=0.6367926597595215
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6226839871791299e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%88] -> nn.relu[%89]
  bit=32, threshold=3.3883557319641113
  SimulatedQuantizeParams(in_scale=1.622684e-09, out_scale=1.6226839871791299e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%89] -> nn.conv2d[%91]
  bit=8, threshold=3.3883557319641113
  SimulatedQuantizeParams(in_scale=1.622684e-09, out_scale=0.02647152915596962, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%90] -> nn.conv2d[%91]
  bit=8, threshold=1.3083791732788086
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.010221712291240692, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%91] -> add[%93]
  bit=32, threshold=1.3365514278411865
  SimulatedQuantizeParams(in_scale=0.00027058437, out_scale=2.4093778083766892e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%92] -> add[%93]
  bit=32, threshold=1.3365514278411865
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.4093778083766892e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%84] -> add[%94]
  bit=32, threshold=5.372775077819824
  SimulatedQuantizeParams(in_scale=1.0105617e-09, out_scale=2.5018933591525183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%93] -> add[%94]
  bit=32, threshold=5.372775077819824
  SimulatedQuantizeParams(in_scale=2.4093778e-09, out_scale=2.5018933591525183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%94] -> nn.relu[%95]
  bit=32, threshold=6.506329536437988
  SimulatedQuantizeParams(in_scale=2.5018934e-09, out_scale=2.5018933591525183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%95] -> nn.conv2d[%97]
  bit=8, threshold=6.506329536437988
  SimulatedQuantizeParams(in_scale=2.5018934e-09, out_scale=0.05083069950342178, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%96] -> nn.conv2d[%97]
  bit=8, threshold=0.25089654326438904
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0019601292442530394, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%97] -> add[%99]
  bit=32, threshold=0.43459564447402954
  SimulatedQuantizeParams(in_scale=9.963474e-05, out_scale=1.6887717890767817e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%98] -> add[%99]
  bit=32, threshold=0.43459564447402954
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6887717890767817e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%99] -> nn.relu[%100]
  bit=32, threshold=3.314180850982666
  SimulatedQuantizeParams(in_scale=1.6887718e-09, out_scale=1.6887717890767817e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%100] -> nn.conv2d[%102]
  bit=8, threshold=2.3603878021240234
  SimulatedQuantizeParams(in_scale=1.6887718e-09, out_scale=0.018440529704093933, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%101] -> nn.conv2d[%102]
  bit=8, threshold=4.82754373550415
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.037715185433626175, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%102] -> add[%104]
  bit=32, threshold=4.27557897567749
  SimulatedQuantizeParams(in_scale=0.000695488, out_scale=1.524445814027331e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%103] -> add[%104]
  bit=32, threshold=4.27557897567749
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.524445814027331e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%95] -> add[%105]
  bit=32, threshold=33.22578430175781
  SimulatedQuantizeParams(in_scale=2.5018934e-09, out_scale=1.5471961489765818e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%104] -> add[%105]
  bit=32, threshold=33.22578430175781
  SimulatedQuantizeParams(in_scale=1.5244458e-08, out_scale=1.5471961489765818e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%105] -> nn.relu[%106]
  bit=32, threshold=35.74312210083008
  SimulatedQuantizeParams(in_scale=1.5471961e-08, out_scale=1.5471961489765818e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%106] -> nn.global_avg_pool2d[%107]
  not quantized
  SimulatedQuantizeParams(in_scale=1.5471961e-08, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.global_avg_pool2d[%107] -> nn.batch_flatten[%108]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.batch_flatten[%108] -> nn.dense[%110]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%109] -> nn.dense[%110]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.dense[%110] -> add[%112]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%111] -> add[%112]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
add[%112] -> OUT
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
measures
Measure(version=0.1, strategy=Strategy(model_hash=-2157293650320921168, bits=[8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32], thresholds=[2.632871389389038, 0.22204819321632385, 4.350693702697754, 0.6874880790710449, 4.432857990264893, 4.432857990264893, 4.432857990264893, 0.5455304384231567, 9.525922775268555, 1.5224659442901611, 8.40014934539795, 3.7981371879577637, 1.077488899230957, 11.16189956665039, 1.2661261558532715, 10.535104751586914, 10.328388214111328, 5.736265659332275, 0.24305854737758636, 5.013894081115723, 1.1799639463424683, 4.2310895919799805, 2.5727033615112305, 1.0661016702651978, 5.685882568359375, 0.9577337503433228, 5.189483642578125, 6.987095832824707, 6.987095832824707, 0.647925615310669, 3.0225610733032227, 0.9377952218055725, 2.7795398235321045, 0.1901145726442337, 3.485645294189453, 0.8193389177322388, 3.3992183208465576, 3.0592164993286133, 0.7278937101364136, 4.662142753601074, 1.0132958889007568, 4.4390950202941895, 4.7327961921691895, 4.7327961921691895, 0.4957537055015564, 3.533898115158081, 1.0434467792510986, 3.6343295574188232, 3.440990447998047, 0.8785249590873718, 5.3235931396484375, 0.9165743589401245, 5.510074615478516, 5.74873161315918, 5.74873161315918, 0.3310992419719696, 1.7465111017227173, 0.5326772928237915, 1.7214946746826172, 0.2213440239429474, 3.5727829933166504, 0.7108252048492432, 3.518185615539551, 3.063507080078125, 0.7101067304611206, 4.747110366821289, 0.739547073841095, 4.880053520202637, 5.2819414138793945, 5.2819414138793945, 0.3446066081523895, 3.999340057373047, 0.7690892815589905, 3.3673243522644043, 3.3673243522644043, 0.7020171284675598, 5.627665042877197, 1.0121638774871826, 5.401540279388428, 6.305295467376709, 6.305295467376709, 1.078283667564392, 2.1701648235321045, 0.6368754506111145, 2.1979291439056396, 0.34631189703941345, 3.484687328338623, 0.6367926597595215, 3.3883557319641113, 3.3883557319641113, 1.3083791732788086, 5.174099445343018, 1.3365514278411865, 5.372775077819824, 6.506329536437988, 6.506329536437988, 0.25089654326438904, 3.6266098022460938, 0.43459564447402954, 3.314180850982666, 2.3603878021240234, 4.82754373550415, 32.73722457885742, 4.27557897567749, 33.22578430175781, 35.74312210083008, 35.74312210083008, 11.637707710266113, 11.637707710266113, 0.8083785176277161, 26.637496948242188, 0.043465450406074524, 26.631872177124023]), result=MeasureResult(accuracy=0.75, kl_distance=None))
best_measure
Measure(version=0.1, strategy=Strategy(model_hash=-2157293650320921168, bits=[8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32], thresholds=[2.632871389389038, 0.22204819321632385, 4.350693702697754, 0.6874880790710449, 4.432857990264893, 4.432857990264893, 4.432857990264893, 0.5455304384231567, 9.525922775268555, 1.5224659442901611, 8.40014934539795, 3.7981371879577637, 1.077488899230957, 11.16189956665039, 1.2661261558532715, 10.535104751586914, 10.328388214111328, 5.736265659332275, 0.24305854737758636, 5.013894081115723, 1.1799639463424683, 4.2310895919799805, 2.5727033615112305, 1.0661016702651978, 5.685882568359375, 0.9577337503433228, 5.189483642578125, 6.987095832824707, 6.987095832824707, 0.647925615310669, 3.0225610733032227, 0.9377952218055725, 2.7795398235321045, 0.1901145726442337, 3.485645294189453, 0.8193389177322388, 3.3992183208465576, 3.0592164993286133, 0.7278937101364136, 4.662142753601074, 1.0132958889007568, 4.4390950202941895, 4.7327961921691895, 4.7327961921691895, 0.4957537055015564, 3.533898115158081, 1.0434467792510986, 3.6343295574188232, 3.440990447998047, 0.8785249590873718, 5.3235931396484375, 0.9165743589401245, 5.510074615478516, 5.74873161315918, 5.74873161315918, 0.3310992419719696, 1.7465111017227173, 0.5326772928237915, 1.7214946746826172, 0.2213440239429474, 3.5727829933166504, 0.7108252048492432, 3.518185615539551, 3.063507080078125, 0.7101067304611206, 4.747110366821289, 0.739547073841095, 4.880053520202637, 5.2819414138793945, 5.2819414138793945, 0.3446066081523895, 3.999340057373047, 0.7690892815589905, 3.3673243522644043, 3.3673243522644043, 0.7020171284675598, 5.627665042877197, 1.0121638774871826, 5.401540279388428, 6.305295467376709, 6.305295467376709, 1.078283667564392, 2.1701648235321045, 0.6368754506111145, 2.1979291439056396, 0.34631189703941345, 3.484687328338623, 0.6367926597595215, 3.3883557319641113, 3.3883557319641113, 1.3083791732788086, 5.174099445343018, 1.3365514278411865, 5.372775077819824, 6.506329536437988, 6.506329536437988, 0.25089654326438904, 3.6266098022460938, 0.43459564447402954, 3.314180850982666, 2.3603878021240234, 4.82754373550415, 32.73722457885742, 4.27557897567749, 33.22578430175781, 35.74312210083008, 35.74312210083008, 11.637707710266113, 11.637707710266113, 0.8083785176277161, 26.637496948242188, 0.043465450406074524, 26.631872177124023]), result=MeasureResult(accuracy=0.75, kl_distance=None))
data
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
add
nn.relu
nn.global_avg_pool2d
nn.batch_flatten
constant
nn.dense
constant
add
data -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.max_pool2d -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
nn.relu -> add
add -> add
add -> nn.relu
nn.relu -> nn.global_avg_pool2d
nn.global_avg_pool2d -> nn.batch_flatten
nn.batch_flatten -> nn.dense
constant -> nn.dense
nn.dense -> add
constant -> add
analyzed condition
node_conds: [False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, False, False, False, False, False]
edge_conds: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]

select descriptor
---------
nn.conv2d[%2]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%4]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%5]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%6]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%8]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%10]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%11]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%13]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%15]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%16]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%17]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%19]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%21]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%22]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%24]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%26]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%27]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%28]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%30]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%32]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%34]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%36]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%37]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%39]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%41]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%42]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%43]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%45]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%47]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%48]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%50]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%52]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%53]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%54]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%56]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%58]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%60]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%62]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%63]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%65]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%67]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%68]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%69]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%71]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%73]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%74]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%76]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%78]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%79]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%80]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%82]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%84]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%86]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%88]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%89]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%91]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%93]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%94]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%95]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%97]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%99]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%100]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%102]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%104]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%105]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%106]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %4 = nn.conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %5 = add(%4, meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %6 = nn.relu(%5) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %7 = nn.conv2d(%6, meta[relay.Constant][4] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %8 = add(%7, meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %9 = add(%3, %8) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.relu(%9) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.conv2d(%10, meta[relay.Constant][6] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %12 = add(%11, meta[relay.Constant][7] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %13 = nn.relu(%12) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, meta[relay.Constant][8] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %15 = add(%14, meta[relay.Constant][9] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %16 = add(%10, %15) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %17 = nn.relu(%16) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %18 = nn.conv2d(%17, meta[relay.Constant][10] /* ty=Tensor[(128, 64, 1, 1), float32] */ /* ty=Tensor[(128, 64, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %19 = add(%18, meta[relay.Constant][11] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %20 = nn.conv2d(%17, meta[relay.Constant][12] /* ty=Tensor[(128, 64, 3, 3), float32] */ /* ty=Tensor[(128, 64, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %21 = add(%20, meta[relay.Constant][13] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %22 = nn.relu(%21) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %23 = nn.conv2d(%22, meta[relay.Constant][14] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %24 = add(%23, meta[relay.Constant][15] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %25 = add(%19, %24) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %26 = nn.relu(%25) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %27 = nn.conv2d(%26, meta[relay.Constant][16] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %28 = add(%27, meta[relay.Constant][17] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %29 = nn.relu(%28) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %30 = nn.conv2d(%29, meta[relay.Constant][18] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %31 = add(%30, meta[relay.Constant][19] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %32 = add(%26, %31) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%33, meta[relay.Constant][20] /* ty=Tensor[(256, 128, 1, 1), float32] */ /* ty=Tensor[(256, 128, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %35 = add(%34, meta[relay.Constant][21] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %36 = nn.conv2d(%33, meta[relay.Constant][22] /* ty=Tensor[(256, 128, 3, 3), float32] */ /* ty=Tensor[(256, 128, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %37 = add(%36, meta[relay.Constant][23] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %38 = nn.relu(%37) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %39 = nn.conv2d(%38, meta[relay.Constant][24] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %40 = add(%39, meta[relay.Constant][25] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %41 = add(%35, %40) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %42 = nn.relu(%41) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %43 = nn.conv2d(%42, meta[relay.Constant][26] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %44 = add(%43, meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %45 = nn.relu(%44) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %46 = nn.conv2d(%45, meta[relay.Constant][28] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %47 = add(%46, meta[relay.Constant][29] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %48 = add(%42, %47) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %49 = nn.relu(%48) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %50 = nn.conv2d(%49, meta[relay.Constant][30] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %51 = add(%50, meta[relay.Constant][31] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %52 = nn.conv2d(%49, meta[relay.Constant][32] /* ty=Tensor[(512, 256, 3, 3), float32] */ /* ty=Tensor[(512, 256, 3, 3), float32] */, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %53 = add(%52, meta[relay.Constant][33] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %54 = nn.relu(%53) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %55 = nn.conv2d(%54, meta[relay.Constant][34] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %56 = add(%55, meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %57 = add(%51, %56) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %59 = nn.conv2d(%58, meta[relay.Constant][36] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %60 = add(%59, meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %61 = nn.relu(%60) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %62 = nn.conv2d(%61, meta[relay.Constant][38] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %63 = add(%62, meta[relay.Constant][39] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %64 = add(%58, %63) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %65 = nn.relu(%64) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %66 = nn.global_avg_pool2d(%65) /* ty=Tensor[(32, 512, 1, 1), float32] */;
  %67 = nn.batch_flatten(%66) /* ty=Tensor[(32, 512), float32] */;
  %68 = nn.dense(%67, meta[relay.Constant][40] /* ty=Tensor[(1000, 512), float32] */ /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%68, meta[relay.Constant][41] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data

calculate parameters
---------
data[%0] -> nn.conv2d[%2]
  bit=8, threshold=2.632871389389038
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.02056930772960186, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%1] -> nn.conv2d[%2]
  bit=8, threshold=0.22204819321632385
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00173475150950253, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%2] -> add[%4]
  bit=32, threshold=0.6874880790710449
  SimulatedQuantizeParams(in_scale=3.568264e-05, out_scale=2.0259496302799107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%3] -> add[%4]
  bit=32, threshold=0.6874880790710449
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.0259496302799107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%4] -> nn.relu[%5]
  bit=32, threshold=4.432857990264893
  SimulatedQuantizeParams(in_scale=2.0259496e-09, out_scale=2.0259496302799107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%5] -> nn.max_pool2d[%6]
  bit=32, threshold=4.432857990264893
  SimulatedQuantizeParams(in_scale=2.0259496e-09, out_scale=2.0642103581991478e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%6] -> nn.conv2d[%8]
  bit=8, threshold=4.432857990264893
  SimulatedQuantizeParams(in_scale=2.0642104e-09, out_scale=0.03463170304894447, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%7] -> nn.conv2d[%8]
  bit=8, threshold=0.5455304384231567
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004261956550180912, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%8] -> add[%10]
  bit=32, threshold=1.5224659442901611
  SimulatedQuantizeParams(in_scale=0.00014759881, out_scale=4.435853462325667e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%9] -> add[%10]
  bit=32, threshold=1.5224659442901611
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.435853462325667e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%10] -> nn.relu[%11]
  bit=32, threshold=8.40014934539795
  SimulatedQuantizeParams(in_scale=4.4358535e-09, out_scale=4.435853462325667e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%11] -> nn.conv2d[%13]
  bit=8, threshold=3.7981371879577637
  SimulatedQuantizeParams(in_scale=4.4358535e-09, out_scale=0.02967294678092003, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%12] -> nn.conv2d[%13]
  bit=8, threshold=1.077488899230957
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008417882025241852, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%13] -> add[%15]
  bit=32, threshold=1.2661261558532715
  SimulatedQuantizeParams(in_scale=0.00024978336, out_scale=5.197664521006118e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%14] -> add[%15]
  bit=32, threshold=1.2661261558532715
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.197664521006118e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.max_pool2d[%6] -> add[%16]
  bit=32, threshold=10.535104751586914
  SimulatedQuantizeParams(in_scale=2.0642104e-09, out_scale=4.905790440545843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%15] -> add[%16]
  bit=32, threshold=10.535104751586914
  SimulatedQuantizeParams(in_scale=5.1976645e-09, out_scale=4.905790440545843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%16] -> nn.relu[%17]
  bit=32, threshold=10.328388214111328
  SimulatedQuantizeParams(in_scale=4.9057904e-09, out_scale=4.905790440545843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%17] -> nn.conv2d[%19]
  bit=8, threshold=5.736265659332275
  SimulatedQuantizeParams(in_scale=4.9057904e-09, out_scale=0.0448145754635334, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%18] -> nn.conv2d[%19]
  bit=8, threshold=0.24305854737758636
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0018988949013873935, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%19] -> add[%21]
  bit=32, threshold=1.1799639463424683
  SimulatedQuantizeParams(in_scale=8.509817e-05, out_scale=2.334776372237002e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%20] -> add[%21]
  bit=32, threshold=1.1799639463424683
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.334776372237002e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%21] -> nn.relu[%22]
  bit=32, threshold=4.2310895919799805
  SimulatedQuantizeParams(in_scale=2.3347764e-09, out_scale=2.334776372237002e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%22] -> nn.conv2d[%24]
  bit=8, threshold=2.5727033615112305
  SimulatedQuantizeParams(in_scale=2.3347764e-09, out_scale=0.020099245011806488, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%23] -> nn.conv2d[%24]
  bit=8, threshold=1.0661016702651978
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008328919298946857, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%24] -> add[%26]
  bit=32, threshold=0.9577337503433228
  SimulatedQuantizeParams(in_scale=0.00016740499, out_scale=2.6476953962628613e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%25] -> add[%26]
  bit=32, threshold=0.9577337503433228
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.6476953962628613e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%17] -> add[%27]
  bit=32, threshold=5.189483642578125
  SimulatedQuantizeParams(in_scale=4.9057904e-09, out_scale=2.671156851263845e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%26] -> add[%27]
  bit=32, threshold=5.189483642578125
  SimulatedQuantizeParams(in_scale=2.6476954e-09, out_scale=2.671156851263845e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%27] -> nn.relu[%28]
  bit=32, threshold=6.987095832824707
  SimulatedQuantizeParams(in_scale=2.6711569e-09, out_scale=2.671156851263845e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%28] -> nn.conv2d[%30]
  bit=8, threshold=6.987095832824707
  SimulatedQuantizeParams(in_scale=2.6711569e-09, out_scale=0.054586686193943024, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%29] -> nn.conv2d[%30]
  bit=8, threshold=0.647925615310669
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005061918869614601, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%30] -> add[%32]
  bit=32, threshold=0.9377952218055725
  SimulatedQuantizeParams(in_scale=0.00027631337, out_scale=1.4074896803606407e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%31] -> add[%32]
  bit=32, threshold=0.9377952218055725
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4074896803606407e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%28] -> nn.conv2d[%34]
  bit=8, threshold=6.987095832824707
  SimulatedQuantizeParams(in_scale=2.6711569e-09, out_scale=0.054586686193943024, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%33] -> nn.conv2d[%34]
  bit=8, threshold=0.1901145726442337
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0014852700987830758, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%34] -> add[%36]
  bit=32, threshold=0.8193389177322388
  SimulatedQuantizeParams(in_scale=8.1075974e-05, out_scale=1.6231300747904243e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%35] -> add[%36]
  bit=32, threshold=0.8193389177322388
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6231300747904243e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%36] -> nn.relu[%37]
  bit=32, threshold=3.3992183208465576
  SimulatedQuantizeParams(in_scale=1.6231301e-09, out_scale=1.6231300747904243e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%37] -> nn.conv2d[%39]
  bit=8, threshold=3.0592164993286133
  SimulatedQuantizeParams(in_scale=1.6231301e-09, out_scale=0.02390012890100479, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%38] -> nn.conv2d[%39]
  bit=8, threshold=0.7278937101364136
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005686669610440731, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%39] -> add[%41]
  bit=32, threshold=1.0132958889007568
  SimulatedQuantizeParams(in_scale=0.00013591214, out_scale=2.1709793962543245e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%40] -> add[%41]
  bit=32, threshold=1.0132958889007568
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.1709793962543245e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%32] -> add[%42]
  bit=32, threshold=4.4390950202941895
  SimulatedQuantizeParams(in_scale=1.4074897e-09, out_scale=2.067114701631567e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%41] -> add[%42]
  bit=32, threshold=4.4390950202941895
  SimulatedQuantizeParams(in_scale=2.1709794e-09, out_scale=2.067114701631567e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%42] -> nn.relu[%43]
  bit=32, threshold=4.7327961921691895
  SimulatedQuantizeParams(in_scale=2.0671147e-09, out_scale=2.067114701631567e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%43] -> nn.conv2d[%45]
  bit=8, threshold=4.7327961921691895
  SimulatedQuantizeParams(in_scale=2.0671147e-09, out_scale=0.03697497025132179, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%44] -> nn.conv2d[%45]
  bit=8, threshold=0.4957537055015564
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0038730758242309093, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%45] -> add[%47]
  bit=32, threshold=1.0434467792510986
  SimulatedQuantizeParams(in_scale=0.00014320687, out_scale=1.6455995455189054e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%46] -> add[%47]
  bit=32, threshold=1.0434467792510986
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6455995455189054e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%47] -> nn.relu[%48]
  bit=32, threshold=3.6343295574188232
  SimulatedQuantizeParams(in_scale=1.6455995e-09, out_scale=1.6455995455189054e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%48] -> nn.conv2d[%50]
  bit=8, threshold=3.440990447998047
  SimulatedQuantizeParams(in_scale=1.6455995e-09, out_scale=0.02688273787498474, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%49] -> nn.conv2d[%50]
  bit=8, threshold=0.8785249590873718
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006863476242870092, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%50] -> add[%52]
  bit=32, threshold=0.9165743589401245
  SimulatedQuantizeParams(in_scale=0.00018450903, out_scale=2.4789912345113407e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%51] -> add[%52]
  bit=32, threshold=0.9165743589401245
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.4789912345113407e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%43] -> add[%53]
  bit=32, threshold=5.510074615478516
  SimulatedQuantizeParams(in_scale=2.0671147e-09, out_scale=2.565828438605422e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%52] -> add[%53]
  bit=32, threshold=5.510074615478516
  SimulatedQuantizeParams(in_scale=2.4789912e-09, out_scale=2.565828438605422e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%53] -> nn.relu[%54]
  bit=32, threshold=5.74873161315918
  SimulatedQuantizeParams(in_scale=2.5658284e-09, out_scale=2.565828438605422e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%54] -> nn.conv2d[%56]
  bit=8, threshold=5.74873161315918
  SimulatedQuantizeParams(in_scale=2.5658284e-09, out_scale=0.04491196572780609, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%55] -> nn.conv2d[%56]
  bit=8, threshold=0.3310992419719696
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0025867128279060125, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%56] -> add[%58]
  bit=32, threshold=0.5326772928237915
  SimulatedQuantizeParams(in_scale=0.00011617436, out_scale=8.132826079254585e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%57] -> add[%58]
  bit=32, threshold=0.5326772928237915
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.132826079254585e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%54] -> nn.conv2d[%60]
  bit=8, threshold=5.74873161315918
  SimulatedQuantizeParams(in_scale=2.5658284e-09, out_scale=0.04491196572780609, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%59] -> nn.conv2d[%60]
  bit=8, threshold=0.2213440239429474
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0017292501870542765, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%60] -> add[%62]
  bit=32, threshold=0.7108252048492432
  SimulatedQuantizeParams(in_scale=7.766402e-05, out_scale=1.6637067279390294e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%61] -> add[%62]
  bit=32, threshold=0.7108252048492432
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6637067279390294e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%62] -> nn.relu[%63]
  bit=32, threshold=3.518185615539551
  SimulatedQuantizeParams(in_scale=1.6637067e-09, out_scale=1.6637067279390294e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%63] -> nn.conv2d[%65]
  bit=8, threshold=3.063507080078125
  SimulatedQuantizeParams(in_scale=1.6637067e-09, out_scale=0.02393364906311035, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%64] -> nn.conv2d[%65]
  bit=8, threshold=0.7101067304611206
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005547708831727505, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%65] -> add[%67]
  bit=32, threshold=0.739547073841095
  SimulatedQuantizeParams(in_scale=0.00013277691, out_scale=2.2105455244059158e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%66] -> add[%67]
  bit=32, threshold=0.739547073841095
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.2105455244059158e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%58] -> add[%68]
  bit=32, threshold=4.880053520202637
  SimulatedQuantizeParams(in_scale=8.132826e-10, out_scale=2.2724520043482244e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%67] -> add[%68]
  bit=32, threshold=4.880053520202637
  SimulatedQuantizeParams(in_scale=2.2105455e-09, out_scale=2.2724520043482244e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%68] -> nn.relu[%69]
  bit=32, threshold=5.2819414138793945
  SimulatedQuantizeParams(in_scale=2.272452e-09, out_scale=2.2724520043482244e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%69] -> nn.conv2d[%71]
  bit=8, threshold=5.2819414138793945
  SimulatedQuantizeParams(in_scale=2.272452e-09, out_scale=0.04126516729593277, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%70] -> nn.conv2d[%71]
  bit=8, threshold=0.3446066081523895
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.002692239126190543, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%71] -> add[%73]
  bit=32, threshold=0.7690892815589905
  SimulatedQuantizeParams(in_scale=0.000111095695, out_scale=1.8623378394977408e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%72] -> add[%73]
  bit=32, threshold=0.7690892815589905
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.8623378394977408e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%73] -> nn.relu[%74]
  bit=32, threshold=3.3673243522644043
  SimulatedQuantizeParams(in_scale=1.8623378e-09, out_scale=1.8623378394977408e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%74] -> nn.conv2d[%76]
  bit=8, threshold=3.3673243522644043
  SimulatedQuantizeParams(in_scale=1.8623378e-09, out_scale=0.02630722150206566, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%75] -> nn.conv2d[%76]
  bit=8, threshold=0.7020171284675598
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005484508816152811, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%76] -> add[%78]
  bit=32, threshold=1.0121638774871826
  SimulatedQuantizeParams(in_scale=0.00014428218, out_scale=2.6205857484029593e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%77] -> add[%78]
  bit=32, threshold=1.0121638774871826
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.6205857484029593e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%69] -> add[%79]
  bit=32, threshold=5.401540279388428
  SimulatedQuantizeParams(in_scale=2.272452e-09, out_scale=2.515288199944621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%78] -> add[%79]
  bit=32, threshold=5.401540279388428
  SimulatedQuantizeParams(in_scale=2.6205857e-09, out_scale=2.515288199944621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%79] -> nn.relu[%80]
  bit=32, threshold=6.305295467376709
  SimulatedQuantizeParams(in_scale=2.5152882e-09, out_scale=2.515288199944621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%80] -> nn.conv2d[%82]
  bit=8, threshold=6.305295467376709
  SimulatedQuantizeParams(in_scale=2.5152882e-09, out_scale=0.04926012083888054, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%81] -> nn.conv2d[%82]
  bit=8, threshold=1.078283667564392
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008424091152846813, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%82] -> add[%84]
  bit=32, threshold=0.6368754506111145
  SimulatedQuantizeParams(in_scale=0.00041497176, out_scale=1.0105617453959326e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%83] -> add[%84]
  bit=32, threshold=0.6368754506111145
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0105617453959326e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%80] -> nn.conv2d[%86]
  bit=8, threshold=6.305295467376709
  SimulatedQuantizeParams(in_scale=2.5152882e-09, out_scale=0.04926012083888054, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%85] -> nn.conv2d[%86]
  bit=8, threshold=0.34631189703941345
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0027055616956204176, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%86] -> add[%88]
  bit=32, threshold=0.6367926597595215
  SimulatedQuantizeParams(in_scale=0.00013327629, out_scale=1.6226839871791299e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%87] -> add[%88]
  bit=32, threshold=0.6367926597595215
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6226839871791299e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%88] -> nn.relu[%89]
  bit=32, threshold=3.3883557319641113
  SimulatedQuantizeParams(in_scale=1.622684e-09, out_scale=1.6226839871791299e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%89] -> nn.conv2d[%91]
  bit=8, threshold=3.3883557319641113
  SimulatedQuantizeParams(in_scale=1.622684e-09, out_scale=0.02647152915596962, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%90] -> nn.conv2d[%91]
  bit=8, threshold=1.3083791732788086
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.010221712291240692, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%91] -> add[%93]
  bit=32, threshold=1.3365514278411865
  SimulatedQuantizeParams(in_scale=0.00027058437, out_scale=2.4093778083766892e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%92] -> add[%93]
  bit=32, threshold=1.3365514278411865
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.4093778083766892e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%84] -> add[%94]
  bit=32, threshold=5.372775077819824
  SimulatedQuantizeParams(in_scale=1.0105617e-09, out_scale=2.5018933591525183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%93] -> add[%94]
  bit=32, threshold=5.372775077819824
  SimulatedQuantizeParams(in_scale=2.4093778e-09, out_scale=2.5018933591525183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%94] -> nn.relu[%95]
  bit=32, threshold=6.506329536437988
  SimulatedQuantizeParams(in_scale=2.5018934e-09, out_scale=2.5018933591525183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%95] -> nn.conv2d[%97]
  bit=8, threshold=6.506329536437988
  SimulatedQuantizeParams(in_scale=2.5018934e-09, out_scale=0.05083069950342178, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%96] -> nn.conv2d[%97]
  bit=8, threshold=0.25089654326438904
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0019601292442530394, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%97] -> add[%99]
  bit=32, threshold=0.43459564447402954
  SimulatedQuantizeParams(in_scale=9.963474e-05, out_scale=1.6887717890767817e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%98] -> add[%99]
  bit=32, threshold=0.43459564447402954
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6887717890767817e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%99] -> nn.relu[%100]
  bit=32, threshold=3.314180850982666
  SimulatedQuantizeParams(in_scale=1.6887718e-09, out_scale=1.6887717890767817e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%100] -> nn.conv2d[%102]
  bit=8, threshold=2.3603878021240234
  SimulatedQuantizeParams(in_scale=1.6887718e-09, out_scale=0.018440529704093933, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%101] -> nn.conv2d[%102]
  bit=8, threshold=4.82754373550415
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.037715185433626175, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%102] -> add[%104]
  bit=32, threshold=4.27557897567749
  SimulatedQuantizeParams(in_scale=0.000695488, out_scale=1.524445814027331e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%103] -> add[%104]
  bit=32, threshold=4.27557897567749
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.524445814027331e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%95] -> add[%105]
  bit=32, threshold=33.22578430175781
  SimulatedQuantizeParams(in_scale=2.5018934e-09, out_scale=1.5471961489765818e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%104] -> add[%105]
  bit=32, threshold=33.22578430175781
  SimulatedQuantizeParams(in_scale=1.5244458e-08, out_scale=1.5471961489765818e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%105] -> nn.relu[%106]
  bit=32, threshold=35.74312210083008
  SimulatedQuantizeParams(in_scale=1.5471961e-08, out_scale=1.5471961489765818e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%106] -> nn.global_avg_pool2d[%107]
  not quantized
  SimulatedQuantizeParams(in_scale=1.5471961e-08, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.global_avg_pool2d[%107] -> nn.batch_flatten[%108]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.batch_flatten[%108] -> nn.dense[%110]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%109] -> nn.dense[%110]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.dense[%110] -> add[%112]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%111] -> add[%112]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
add[%112] -> OUT
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
num of snodes:
113
num of nodes:
113
---------
simulated_quantize(data[%0])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0205693f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%6])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00173475f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%12])
  in_scale: 3.56826e-05f /* ty=float32 */
  out_scale: 2.02595e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%18])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.02595e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%24])
  in_scale: 2.02595e-09f /* ty=float32 */
  out_scale: 2.02595e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%30])
  in_scale: 2.02595e-09f /* ty=float32 */
  out_scale: 2.06421e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%36])
  in_scale: 2.06421e-09f /* ty=float32 */
  out_scale: 4.90579e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%36])
  in_scale: 2.06421e-09f /* ty=float32 */
  out_scale: 0.0346317f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%47])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00426196f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%53])
  in_scale: 0.000147599f /* ty=float32 */
  out_scale: 4.43585e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%59])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.43585e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%65])
  in_scale: 4.43585e-09f /* ty=float32 */
  out_scale: 4.43585e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%71])
  in_scale: 4.43585e-09f /* ty=float32 */
  out_scale: 0.0296729f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%77])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00841788f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%83])
  in_scale: 0.000249783f /* ty=float32 */
  out_scale: 5.19766e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%89])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.19766e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%95])
  in_scale: 5.19766e-09f /* ty=float32 */
  out_scale: 4.90579e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%101])
  in_scale: 4.90579e-09f /* ty=float32 */
  out_scale: 4.90579e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%107])
  in_scale: 4.90579e-09f /* ty=float32 */
  out_scale: 2.67116e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%107])
  in_scale: 4.90579e-09f /* ty=float32 */
  out_scale: 0.0448146f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%118])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00189889f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%124])
  in_scale: 8.50982e-05f /* ty=float32 */
  out_scale: 2.33478e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%130])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.33478e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%136])
  in_scale: 2.33478e-09f /* ty=float32 */
  out_scale: 2.33478e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%142])
  in_scale: 2.33478e-09f /* ty=float32 */
  out_scale: 0.0200992f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%148])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00832892f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%154])
  in_scale: 0.000167405f /* ty=float32 */
  out_scale: 2.6477e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%160])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.6477e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%166])
  in_scale: 2.6477e-09f /* ty=float32 */
  out_scale: 2.67116e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%172])
  in_scale: 2.67116e-09f /* ty=float32 */
  out_scale: 2.67116e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%178])
  in_scale: 2.67116e-09f /* ty=float32 */
  out_scale: 0.0545867f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%184])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00506192f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%190])
  in_scale: 0.000276313f /* ty=float32 */
  out_scale: 1.40749e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%196])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.40749e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%202])
  in_scale: 1.40749e-09f /* ty=float32 */
  out_scale: 2.06711e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%178])
  in_scale: 2.67116e-09f /* ty=float32 */
  out_scale: 0.0545867f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%213])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00148527f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%219])
  in_scale: 8.1076e-05f /* ty=float32 */
  out_scale: 1.62313e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%225])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.62313e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%231])
  in_scale: 1.62313e-09f /* ty=float32 */
  out_scale: 1.62313e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%237])
  in_scale: 1.62313e-09f /* ty=float32 */
  out_scale: 0.0239001f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%243])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00568667f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%249])
  in_scale: 0.000135912f /* ty=float32 */
  out_scale: 2.17098e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%255])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.17098e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%261])
  in_scale: 2.17098e-09f /* ty=float32 */
  out_scale: 2.06711e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%267])
  in_scale: 2.06711e-09f /* ty=float32 */
  out_scale: 2.06711e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%273])
  in_scale: 2.06711e-09f /* ty=float32 */
  out_scale: 2.56583e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%273])
  in_scale: 2.06711e-09f /* ty=float32 */
  out_scale: 0.036975f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%284])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00387308f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%290])
  in_scale: 0.000143207f /* ty=float32 */
  out_scale: 1.6456e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%296])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.6456e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%302])
  in_scale: 1.6456e-09f /* ty=float32 */
  out_scale: 1.6456e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%308])
  in_scale: 1.6456e-09f /* ty=float32 */
  out_scale: 0.0268827f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%314])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00686348f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%320])
  in_scale: 0.000184509f /* ty=float32 */
  out_scale: 2.47899e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%326])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.47899e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%332])
  in_scale: 2.47899e-09f /* ty=float32 */
  out_scale: 2.56583e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%338])
  in_scale: 2.56583e-09f /* ty=float32 */
  out_scale: 2.56583e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%344])
  in_scale: 2.56583e-09f /* ty=float32 */
  out_scale: 0.044912f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%350])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00258671f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%356])
  in_scale: 0.000116174f /* ty=float32 */
  out_scale: 8.13283e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%362])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.13283e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%368])
  in_scale: 8.13283e-10f /* ty=float32 */
  out_scale: 2.27245e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%344])
  in_scale: 2.56583e-09f /* ty=float32 */
  out_scale: 0.044912f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%379])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00172925f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%385])
  in_scale: 7.7664e-05f /* ty=float32 */
  out_scale: 1.66371e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%391])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.66371e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%397])
  in_scale: 1.66371e-09f /* ty=float32 */
  out_scale: 1.66371e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%403])
  in_scale: 1.66371e-09f /* ty=float32 */
  out_scale: 0.0239336f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%409])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00554771f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%415])
  in_scale: 0.000132777f /* ty=float32 */
  out_scale: 2.21055e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%421])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.21055e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%427])
  in_scale: 2.21055e-09f /* ty=float32 */
  out_scale: 2.27245e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%433])
  in_scale: 2.27245e-09f /* ty=float32 */
  out_scale: 2.27245e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%439])
  in_scale: 2.27245e-09f /* ty=float32 */
  out_scale: 2.51529e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%439])
  in_scale: 2.27245e-09f /* ty=float32 */
  out_scale: 0.0412652f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%450])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00269224f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%456])
  in_scale: 0.000111096f /* ty=float32 */
  out_scale: 1.86234e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%462])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.86234e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%468])
  in_scale: 1.86234e-09f /* ty=float32 */
  out_scale: 1.86234e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%474])
  in_scale: 1.86234e-09f /* ty=float32 */
  out_scale: 0.0263072f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%480])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00548451f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%486])
  in_scale: 0.000144282f /* ty=float32 */
  out_scale: 2.62059e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%492])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.62059e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%498])
  in_scale: 2.62059e-09f /* ty=float32 */
  out_scale: 2.51529e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%504])
  in_scale: 2.51529e-09f /* ty=float32 */
  out_scale: 2.51529e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%510])
  in_scale: 2.51529e-09f /* ty=float32 */
  out_scale: 0.0492601f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%516])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00842409f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%522])
  in_scale: 0.000414972f /* ty=float32 */
  out_scale: 1.01056e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%528])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.01056e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%534])
  in_scale: 1.01056e-09f /* ty=float32 */
  out_scale: 2.50189e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%510])
  in_scale: 2.51529e-09f /* ty=float32 */
  out_scale: 0.0492601f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%545])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00270556f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%551])
  in_scale: 0.000133276f /* ty=float32 */
  out_scale: 1.62268e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%557])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.62268e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%563])
  in_scale: 1.62268e-09f /* ty=float32 */
  out_scale: 1.62268e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%569])
  in_scale: 1.62268e-09f /* ty=float32 */
  out_scale: 0.0264715f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%575])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0102217f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%581])
  in_scale: 0.000270584f /* ty=float32 */
  out_scale: 2.40938e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%587])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.40938e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%593])
  in_scale: 2.40938e-09f /* ty=float32 */
  out_scale: 2.50189e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%599])
  in_scale: 2.50189e-09f /* ty=float32 */
  out_scale: 2.50189e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%605])
  in_scale: 2.50189e-09f /* ty=float32 */
  out_scale: 1.5472e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%605])
  in_scale: 2.50189e-09f /* ty=float32 */
  out_scale: 0.0508307f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%616])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00196013f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%622])
  in_scale: 9.96347e-05f /* ty=float32 */
  out_scale: 1.68877e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%628])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.68877e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%634])
  in_scale: 1.68877e-09f /* ty=float32 */
  out_scale: 1.68877e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%640])
  in_scale: 1.68877e-09f /* ty=float32 */
  out_scale: 0.0184405f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%646])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0377152f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%652])
  in_scale: 0.000695488f /* ty=float32 */
  out_scale: 1.52445e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%658])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.52445e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%664])
  in_scale: 1.52445e-08f /* ty=float32 */
  out_scale: 1.5472e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%670])
  in_scale: 1.5472e-08f /* ty=float32 */
  out_scale: 1.5472e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%676])
  in_scale: 1.5472e-08f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.global_avg_pool2d[%682])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.batch_flatten[%688])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%694])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.dense[%700])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%706])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%712])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
DEBUG:root:simulated graph
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.simulated_quantize(%data, 1f /* ty=float32 */, 0.0205693f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 3, 224, 224), float32] */;
  %1 = nn.simulated_quantize(meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, 1f /* ty=float32 */, 0.00173475f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %2 = nn.conv2d(%0, %1, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.simulated_quantize(%2, 3.56826e-05f /* ty=float32 */, 2.02595e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %4 = nn.simulated_quantize(meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 2.02595e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %5 = add(%3, %4) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %6 = nn.simulated_quantize(%5, 2.02595e-09f /* ty=float32 */, 2.02595e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %8 = nn.simulated_quantize(%7, 2.02595e-09f /* ty=float32 */, 2.06421e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.simulated_quantize(%9, 2.06421e-09f /* ty=float32 */, 4.90579e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.simulated_quantize(%9, 2.06421e-09f /* ty=float32 */, 0.0346317f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %12 = nn.simulated_quantize(meta[relay.Constant][2] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00426196f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %13 = nn.conv2d(%11, %12, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %14 = nn.simulated_quantize(%13, 0.000147599f /* ty=float32 */, 4.43585e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %15 = nn.simulated_quantize(meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 4.43585e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %17 = nn.simulated_quantize(%16, 4.43585e-09f /* ty=float32 */, 4.43585e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %18 = nn.relu(%17) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %19 = nn.simulated_quantize(%18, 4.43585e-09f /* ty=float32 */, 0.0296729f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %20 = nn.simulated_quantize(meta[relay.Constant][4] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00841788f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %21 = nn.conv2d(%19, %20, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %22 = nn.simulated_quantize(%21, 0.000249783f /* ty=float32 */, 5.19766e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %23 = nn.simulated_quantize(meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 5.19766e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %24 = add(%22, %23) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %25 = nn.simulated_quantize(%24, 5.19766e-09f /* ty=float32 */, 4.90579e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %26 = add(%10, %25) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %27 = nn.simulated_quantize(%26, 4.90579e-09f /* ty=float32 */, 4.90579e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %29 = nn.simulated_quantize(%28, 4.90579e-09f /* ty=float32 */, 2.67116e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %30 = nn.simulated_quantize(%28, 4.90579e-09f /* ty=float32 */, 0.0448146f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %31 = nn.simulated_quantize(meta[relay.Constant][6] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00189889f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %32 = nn.conv2d(%30, %31, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %33 = nn.simulated_quantize(%32, 8.50982e-05f /* ty=float32 */, 2.33478e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %34 = nn.simulated_quantize(meta[relay.Constant][7] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 2.33478e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %36 = nn.simulated_quantize(%35, 2.33478e-09f /* ty=float32 */, 2.33478e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %37 = nn.relu(%36) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %38 = nn.simulated_quantize(%37, 2.33478e-09f /* ty=float32 */, 0.0200992f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %39 = nn.simulated_quantize(meta[relay.Constant][8] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00832892f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %40 = nn.conv2d(%38, %39, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %41 = nn.simulated_quantize(%40, 0.000167405f /* ty=float32 */, 2.6477e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %42 = nn.simulated_quantize(meta[relay.Constant][9] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 2.6477e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %43 = add(%41, %42) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %44 = nn.simulated_quantize(%43, 2.6477e-09f /* ty=float32 */, 2.67116e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %45 = add(%29, %44) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %46 = nn.simulated_quantize(%45, 2.67116e-09f /* ty=float32 */, 2.67116e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %47 = nn.relu(%46) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %48 = nn.simulated_quantize(%47, 2.67116e-09f /* ty=float32 */, 0.0545867f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %49 = nn.simulated_quantize(meta[relay.Constant][10] /* ty=Tensor[(128, 64, 1, 1), float32] */ /* ty=Tensor[(128, 64, 1, 1), float32] */, 1f /* ty=float32 */, 0.00506192f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %50 = nn.conv2d(%48, %49, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %51 = nn.simulated_quantize(%50, 0.000276313f /* ty=float32 */, 1.40749e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %52 = nn.simulated_quantize(meta[relay.Constant][11] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.40749e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %53 = add(%51, %52) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %54 = nn.simulated_quantize(%53, 1.40749e-09f /* ty=float32 */, 2.06711e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %55 = nn.simulated_quantize(%47, 2.67116e-09f /* ty=float32 */, 0.0545867f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %56 = nn.simulated_quantize(meta[relay.Constant][12] /* ty=Tensor[(128, 64, 3, 3), float32] */ /* ty=Tensor[(128, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00148527f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %57 = nn.conv2d(%55, %56, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %58 = nn.simulated_quantize(%57, 8.1076e-05f /* ty=float32 */, 1.62313e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %59 = nn.simulated_quantize(meta[relay.Constant][13] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.62313e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %60 = add(%58, %59) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %61 = nn.simulated_quantize(%60, 1.62313e-09f /* ty=float32 */, 1.62313e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %63 = nn.simulated_quantize(%62, 1.62313e-09f /* ty=float32 */, 0.0239001f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %64 = nn.simulated_quantize(meta[relay.Constant][14] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 1f /* ty=float32 */, 0.00568667f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %65 = nn.conv2d(%63, %64, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %66 = nn.simulated_quantize(%65, 0.000135912f /* ty=float32 */, 2.17098e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %67 = nn.simulated_quantize(meta[relay.Constant][15] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 2.17098e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %68 = add(%66, %67) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %69 = nn.simulated_quantize(%68, 2.17098e-09f /* ty=float32 */, 2.06711e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %70 = add(%54, %69) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %71 = nn.simulated_quantize(%70, 2.06711e-09f /* ty=float32 */, 2.06711e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %72 = nn.relu(%71) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %73 = nn.simulated_quantize(%72, 2.06711e-09f /* ty=float32 */, 2.56583e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %74 = nn.simulated_quantize(%72, 2.06711e-09f /* ty=float32 */, 0.036975f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %75 = nn.simulated_quantize(meta[relay.Constant][16] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 1f /* ty=float32 */, 0.00387308f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %76 = nn.conv2d(%74, %75, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %77 = nn.simulated_quantize(%76, 0.000143207f /* ty=float32 */, 1.6456e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %78 = nn.simulated_quantize(meta[relay.Constant][17] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.6456e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %79 = add(%77, %78) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %80 = nn.simulated_quantize(%79, 1.6456e-09f /* ty=float32 */, 1.6456e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %81 = nn.relu(%80) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %82 = nn.simulated_quantize(%81, 1.6456e-09f /* ty=float32 */, 0.0268827f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %83 = nn.simulated_quantize(meta[relay.Constant][18] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 1f /* ty=float32 */, 0.00686348f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %84 = nn.conv2d(%82, %83, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %85 = nn.simulated_quantize(%84, 0.000184509f /* ty=float32 */, 2.47899e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %86 = nn.simulated_quantize(meta[relay.Constant][19] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 2.47899e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %87 = add(%85, %86) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %88 = nn.simulated_quantize(%87, 2.47899e-09f /* ty=float32 */, 2.56583e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %89 = add(%73, %88) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %90 = nn.simulated_quantize(%89, 2.56583e-09f /* ty=float32 */, 2.56583e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %92 = nn.simulated_quantize(%91, 2.56583e-09f /* ty=float32 */, 0.044912f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %93 = nn.simulated_quantize(meta[relay.Constant][20] /* ty=Tensor[(256, 128, 1, 1), float32] */ /* ty=Tensor[(256, 128, 1, 1), float32] */, 1f /* ty=float32 */, 0.00258671f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %94 = nn.conv2d(%92, %93, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %95 = nn.simulated_quantize(%94, 0.000116174f /* ty=float32 */, 8.13283e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %96 = nn.simulated_quantize(meta[relay.Constant][21] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 8.13283e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %97 = add(%95, %96) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %98 = nn.simulated_quantize(%97, 8.13283e-10f /* ty=float32 */, 2.27245e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %99 = nn.simulated_quantize(%91, 2.56583e-09f /* ty=float32 */, 0.044912f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %100 = nn.simulated_quantize(meta[relay.Constant][22] /* ty=Tensor[(256, 128, 3, 3), float32] */ /* ty=Tensor[(256, 128, 3, 3), float32] */, 1f /* ty=float32 */, 0.00172925f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %101 = nn.conv2d(%99, %100, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %102 = nn.simulated_quantize(%101, 7.7664e-05f /* ty=float32 */, 1.66371e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %103 = nn.simulated_quantize(meta[relay.Constant][23] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.66371e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %104 = add(%102, %103) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %105 = nn.simulated_quantize(%104, 1.66371e-09f /* ty=float32 */, 1.66371e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %106 = nn.relu(%105) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %107 = nn.simulated_quantize(%106, 1.66371e-09f /* ty=float32 */, 0.0239336f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %108 = nn.simulated_quantize(meta[relay.Constant][24] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00554771f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %109 = nn.conv2d(%107, %108, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %110 = nn.simulated_quantize(%109, 0.000132777f /* ty=float32 */, 2.21055e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %111 = nn.simulated_quantize(meta[relay.Constant][25] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 2.21055e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %112 = add(%110, %111) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %113 = nn.simulated_quantize(%112, 2.21055e-09f /* ty=float32 */, 2.27245e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %114 = add(%98, %113) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %115 = nn.simulated_quantize(%114, 2.27245e-09f /* ty=float32 */, 2.27245e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %116 = nn.relu(%115) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %117 = nn.simulated_quantize(%116, 2.27245e-09f /* ty=float32 */, 2.51529e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %118 = nn.simulated_quantize(%116, 2.27245e-09f /* ty=float32 */, 0.0412652f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %119 = nn.simulated_quantize(meta[relay.Constant][26] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00269224f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %120 = nn.conv2d(%118, %119, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %121 = nn.simulated_quantize(%120, 0.000111096f /* ty=float32 */, 1.86234e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %122 = nn.simulated_quantize(meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.86234e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %123 = add(%121, %122) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %124 = nn.simulated_quantize(%123, 1.86234e-09f /* ty=float32 */, 1.86234e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %125 = nn.relu(%124) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %126 = nn.simulated_quantize(%125, 1.86234e-09f /* ty=float32 */, 0.0263072f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %127 = nn.simulated_quantize(meta[relay.Constant][28] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00548451f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %129 = nn.simulated_quantize(%128, 0.000144282f /* ty=float32 */, 2.62059e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %130 = nn.simulated_quantize(meta[relay.Constant][29] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 2.62059e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %131 = add(%129, %130) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %132 = nn.simulated_quantize(%131, 2.62059e-09f /* ty=float32 */, 2.51529e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %133 = add(%117, %132) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %134 = nn.simulated_quantize(%133, 2.51529e-09f /* ty=float32 */, 2.51529e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %135 = nn.relu(%134) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %136 = nn.simulated_quantize(%135, 2.51529e-09f /* ty=float32 */, 0.0492601f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %137 = nn.simulated_quantize(meta[relay.Constant][30] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00842409f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %138 = nn.conv2d(%136, %137, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %139 = nn.simulated_quantize(%138, 0.000414972f /* ty=float32 */, 1.01056e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %140 = nn.simulated_quantize(meta[relay.Constant][31] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.01056e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %141 = add(%139, %140) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %142 = nn.simulated_quantize(%141, 1.01056e-09f /* ty=float32 */, 2.50189e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %143 = nn.simulated_quantize(%135, 2.51529e-09f /* ty=float32 */, 0.0492601f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %144 = nn.simulated_quantize(meta[relay.Constant][32] /* ty=Tensor[(512, 256, 3, 3), float32] */ /* ty=Tensor[(512, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00270556f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %145 = nn.conv2d(%143, %144, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %146 = nn.simulated_quantize(%145, 0.000133276f /* ty=float32 */, 1.62268e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %147 = nn.simulated_quantize(meta[relay.Constant][33] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.62268e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %148 = add(%146, %147) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %149 = nn.simulated_quantize(%148, 1.62268e-09f /* ty=float32 */, 1.62268e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %150 = nn.relu(%149) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %151 = nn.simulated_quantize(%150, 1.62268e-09f /* ty=float32 */, 0.0264715f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %152 = nn.simulated_quantize(meta[relay.Constant][34] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 1f /* ty=float32 */, 0.0102217f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %153 = nn.conv2d(%151, %152, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %154 = nn.simulated_quantize(%153, 0.000270584f /* ty=float32 */, 2.40938e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %155 = nn.simulated_quantize(meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 2.40938e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %156 = add(%154, %155) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %157 = nn.simulated_quantize(%156, 2.40938e-09f /* ty=float32 */, 2.50189e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %158 = add(%142, %157) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %159 = nn.simulated_quantize(%158, 2.50189e-09f /* ty=float32 */, 2.50189e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %161 = nn.simulated_quantize(%160, 2.50189e-09f /* ty=float32 */, 1.5472e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %162 = nn.simulated_quantize(%160, 2.50189e-09f /* ty=float32 */, 0.0508307f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %163 = nn.simulated_quantize(meta[relay.Constant][36] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 1f /* ty=float32 */, 0.00196013f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %164 = nn.conv2d(%162, %163, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %165 = nn.simulated_quantize(%164, 9.96347e-05f /* ty=float32 */, 1.68877e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %166 = nn.simulated_quantize(meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.68877e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %167 = add(%165, %166) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %168 = nn.simulated_quantize(%167, 1.68877e-09f /* ty=float32 */, 1.68877e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %169 = nn.relu(%168) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %170 = nn.simulated_quantize(%169, 1.68877e-09f /* ty=float32 */, 0.0184405f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %171 = nn.simulated_quantize(meta[relay.Constant][38] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 1f /* ty=float32 */, 0.0377152f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %172 = nn.conv2d(%170, %171, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %173 = nn.simulated_quantize(%172, 0.000695488f /* ty=float32 */, 1.52445e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %174 = nn.simulated_quantize(meta[relay.Constant][39] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.52445e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %175 = add(%173, %174) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %176 = nn.simulated_quantize(%175, 1.52445e-08f /* ty=float32 */, 1.5472e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %177 = add(%161, %176) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %178 = nn.simulated_quantize(%177, 1.5472e-08f /* ty=float32 */, 1.5472e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %180 = nn.simulated_quantize(%179, 1.5472e-08f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %181 = nn.global_avg_pool2d(%180) /* ty=Tensor[(32, 512, 1, 1), float32] */;
  %182 = nn.simulated_quantize(%181, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 512, 1, 1), float32] */;
  %183 = nn.batch_flatten(%182) /* ty=Tensor[(32, 512), float32] */;
  %184 = nn.simulated_quantize(%183, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 512), float32] */;
  %185 = nn.simulated_quantize(meta[relay.Constant][40] /* ty=Tensor[(1000, 512), float32] */ /* ty=Tensor[(1000, 512), float32] */, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000, 512), float32] */;
  %186 = nn.dense(%184, %185, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  %187 = nn.simulated_quantize(%186, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */;
  %188 = nn.simulated_quantize(meta[relay.Constant][41] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000), float32] */;
  %189 = add(%187, %188) /* ty=Tensor[(32, 1000), float32] */;
  nn.simulated_quantize(%189, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:root:quantize graph
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = qnn.quantize(%data, 0.0205693f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 3, 224, 224), int8] */;
  %1 = qnn.quantize(meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, 0.00173475f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 3, 7, 7), int8] */;
  %2 = nn.conv2d(%0, %1, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7], out_dtype="int32") /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %3 = qnn.requantize(%2, 3.56826e-05f /* ty=float32 */, 0 /* ty=int32 */, 2.02595e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %4 = qnn.quantize(meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 2.02595e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %5 = add(%3, %4) /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %6 = qnn.requantize(%5, 2.02595e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.02595e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %8 = qnn.requantize(%7, 2.02595e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.06421e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %10 = qnn.requantize(%9, 2.06421e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.90579e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %11 = qnn.requantize(%9, 2.06421e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0346317f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %12 = qnn.quantize(meta[relay.Constant][2] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 0.00426196f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 64, 3, 3), int8] */;
  %13 = nn.conv2d(%11, %12, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %14 = qnn.requantize(%13, 0.000147599f /* ty=float32 */, 0 /* ty=int32 */, 4.43585e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %15 = qnn.quantize(meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 4.43585e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %16 = add(%14, %15) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %17 = qnn.requantize(%16, 4.43585e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.43585e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %18 = nn.relu(%17) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %19 = qnn.requantize(%18, 4.43585e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0296729f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %20 = qnn.quantize(meta[relay.Constant][4] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 0.00841788f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 64, 3, 3), int8] */;
  %21 = nn.conv2d(%19, %20, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %22 = qnn.requantize(%21, 0.000249783f /* ty=float32 */, 0 /* ty=int32 */, 5.19766e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %23 = qnn.quantize(meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 5.19766e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %24 = add(%22, %23) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %25 = qnn.requantize(%24, 5.19766e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.90579e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %26 = add(%10, %25) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %27 = qnn.requantize(%26, 4.90579e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.90579e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %29 = qnn.requantize(%28, 4.90579e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.67116e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %30 = qnn.requantize(%28, 4.90579e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0448146f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %31 = qnn.quantize(meta[relay.Constant][6] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 0.00189889f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 64, 3, 3), int8] */;
  %32 = nn.conv2d(%30, %31, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %33 = qnn.requantize(%32, 8.50982e-05f /* ty=float32 */, 0 /* ty=int32 */, 2.33478e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %34 = qnn.quantize(meta[relay.Constant][7] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 2.33478e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %35 = add(%33, %34) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %36 = qnn.requantize(%35, 2.33478e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.33478e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %37 = nn.relu(%36) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %38 = qnn.requantize(%37, 2.33478e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0200992f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %39 = qnn.quantize(meta[relay.Constant][8] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 0.00832892f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 64, 3, 3), int8] */;
  %40 = nn.conv2d(%38, %39, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %41 = qnn.requantize(%40, 0.000167405f /* ty=float32 */, 0 /* ty=int32 */, 2.6477e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %42 = qnn.quantize(meta[relay.Constant][9] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 2.6477e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %43 = add(%41, %42) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %44 = qnn.requantize(%43, 2.6477e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.67116e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %45 = add(%29, %44) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %46 = qnn.requantize(%45, 2.67116e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.67116e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %47 = nn.relu(%46) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %48 = qnn.requantize(%47, 2.67116e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0545867f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %49 = qnn.quantize(meta[relay.Constant][10] /* ty=Tensor[(128, 64, 1, 1), float32] */ /* ty=Tensor[(128, 64, 1, 1), float32] */, 0.00506192f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 64, 1, 1), int8] */;
  %50 = nn.conv2d(%48, %49, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %51 = qnn.requantize(%50, 0.000276313f /* ty=float32 */, 0 /* ty=int32 */, 1.40749e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %52 = qnn.quantize(meta[relay.Constant][11] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.40749e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %53 = add(%51, %52) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %54 = qnn.requantize(%53, 1.40749e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.06711e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %55 = qnn.requantize(%47, 2.67116e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0545867f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %56 = qnn.quantize(meta[relay.Constant][12] /* ty=Tensor[(128, 64, 3, 3), float32] */ /* ty=Tensor[(128, 64, 3, 3), float32] */, 0.00148527f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 64, 3, 3), int8] */;
  %57 = nn.conv2d(%55, %56, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %58 = qnn.requantize(%57, 8.1076e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.62313e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %59 = qnn.quantize(meta[relay.Constant][13] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.62313e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %60 = add(%58, %59) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %61 = qnn.requantize(%60, 1.62313e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.62313e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %63 = qnn.requantize(%62, 1.62313e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0239001f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %64 = qnn.quantize(meta[relay.Constant][14] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 0.00568667f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 3, 3), int8] */;
  %65 = nn.conv2d(%63, %64, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %66 = qnn.requantize(%65, 0.000135912f /* ty=float32 */, 0 /* ty=int32 */, 2.17098e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %67 = qnn.quantize(meta[relay.Constant][15] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 2.17098e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %68 = add(%66, %67) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %69 = qnn.requantize(%68, 2.17098e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.06711e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %70 = add(%54, %69) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %71 = qnn.requantize(%70, 2.06711e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.06711e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %72 = nn.relu(%71) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %73 = qnn.requantize(%72, 2.06711e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.56583e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %74 = qnn.requantize(%72, 2.06711e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.036975f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %75 = qnn.quantize(meta[relay.Constant][16] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 0.00387308f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 3, 3), int8] */;
  %76 = nn.conv2d(%74, %75, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %77 = qnn.requantize(%76, 0.000143207f /* ty=float32 */, 0 /* ty=int32 */, 1.6456e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %78 = qnn.quantize(meta[relay.Constant][17] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.6456e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %79 = add(%77, %78) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %80 = qnn.requantize(%79, 1.6456e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.6456e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %81 = nn.relu(%80) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %82 = qnn.requantize(%81, 1.6456e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0268827f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %83 = qnn.quantize(meta[relay.Constant][18] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 0.00686348f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 3, 3), int8] */;
  %84 = nn.conv2d(%82, %83, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %85 = qnn.requantize(%84, 0.000184509f /* ty=float32 */, 0 /* ty=int32 */, 2.47899e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %86 = qnn.quantize(meta[relay.Constant][19] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 2.47899e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %87 = add(%85, %86) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %88 = qnn.requantize(%87, 2.47899e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.56583e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %89 = add(%73, %88) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %90 = qnn.requantize(%89, 2.56583e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.56583e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %92 = qnn.requantize(%91, 2.56583e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.044912f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %93 = qnn.quantize(meta[relay.Constant][20] /* ty=Tensor[(256, 128, 1, 1), float32] */ /* ty=Tensor[(256, 128, 1, 1), float32] */, 0.00258671f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 128, 1, 1), int8] */;
  %94 = nn.conv2d(%92, %93, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %95 = qnn.requantize(%94, 0.000116174f /* ty=float32 */, 0 /* ty=int32 */, 8.13283e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %96 = qnn.quantize(meta[relay.Constant][21] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 8.13283e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %97 = add(%95, %96) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %98 = qnn.requantize(%97, 8.13283e-10f /* ty=float32 */, 0 /* ty=int32 */, 2.27245e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %99 = qnn.requantize(%91, 2.56583e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.044912f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %100 = qnn.quantize(meta[relay.Constant][22] /* ty=Tensor[(256, 128, 3, 3), float32] */ /* ty=Tensor[(256, 128, 3, 3), float32] */, 0.00172925f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 128, 3, 3), int8] */;
  %101 = nn.conv2d(%99, %100, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %102 = qnn.requantize(%101, 7.7664e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.66371e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %103 = qnn.quantize(meta[relay.Constant][23] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.66371e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %104 = add(%102, %103) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %105 = qnn.requantize(%104, 1.66371e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.66371e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %106 = nn.relu(%105) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %107 = qnn.requantize(%106, 1.66371e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0239336f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %108 = qnn.quantize(meta[relay.Constant][24] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 0.00554771f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 256, 3, 3), int8] */;
  %109 = nn.conv2d(%107, %108, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %110 = qnn.requantize(%109, 0.000132777f /* ty=float32 */, 0 /* ty=int32 */, 2.21055e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %111 = qnn.quantize(meta[relay.Constant][25] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 2.21055e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %112 = add(%110, %111) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %113 = qnn.requantize(%112, 2.21055e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.27245e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %114 = add(%98, %113) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %115 = qnn.requantize(%114, 2.27245e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.27245e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %116 = nn.relu(%115) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %117 = qnn.requantize(%116, 2.27245e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.51529e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %118 = qnn.requantize(%116, 2.27245e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0412652f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %119 = qnn.quantize(meta[relay.Constant][26] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 0.00269224f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 256, 3, 3), int8] */;
  %120 = nn.conv2d(%118, %119, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %121 = qnn.requantize(%120, 0.000111096f /* ty=float32 */, 0 /* ty=int32 */, 1.86234e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %122 = qnn.quantize(meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.86234e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %123 = add(%121, %122) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %124 = qnn.requantize(%123, 1.86234e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.86234e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %125 = nn.relu(%124) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %126 = qnn.requantize(%125, 1.86234e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0263072f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %127 = qnn.quantize(meta[relay.Constant][28] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 0.00548451f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 256, 3, 3), int8] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %129 = qnn.requantize(%128, 0.000144282f /* ty=float32 */, 0 /* ty=int32 */, 2.62059e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %130 = qnn.quantize(meta[relay.Constant][29] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 2.62059e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %131 = add(%129, %130) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %132 = qnn.requantize(%131, 2.62059e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.51529e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %133 = add(%117, %132) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %134 = qnn.requantize(%133, 2.51529e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.51529e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %135 = nn.relu(%134) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %136 = qnn.requantize(%135, 2.51529e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0492601f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %137 = qnn.quantize(meta[relay.Constant][30] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, 0.00842409f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 256, 1, 1), int8] */;
  %138 = nn.conv2d(%136, %137, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %139 = qnn.requantize(%138, 0.000414972f /* ty=float32 */, 0 /* ty=int32 */, 1.01056e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %140 = qnn.quantize(meta[relay.Constant][31] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.01056e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %141 = add(%139, %140) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %142 = qnn.requantize(%141, 1.01056e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.50189e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %143 = qnn.requantize(%135, 2.51529e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0492601f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %144 = qnn.quantize(meta[relay.Constant][32] /* ty=Tensor[(512, 256, 3, 3), float32] */ /* ty=Tensor[(512, 256, 3, 3), float32] */, 0.00270556f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 256, 3, 3), int8] */;
  %145 = nn.conv2d(%143, %144, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %146 = qnn.requantize(%145, 0.000133276f /* ty=float32 */, 0 /* ty=int32 */, 1.62268e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %147 = qnn.quantize(meta[relay.Constant][33] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.62268e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %148 = add(%146, %147) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %149 = qnn.requantize(%148, 1.62268e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.62268e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %150 = nn.relu(%149) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %151 = qnn.requantize(%150, 1.62268e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0264715f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 7, 7), int8] */;
  %152 = qnn.quantize(meta[relay.Constant][34] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 0.0102217f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 512, 3, 3), int8] */;
  %153 = nn.conv2d(%151, %152, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %154 = qnn.requantize(%153, 0.000270584f /* ty=float32 */, 0 /* ty=int32 */, 2.40938e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %155 = qnn.quantize(meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 2.40938e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %156 = add(%154, %155) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %157 = qnn.requantize(%156, 2.40938e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.50189e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %158 = add(%142, %157) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %159 = qnn.requantize(%158, 2.50189e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.50189e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %161 = qnn.requantize(%160, 2.50189e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.5472e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %162 = qnn.requantize(%160, 2.50189e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0508307f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 7, 7), int8] */;
  %163 = qnn.quantize(meta[relay.Constant][36] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 0.00196013f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 512, 3, 3), int8] */;
  %164 = nn.conv2d(%162, %163, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %165 = qnn.requantize(%164, 9.96347e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.68877e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %166 = qnn.quantize(meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.68877e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %167 = add(%165, %166) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %168 = qnn.requantize(%167, 1.68877e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.68877e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %169 = nn.relu(%168) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %170 = qnn.requantize(%169, 1.68877e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0184405f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 7, 7), int8] */;
  %171 = qnn.quantize(meta[relay.Constant][38] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 0.0377152f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 512, 3, 3), int8] */;
  %172 = nn.conv2d(%170, %171, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %173 = qnn.requantize(%172, 0.000695488f /* ty=float32 */, 0 /* ty=int32 */, 1.52445e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %174 = qnn.quantize(meta[relay.Constant][39] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.52445e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %175 = add(%173, %174) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %176 = qnn.requantize(%175, 1.52445e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.5472e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %177 = add(%161, %176) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %178 = qnn.requantize(%177, 1.5472e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.5472e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %180 = qnn.dequantize(%179, 1.5472e-08f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %181 = nn.global_avg_pool2d(%180) /* ty=Tensor[(32, 512, 1, 1), float32] */;
  %182 = nn.batch_flatten(%181) /* ty=Tensor[(32, 512), float32] */;
  %183 = nn.dense(%182, meta[relay.Constant][40] /* ty=Tensor[(1000, 512), float32] */ /* ty=Tensor[(1000, 512), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%183, meta[relay.Constant][41] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op nn.pad
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 4, 224, 224), 'int8'), ('TENSOR', (64, 4, 7, 7), 'int8'), (2, 2), (3, 3, 3, 3), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 64, 56, 56), 'int8'), ('TENSOR', (64, 64, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 64, 56, 56), 'int8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 64, 56, 56), 'int8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 128, 28, 28), 'int8'), ('TENSOR', (128, 128, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 128, 28, 28), 'int8'), ('TENSOR', (256, 128, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 128, 28, 28), 'int8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 256, 14, 14), 'int8'), ('TENSOR', (256, 256, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 256, 14, 14), 'int8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 256, 14, 14), 'int8'), ('TENSOR', (512, 256, 3, 3), 'int8'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 512, 7, 7), 'int8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.pad
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op right_shift
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
learning_based_quantize.py:100: DeprecationWarning: legacy graph runtime behaviour of producing json / lib / params will be removed in the next release 
  graph, lib, params = relay.build(mod, target)
INFO:root:[3200 samples] validation: acc-top1=0.705000 acc-top5=0.908438
INFO:root:[6400 samples] validation: acc-top1=0.707656 acc-top5=0.902188
INFO:root:[9600 samples] validation: acc-top1=0.707812 acc-top5=0.899687
INFO:root:[12800 samples] validation: acc-top1=0.705078 acc-top5=0.896563
INFO:root:[16000 samples] validation: acc-top1=0.705688 acc-top5=0.896500
INFO:root:[19200 samples] validation: acc-top1=0.703594 acc-top5=0.895625
INFO:root:[22400 samples] validation: acc-top1=0.704107 acc-top5=0.895223
INFO:root:[25600 samples] validation: acc-top1=0.703750 acc-top5=0.894844
INFO:root:[28800 samples] validation: acc-top1=0.703472 acc-top5=0.894861
INFO:root:[32000 samples] validation: acc-top1=0.702906 acc-top5=0.894344
INFO:root:[35200 samples] validation: acc-top1=0.703182 acc-top5=0.894347
INFO:root:[38400 samples] validation: acc-top1=0.703724 acc-top5=0.894557
INFO:root:[41600 samples] validation: acc-top1=0.703005 acc-top5=0.894447
INFO:root:[44800 samples] validation: acc-top1=0.702455 acc-top5=0.894665
INFO:root:[48000 samples] validation: acc-top1=0.703187 acc-top5=0.894854
INFO:root:[final] validation: acc-top1=0.703187 acc-top5=0.894854
Final accuracy int8 0.7031875
[18:48:41] src/io/iter_image_recordio_2.cc:178: ImageRecordIOParser2: /home/woongkyu/imagenet/val.rec, use 4 threads for decoding..
INFO:root:Model file not found. Downloading to /home/woongkyu/.mxnet/models/resnet50_v1-0aee57f9.params.
Downloading /home/woongkyu/.mxnet/models/resnet50_v1-0aee57f9.zipab64add9-0558-4950-80bd-67a57a9e88e4 from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet50_v1-0aee57f9.zip...
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443 "GET /gluon/models/resnet50_v1-0aee57f9.zip HTTP/1.1" 200 94999620
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
DEBUG:root:original
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %4 = nn.conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(64, 64, 1, 1), float32] */ /* ty=Tensor[(64, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %5 = add(%4, meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %6 = add(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%7, meta[relay.Constant][5] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %9 = add(%8, meta[relay.Constant][6] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.relu(%9) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.conv2d(%10, meta[relay.Constant][7] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %12 = add(%11, meta[relay.Constant][8] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %13 = add(%12, meta[relay.Constant][9] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %14 = nn.conv2d(%3, meta[relay.Constant][10] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %15 = add(%14, meta[relay.Constant][11] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %16 = add(%13, %15) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %17 = nn.relu(%16) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %18 = nn.conv2d(%17, meta[relay.Constant][12] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %19 = add(%18, meta[relay.Constant][13] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %20 = add(%19, meta[relay.Constant][14] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %21 = nn.relu(%20) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, meta[relay.Constant][15] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %23 = add(%22, meta[relay.Constant][16] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %24 = nn.relu(%23) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %25 = nn.conv2d(%24, meta[relay.Constant][17] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %26 = add(%25, meta[relay.Constant][18] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %27 = add(%26, meta[relay.Constant][19] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %28 = add(%27, %17) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %29 = nn.relu(%28) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %30 = nn.conv2d(%29, meta[relay.Constant][20] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %31 = add(%30, meta[relay.Constant][21] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %32 = add(%31, meta[relay.Constant][22] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %34 = nn.conv2d(%33, meta[relay.Constant][23] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %35 = add(%34, meta[relay.Constant][24] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %36 = nn.relu(%35) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %37 = nn.conv2d(%36, meta[relay.Constant][25] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %38 = add(%37, meta[relay.Constant][26] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %39 = add(%38, meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %40 = add(%39, %29) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %42 = nn.conv2d(%41, meta[relay.Constant][28] /* ty=Tensor[(128, 256, 1, 1), float32] */ /* ty=Tensor[(128, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %43 = add(%42, meta[relay.Constant][29] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %44 = add(%43, meta[relay.Constant][30] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %45 = nn.relu(%44) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %46 = nn.conv2d(%45, meta[relay.Constant][31] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %47 = add(%46, meta[relay.Constant][32] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %48 = nn.relu(%47) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %49 = nn.conv2d(%48, meta[relay.Constant][33] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %50 = add(%49, meta[relay.Constant][34] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %51 = add(%50, meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %52 = nn.conv2d(%41, meta[relay.Constant][36] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %53 = add(%52, meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %54 = add(%51, %53) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %55 = nn.relu(%54) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %56 = nn.conv2d(%55, meta[relay.Constant][38] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %57 = add(%56, meta[relay.Constant][39] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %58 = add(%57, meta[relay.Constant][40] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %59 = nn.relu(%58) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %60 = nn.conv2d(%59, meta[relay.Constant][41] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %61 = add(%60, meta[relay.Constant][42] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %63 = nn.conv2d(%62, meta[relay.Constant][43] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %64 = add(%63, meta[relay.Constant][44] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %65 = add(%64, meta[relay.Constant][45] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %66 = add(%65, %55) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %68 = nn.conv2d(%67, meta[relay.Constant][46] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %69 = add(%68, meta[relay.Constant][47] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %70 = add(%69, meta[relay.Constant][48] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %71 = nn.relu(%70) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %72 = nn.conv2d(%71, meta[relay.Constant][49] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %73 = add(%72, meta[relay.Constant][50] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %74 = nn.relu(%73) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %75 = nn.conv2d(%74, meta[relay.Constant][51] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %76 = add(%75, meta[relay.Constant][52] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %77 = add(%76, meta[relay.Constant][53] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %78 = add(%77, %67) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %79 = nn.relu(%78) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %80 = nn.conv2d(%79, meta[relay.Constant][54] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %81 = add(%80, meta[relay.Constant][55] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %82 = add(%81, meta[relay.Constant][56] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %83 = nn.relu(%82) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %84 = nn.conv2d(%83, meta[relay.Constant][57] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %85 = add(%84, meta[relay.Constant][58] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %86 = nn.relu(%85) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %87 = nn.conv2d(%86, meta[relay.Constant][59] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %88 = add(%87, meta[relay.Constant][60] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %89 = add(%88, meta[relay.Constant][61] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %90 = add(%89, %79) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %92 = nn.conv2d(%91, meta[relay.Constant][62] /* ty=Tensor[(256, 512, 1, 1), float32] */ /* ty=Tensor[(256, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %93 = add(%92, meta[relay.Constant][63] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %94 = add(%93, meta[relay.Constant][64] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %95 = nn.relu(%94) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %96 = nn.conv2d(%95, meta[relay.Constant][65] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %97 = add(%96, meta[relay.Constant][66] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %98 = nn.relu(%97) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %99 = nn.conv2d(%98, meta[relay.Constant][67] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %100 = add(%99, meta[relay.Constant][68] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %101 = add(%100, meta[relay.Constant][69] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %102 = nn.conv2d(%91, meta[relay.Constant][70] /* ty=Tensor[(1024, 512, 1, 1), float32] */ /* ty=Tensor[(1024, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %103 = add(%102, meta[relay.Constant][71] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %104 = add(%101, %103) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %105 = nn.relu(%104) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %106 = nn.conv2d(%105, meta[relay.Constant][72] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %107 = add(%106, meta[relay.Constant][73] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %108 = add(%107, meta[relay.Constant][74] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %109 = nn.relu(%108) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %110 = nn.conv2d(%109, meta[relay.Constant][75] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %111 = add(%110, meta[relay.Constant][76] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %112 = nn.relu(%111) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %113 = nn.conv2d(%112, meta[relay.Constant][77] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %114 = add(%113, meta[relay.Constant][78] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %115 = add(%114, meta[relay.Constant][79] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %116 = add(%115, %105) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %117 = nn.relu(%116) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %118 = nn.conv2d(%117, meta[relay.Constant][80] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %119 = add(%118, meta[relay.Constant][81] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %120 = add(%119, meta[relay.Constant][82] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %121 = nn.relu(%120) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %122 = nn.conv2d(%121, meta[relay.Constant][83] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %123 = add(%122, meta[relay.Constant][84] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %125 = nn.conv2d(%124, meta[relay.Constant][85] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %126 = add(%125, meta[relay.Constant][86] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %127 = add(%126, meta[relay.Constant][87] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %128 = add(%127, %117) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %129 = nn.relu(%128) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %130 = nn.conv2d(%129, meta[relay.Constant][88] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %131 = add(%130, meta[relay.Constant][89] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %132 = add(%131, meta[relay.Constant][90] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %133 = nn.relu(%132) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %134 = nn.conv2d(%133, meta[relay.Constant][91] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %135 = add(%134, meta[relay.Constant][92] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %136 = nn.relu(%135) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %137 = nn.conv2d(%136, meta[relay.Constant][93] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %138 = add(%137, meta[relay.Constant][94] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %139 = add(%138, meta[relay.Constant][95] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %140 = add(%139, %129) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %141 = nn.relu(%140) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %142 = nn.conv2d(%141, meta[relay.Constant][96] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %143 = add(%142, meta[relay.Constant][97] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %144 = add(%143, meta[relay.Constant][98] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %145 = nn.relu(%144) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %146 = nn.conv2d(%145, meta[relay.Constant][99] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %147 = add(%146, meta[relay.Constant][100] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %148 = nn.relu(%147) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %149 = nn.conv2d(%148, meta[relay.Constant][101] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %150 = add(%149, meta[relay.Constant][102] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %151 = add(%150, meta[relay.Constant][103] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %152 = add(%151, %141) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %153 = nn.relu(%152) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %154 = nn.conv2d(%153, meta[relay.Constant][104] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %155 = add(%154, meta[relay.Constant][105] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %156 = add(%155, meta[relay.Constant][106] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %158 = nn.conv2d(%157, meta[relay.Constant][107] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %159 = add(%158, meta[relay.Constant][108] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %161 = nn.conv2d(%160, meta[relay.Constant][109] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %162 = add(%161, meta[relay.Constant][110] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %163 = add(%162, meta[relay.Constant][111] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %164 = add(%163, %153) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %165 = nn.relu(%164) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %166 = nn.conv2d(%165, meta[relay.Constant][112] /* ty=Tensor[(512, 1024, 1, 1), float32] */ /* ty=Tensor[(512, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %167 = add(%166, meta[relay.Constant][113] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %168 = add(%167, meta[relay.Constant][114] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %169 = nn.relu(%168) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %170 = nn.conv2d(%169, meta[relay.Constant][115] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %171 = add(%170, meta[relay.Constant][116] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %173 = nn.conv2d(%172, meta[relay.Constant][117] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %174 = add(%173, meta[relay.Constant][118] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %175 = add(%174, meta[relay.Constant][119] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %176 = nn.conv2d(%165, meta[relay.Constant][120] /* ty=Tensor[(2048, 1024, 1, 1), float32] */ /* ty=Tensor[(2048, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %177 = add(%176, meta[relay.Constant][121] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %178 = add(%175, %177) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %180 = nn.conv2d(%179, meta[relay.Constant][122] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %181 = add(%180, meta[relay.Constant][123] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %182 = add(%181, meta[relay.Constant][124] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %183 = nn.relu(%182) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %184 = nn.conv2d(%183, meta[relay.Constant][125] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %185 = add(%184, meta[relay.Constant][126] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %186 = nn.relu(%185) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %187 = nn.conv2d(%186, meta[relay.Constant][127] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %188 = add(%187, meta[relay.Constant][128] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %189 = add(%188, meta[relay.Constant][129] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %190 = add(%189, %179) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %191 = nn.relu(%190) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %192 = nn.conv2d(%191, meta[relay.Constant][130] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %193 = add(%192, meta[relay.Constant][131] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %194 = add(%193, meta[relay.Constant][132] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %195 = nn.relu(%194) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %196 = nn.conv2d(%195, meta[relay.Constant][133] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %197 = add(%196, meta[relay.Constant][134] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %198 = nn.relu(%197) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %199 = nn.conv2d(%198, meta[relay.Constant][135] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %200 = add(%199, meta[relay.Constant][136] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %201 = add(%200, meta[relay.Constant][137] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %202 = add(%201, %191) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %203 = nn.relu(%202) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %204 = nn.global_avg_pool2d(%203) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %205 = nn.batch_flatten(%204) /* ty=Tensor[(32, 2048), float32] */;
  %206 = nn.dense(%205, meta[relay.Constant][138] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%206, meta[relay.Constant][139] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:autotvm:Finish loading 688 records
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 64, 56, 56), 'float32'), ('TENSOR', (64, 64, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 64, 56, 56), 'float32'), ('TENSOR', (256, 64, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 256, 56, 56), 'float32'), ('TENSOR', (64, 256, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 256, 56, 56), 'float32'), ('TENSOR', (128, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 128, 28, 28), 'float32'), ('TENSOR', (512, 128, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 256, 56, 56), 'float32'), ('TENSOR', (512, 256, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 512, 28, 28), 'float32'), ('TENSOR', (128, 512, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 512, 28, 28), 'float32'), ('TENSOR', (256, 512, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 256, 14, 14), 'float32'), ('TENSOR', (1024, 256, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 512, 28, 28), 'float32'), ('TENSOR', (1024, 512, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 1024, 14, 14), 'float32'), ('TENSOR', (256, 1024, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 1024, 14, 14), 'float32'), ('TENSOR', (512, 1024, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 512, 7, 7), 'float32'), ('TENSOR', (2048, 512, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 1024, 14, 14), 'float32'), ('TENSOR', (2048, 1024, 1, 1), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 2048, 7, 7), 'float32'), ('TENSOR', (512, 2048, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('dense_tensorcore.cuda', ('TENSOR', (32, 2048), 'float32'), ('TENSOR', (1000, 2048), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.
INFO:compile_engine:Use implementation dense_tensorcore.cuda for op nn.dense
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.batch_flatten
INFO:compile_engine:Use implementation adaptive_pool.cuda for op nn.global_avg_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
learning_based_quantize.py:100: DeprecationWarning: legacy graph runtime behaviour of producing json / lib / params will be removed in the next release 
  graph, lib, params = relay.build(mod, target)
INFO:root:[3200 samples] validation: acc-top1=0.768125 acc-top5=0.940937
INFO:root:[6400 samples] validation: acc-top1=0.769687 acc-top5=0.934063
INFO:root:[9600 samples] validation: acc-top1=0.771042 acc-top5=0.931458
INFO:root:[12800 samples] validation: acc-top1=0.766406 acc-top5=0.929922
INFO:root:[16000 samples] validation: acc-top1=0.766563 acc-top5=0.930625
INFO:root:[19200 samples] validation: acc-top1=0.764115 acc-top5=0.930260
INFO:root:[22400 samples] validation: acc-top1=0.765000 acc-top5=0.929911
INFO:root:[25600 samples] validation: acc-top1=0.763828 acc-top5=0.930430
INFO:root:[28800 samples] validation: acc-top1=0.764653 acc-top5=0.931319
INFO:root:[32000 samples] validation: acc-top1=0.763719 acc-top5=0.931250
INFO:root:[35200 samples] validation: acc-top1=0.764006 acc-top5=0.931506
INFO:root:[38400 samples] validation: acc-top1=0.763958 acc-top5=0.931432
INFO:root:[41600 samples] validation: acc-top1=0.763558 acc-top5=0.931562
INFO:root:[44800 samples] validation: acc-top1=0.763371 acc-top5=0.931763
INFO:root:[48000 samples] validation: acc-top1=0.764021 acc-top5=0.931750
INFO:root:[final] validation: acc-top1=0.764021 acc-top5=0.931750
Final accuracy fp32 0.7640208333333334
DEBUG:root:original
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %4 = nn.conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(64, 64, 1, 1), float32] */ /* ty=Tensor[(64, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %5 = add(%4, meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %6 = add(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%7, meta[relay.Constant][5] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %9 = add(%8, meta[relay.Constant][6] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.relu(%9) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.conv2d(%10, meta[relay.Constant][7] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %12 = add(%11, meta[relay.Constant][8] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %13 = add(%12, meta[relay.Constant][9] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %14 = nn.conv2d(%3, meta[relay.Constant][10] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %15 = add(%14, meta[relay.Constant][11] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %16 = add(%13, %15) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %17 = nn.relu(%16) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %18 = nn.conv2d(%17, meta[relay.Constant][12] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %19 = add(%18, meta[relay.Constant][13] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %20 = add(%19, meta[relay.Constant][14] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %21 = nn.relu(%20) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, meta[relay.Constant][15] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %23 = add(%22, meta[relay.Constant][16] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %24 = nn.relu(%23) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %25 = nn.conv2d(%24, meta[relay.Constant][17] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %26 = add(%25, meta[relay.Constant][18] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %27 = add(%26, meta[relay.Constant][19] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %28 = add(%27, %17) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %29 = nn.relu(%28) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %30 = nn.conv2d(%29, meta[relay.Constant][20] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %31 = add(%30, meta[relay.Constant][21] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %32 = add(%31, meta[relay.Constant][22] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %34 = nn.conv2d(%33, meta[relay.Constant][23] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %35 = add(%34, meta[relay.Constant][24] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %36 = nn.relu(%35) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %37 = nn.conv2d(%36, meta[relay.Constant][25] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %38 = add(%37, meta[relay.Constant][26] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %39 = add(%38, meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %40 = add(%39, %29) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %42 = nn.conv2d(%41, meta[relay.Constant][28] /* ty=Tensor[(128, 256, 1, 1), float32] */ /* ty=Tensor[(128, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %43 = add(%42, meta[relay.Constant][29] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %44 = add(%43, meta[relay.Constant][30] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %45 = nn.relu(%44) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %46 = nn.conv2d(%45, meta[relay.Constant][31] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %47 = add(%46, meta[relay.Constant][32] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %48 = nn.relu(%47) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %49 = nn.conv2d(%48, meta[relay.Constant][33] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %50 = add(%49, meta[relay.Constant][34] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %51 = add(%50, meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %52 = nn.conv2d(%41, meta[relay.Constant][36] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %53 = add(%52, meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %54 = add(%51, %53) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %55 = nn.relu(%54) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %56 = nn.conv2d(%55, meta[relay.Constant][38] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %57 = add(%56, meta[relay.Constant][39] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %58 = add(%57, meta[relay.Constant][40] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %59 = nn.relu(%58) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %60 = nn.conv2d(%59, meta[relay.Constant][41] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %61 = add(%60, meta[relay.Constant][42] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %63 = nn.conv2d(%62, meta[relay.Constant][43] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %64 = add(%63, meta[relay.Constant][44] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %65 = add(%64, meta[relay.Constant][45] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %66 = add(%65, %55) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %68 = nn.conv2d(%67, meta[relay.Constant][46] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %69 = add(%68, meta[relay.Constant][47] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %70 = add(%69, meta[relay.Constant][48] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %71 = nn.relu(%70) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %72 = nn.conv2d(%71, meta[relay.Constant][49] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %73 = add(%72, meta[relay.Constant][50] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %74 = nn.relu(%73) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %75 = nn.conv2d(%74, meta[relay.Constant][51] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %76 = add(%75, meta[relay.Constant][52] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %77 = add(%76, meta[relay.Constant][53] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %78 = add(%77, %67) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %79 = nn.relu(%78) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %80 = nn.conv2d(%79, meta[relay.Constant][54] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %81 = add(%80, meta[relay.Constant][55] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %82 = add(%81, meta[relay.Constant][56] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %83 = nn.relu(%82) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %84 = nn.conv2d(%83, meta[relay.Constant][57] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %85 = add(%84, meta[relay.Constant][58] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %86 = nn.relu(%85) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %87 = nn.conv2d(%86, meta[relay.Constant][59] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %88 = add(%87, meta[relay.Constant][60] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %89 = add(%88, meta[relay.Constant][61] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %90 = add(%89, %79) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %92 = nn.conv2d(%91, meta[relay.Constant][62] /* ty=Tensor[(256, 512, 1, 1), float32] */ /* ty=Tensor[(256, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %93 = add(%92, meta[relay.Constant][63] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %94 = add(%93, meta[relay.Constant][64] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %95 = nn.relu(%94) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %96 = nn.conv2d(%95, meta[relay.Constant][65] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %97 = add(%96, meta[relay.Constant][66] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %98 = nn.relu(%97) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %99 = nn.conv2d(%98, meta[relay.Constant][67] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %100 = add(%99, meta[relay.Constant][68] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %101 = add(%100, meta[relay.Constant][69] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %102 = nn.conv2d(%91, meta[relay.Constant][70] /* ty=Tensor[(1024, 512, 1, 1), float32] */ /* ty=Tensor[(1024, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %103 = add(%102, meta[relay.Constant][71] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %104 = add(%101, %103) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %105 = nn.relu(%104) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %106 = nn.conv2d(%105, meta[relay.Constant][72] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %107 = add(%106, meta[relay.Constant][73] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %108 = add(%107, meta[relay.Constant][74] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %109 = nn.relu(%108) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %110 = nn.conv2d(%109, meta[relay.Constant][75] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %111 = add(%110, meta[relay.Constant][76] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %112 = nn.relu(%111) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %113 = nn.conv2d(%112, meta[relay.Constant][77] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %114 = add(%113, meta[relay.Constant][78] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %115 = add(%114, meta[relay.Constant][79] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %116 = add(%115, %105) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %117 = nn.relu(%116) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %118 = nn.conv2d(%117, meta[relay.Constant][80] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %119 = add(%118, meta[relay.Constant][81] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %120 = add(%119, meta[relay.Constant][82] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %121 = nn.relu(%120) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %122 = nn.conv2d(%121, meta[relay.Constant][83] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %123 = add(%122, meta[relay.Constant][84] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %125 = nn.conv2d(%124, meta[relay.Constant][85] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %126 = add(%125, meta[relay.Constant][86] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %127 = add(%126, meta[relay.Constant][87] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %128 = add(%127, %117) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %129 = nn.relu(%128) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %130 = nn.conv2d(%129, meta[relay.Constant][88] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %131 = add(%130, meta[relay.Constant][89] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %132 = add(%131, meta[relay.Constant][90] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %133 = nn.relu(%132) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %134 = nn.conv2d(%133, meta[relay.Constant][91] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %135 = add(%134, meta[relay.Constant][92] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %136 = nn.relu(%135) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %137 = nn.conv2d(%136, meta[relay.Constant][93] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %138 = add(%137, meta[relay.Constant][94] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %139 = add(%138, meta[relay.Constant][95] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %140 = add(%139, %129) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %141 = nn.relu(%140) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %142 = nn.conv2d(%141, meta[relay.Constant][96] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %143 = add(%142, meta[relay.Constant][97] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %144 = add(%143, meta[relay.Constant][98] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %145 = nn.relu(%144) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %146 = nn.conv2d(%145, meta[relay.Constant][99] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %147 = add(%146, meta[relay.Constant][100] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %148 = nn.relu(%147) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %149 = nn.conv2d(%148, meta[relay.Constant][101] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %150 = add(%149, meta[relay.Constant][102] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %151 = add(%150, meta[relay.Constant][103] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %152 = add(%151, %141) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %153 = nn.relu(%152) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %154 = nn.conv2d(%153, meta[relay.Constant][104] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %155 = add(%154, meta[relay.Constant][105] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %156 = add(%155, meta[relay.Constant][106] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %158 = nn.conv2d(%157, meta[relay.Constant][107] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %159 = add(%158, meta[relay.Constant][108] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %161 = nn.conv2d(%160, meta[relay.Constant][109] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %162 = add(%161, meta[relay.Constant][110] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %163 = add(%162, meta[relay.Constant][111] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %164 = add(%163, %153) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %165 = nn.relu(%164) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %166 = nn.conv2d(%165, meta[relay.Constant][112] /* ty=Tensor[(512, 1024, 1, 1), float32] */ /* ty=Tensor[(512, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %167 = add(%166, meta[relay.Constant][113] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %168 = add(%167, meta[relay.Constant][114] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %169 = nn.relu(%168) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %170 = nn.conv2d(%169, meta[relay.Constant][115] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %171 = add(%170, meta[relay.Constant][116] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %173 = nn.conv2d(%172, meta[relay.Constant][117] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %174 = add(%173, meta[relay.Constant][118] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %175 = add(%174, meta[relay.Constant][119] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %176 = nn.conv2d(%165, meta[relay.Constant][120] /* ty=Tensor[(2048, 1024, 1, 1), float32] */ /* ty=Tensor[(2048, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %177 = add(%176, meta[relay.Constant][121] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %178 = add(%175, %177) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %180 = nn.conv2d(%179, meta[relay.Constant][122] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %181 = add(%180, meta[relay.Constant][123] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %182 = add(%181, meta[relay.Constant][124] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %183 = nn.relu(%182) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %184 = nn.conv2d(%183, meta[relay.Constant][125] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %185 = add(%184, meta[relay.Constant][126] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %186 = nn.relu(%185) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %187 = nn.conv2d(%186, meta[relay.Constant][127] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %188 = add(%187, meta[relay.Constant][128] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %189 = add(%188, meta[relay.Constant][129] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %190 = add(%189, %179) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %191 = nn.relu(%190) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %192 = nn.conv2d(%191, meta[relay.Constant][130] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %193 = add(%192, meta[relay.Constant][131] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %194 = add(%193, meta[relay.Constant][132] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %195 = nn.relu(%194) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %196 = nn.conv2d(%195, meta[relay.Constant][133] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %197 = add(%196, meta[relay.Constant][134] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %198 = nn.relu(%197) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %199 = nn.conv2d(%198, meta[relay.Constant][135] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %200 = add(%199, meta[relay.Constant][136] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %201 = add(%200, meta[relay.Constant][137] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %202 = add(%201, %191) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %203 = nn.relu(%202) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %204 = nn.global_avg_pool2d(%203) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %205 = nn.batch_flatten(%204) /* ty=Tensor[(32, 2048), float32] */;
  %206 = nn.dense(%205, meta[relay.Constant][138] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%206, meta[relay.Constant][139] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:root:current quantize config
DEBUG:root:<tvm.hago.base.QConfig object at 0x7fa89e7e9070>
data
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
nn.global_avg_pool2d
nn.batch_flatten
constant
nn.dense
constant
add
data -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.global_avg_pool2d
nn.global_avg_pool2d -> nn.batch_flatten
nn.batch_flatten -> nn.dense
constant -> nn.dense
nn.dense -> add
constant -> add
analyzed condition
node_conds: [False, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, False, False, False, False, False]
edge_conds: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]
bit limit
[8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32]
--------
nn.conv2d[%2]: [32]
  data[%0] -> nn.conv2d[%2] : 8
  constant[%1] -> nn.conv2d[%2] : 8
--------
add[%4]: [32]
  nn.conv2d[%2] -> add[%4] : 32
  constant[%3] -> add[%4] : 32
--------
nn.relu[%5]: [32]
  add[%4] -> nn.relu[%5] : 32
--------
nn.max_pool2d[%6]: [8, 8]
  nn.relu[%5] -> nn.max_pool2d[%6] : 32
--------
nn.conv2d[%8]: [32]
  nn.max_pool2d[%6] -> nn.conv2d[%8] : 8
  constant[%7] -> nn.conv2d[%8] : 8
--------
add[%10]: [32]
  nn.conv2d[%8] -> add[%10] : 32
  constant[%9] -> add[%10] : 32
--------
add[%12]: [32]
  add[%10] -> add[%12] : 32
  constant[%11] -> add[%12] : 32
--------
nn.relu[%13]: [8]
  add[%12] -> nn.relu[%13] : 32
--------
nn.conv2d[%15]: [32]
  nn.relu[%13] -> nn.conv2d[%15] : 8
  constant[%14] -> nn.conv2d[%15] : 8
--------
add[%17]: [32]
  nn.conv2d[%15] -> add[%17] : 32
  constant[%16] -> add[%17] : 32
--------
nn.relu[%18]: [8]
  add[%17] -> nn.relu[%18] : 32
--------
nn.conv2d[%20]: [32]
  nn.relu[%18] -> nn.conv2d[%20] : 8
  constant[%19] -> nn.conv2d[%20] : 8
--------
add[%22]: [32]
  nn.conv2d[%20] -> add[%22] : 32
  constant[%21] -> add[%22] : 32
--------
add[%24]: [32]
  add[%22] -> add[%24] : 32
  constant[%23] -> add[%24] : 32
--------
nn.conv2d[%26]: [32]
  nn.max_pool2d[%6] -> nn.conv2d[%26] : 8
  constant[%25] -> nn.conv2d[%26] : 8
--------
add[%28]: [32]
  nn.conv2d[%26] -> add[%28] : 32
  constant[%27] -> add[%28] : 32
--------
add[%29]: [32]
  add[%24] -> add[%29] : 32
  add[%28] -> add[%29] : 32
--------
nn.relu[%30]: [8, 32]
  add[%29] -> nn.relu[%30] : 32
--------
nn.conv2d[%32]: [32]
  nn.relu[%30] -> nn.conv2d[%32] : 8
  constant[%31] -> nn.conv2d[%32] : 8
--------
add[%34]: [32]
  nn.conv2d[%32] -> add[%34] : 32
  constant[%33] -> add[%34] : 32
--------
add[%36]: [32]
  add[%34] -> add[%36] : 32
  constant[%35] -> add[%36] : 32
--------
nn.relu[%37]: [8]
  add[%36] -> nn.relu[%37] : 32
--------
nn.conv2d[%39]: [32]
  nn.relu[%37] -> nn.conv2d[%39] : 8
  constant[%38] -> nn.conv2d[%39] : 8
--------
add[%41]: [32]
  nn.conv2d[%39] -> add[%41] : 32
  constant[%40] -> add[%41] : 32
--------
nn.relu[%42]: [8]
  add[%41] -> nn.relu[%42] : 32
--------
nn.conv2d[%44]: [32]
  nn.relu[%42] -> nn.conv2d[%44] : 8
  constant[%43] -> nn.conv2d[%44] : 8
--------
add[%46]: [32]
  nn.conv2d[%44] -> add[%46] : 32
  constant[%45] -> add[%46] : 32
--------
add[%48]: [32]
  add[%46] -> add[%48] : 32
  constant[%47] -> add[%48] : 32
--------
add[%49]: [32]
  add[%48] -> add[%49] : 32
  nn.relu[%30] -> add[%49] : 32
--------
nn.relu[%50]: [8, 32]
  add[%49] -> nn.relu[%50] : 32
--------
nn.conv2d[%52]: [32]
  nn.relu[%50] -> nn.conv2d[%52] : 8
  constant[%51] -> nn.conv2d[%52] : 8
--------
add[%54]: [32]
  nn.conv2d[%52] -> add[%54] : 32
  constant[%53] -> add[%54] : 32
--------
add[%56]: [32]
  add[%54] -> add[%56] : 32
  constant[%55] -> add[%56] : 32
--------
nn.relu[%57]: [8]
  add[%56] -> nn.relu[%57] : 32
--------
nn.conv2d[%59]: [32]
  nn.relu[%57] -> nn.conv2d[%59] : 8
  constant[%58] -> nn.conv2d[%59] : 8
--------
add[%61]: [32]
  nn.conv2d[%59] -> add[%61] : 32
  constant[%60] -> add[%61] : 32
--------
nn.relu[%62]: [8]
  add[%61] -> nn.relu[%62] : 32
--------
nn.conv2d[%64]: [32]
  nn.relu[%62] -> nn.conv2d[%64] : 8
  constant[%63] -> nn.conv2d[%64] : 8
--------
add[%66]: [32]
  nn.conv2d[%64] -> add[%66] : 32
  constant[%65] -> add[%66] : 32
--------
add[%68]: [32]
  add[%66] -> add[%68] : 32
  constant[%67] -> add[%68] : 32
--------
add[%69]: [32]
  add[%68] -> add[%69] : 32
  nn.relu[%50] -> add[%69] : 32
--------
nn.relu[%70]: [8, 8]
  add[%69] -> nn.relu[%70] : 32
--------
nn.conv2d[%72]: [32]
  nn.relu[%70] -> nn.conv2d[%72] : 8
  constant[%71] -> nn.conv2d[%72] : 8
--------
add[%74]: [32]
  nn.conv2d[%72] -> add[%74] : 32
  constant[%73] -> add[%74] : 32
--------
add[%76]: [32]
  add[%74] -> add[%76] : 32
  constant[%75] -> add[%76] : 32
--------
nn.relu[%77]: [8]
  add[%76] -> nn.relu[%77] : 32
--------
nn.conv2d[%79]: [32]
  nn.relu[%77] -> nn.conv2d[%79] : 8
  constant[%78] -> nn.conv2d[%79] : 8
--------
add[%81]: [32]
  nn.conv2d[%79] -> add[%81] : 32
  constant[%80] -> add[%81] : 32
--------
nn.relu[%82]: [8]
  add[%81] -> nn.relu[%82] : 32
--------
nn.conv2d[%84]: [32]
  nn.relu[%82] -> nn.conv2d[%84] : 8
  constant[%83] -> nn.conv2d[%84] : 8
--------
add[%86]: [32]
  nn.conv2d[%84] -> add[%86] : 32
  constant[%85] -> add[%86] : 32
--------
add[%88]: [32]
  add[%86] -> add[%88] : 32
  constant[%87] -> add[%88] : 32
--------
nn.conv2d[%90]: [32]
  nn.relu[%70] -> nn.conv2d[%90] : 8
  constant[%89] -> nn.conv2d[%90] : 8
--------
add[%92]: [32]
  nn.conv2d[%90] -> add[%92] : 32
  constant[%91] -> add[%92] : 32
--------
add[%93]: [32]
  add[%88] -> add[%93] : 32
  add[%92] -> add[%93] : 32
--------
nn.relu[%94]: [8, 32]
  add[%93] -> nn.relu[%94] : 32
--------
nn.conv2d[%96]: [32]
  nn.relu[%94] -> nn.conv2d[%96] : 8
  constant[%95] -> nn.conv2d[%96] : 8
--------
add[%98]: [32]
  nn.conv2d[%96] -> add[%98] : 32
  constant[%97] -> add[%98] : 32
--------
add[%100]: [32]
  add[%98] -> add[%100] : 32
  constant[%99] -> add[%100] : 32
--------
nn.relu[%101]: [8]
  add[%100] -> nn.relu[%101] : 32
--------
nn.conv2d[%103]: [32]
  nn.relu[%101] -> nn.conv2d[%103] : 8
  constant[%102] -> nn.conv2d[%103] : 8
--------
add[%105]: [32]
  nn.conv2d[%103] -> add[%105] : 32
  constant[%104] -> add[%105] : 32
--------
nn.relu[%106]: [8]
  add[%105] -> nn.relu[%106] : 32
--------
nn.conv2d[%108]: [32]
  nn.relu[%106] -> nn.conv2d[%108] : 8
  constant[%107] -> nn.conv2d[%108] : 8
--------
add[%110]: [32]
  nn.conv2d[%108] -> add[%110] : 32
  constant[%109] -> add[%110] : 32
--------
add[%112]: [32]
  add[%110] -> add[%112] : 32
  constant[%111] -> add[%112] : 32
--------
add[%113]: [32]
  add[%112] -> add[%113] : 32
  nn.relu[%94] -> add[%113] : 32
--------
nn.relu[%114]: [8, 32]
  add[%113] -> nn.relu[%114] : 32
--------
nn.conv2d[%116]: [32]
  nn.relu[%114] -> nn.conv2d[%116] : 8
  constant[%115] -> nn.conv2d[%116] : 8
--------
add[%118]: [32]
  nn.conv2d[%116] -> add[%118] : 32
  constant[%117] -> add[%118] : 32
--------
add[%120]: [32]
  add[%118] -> add[%120] : 32
  constant[%119] -> add[%120] : 32
--------
nn.relu[%121]: [8]
  add[%120] -> nn.relu[%121] : 32
--------
nn.conv2d[%123]: [32]
  nn.relu[%121] -> nn.conv2d[%123] : 8
  constant[%122] -> nn.conv2d[%123] : 8
--------
add[%125]: [32]
  nn.conv2d[%123] -> add[%125] : 32
  constant[%124] -> add[%125] : 32
--------
nn.relu[%126]: [8]
  add[%125] -> nn.relu[%126] : 32
--------
nn.conv2d[%128]: [32]
  nn.relu[%126] -> nn.conv2d[%128] : 8
  constant[%127] -> nn.conv2d[%128] : 8
--------
add[%130]: [32]
  nn.conv2d[%128] -> add[%130] : 32
  constant[%129] -> add[%130] : 32
--------
add[%132]: [32]
  add[%130] -> add[%132] : 32
  constant[%131] -> add[%132] : 32
--------
add[%133]: [32]
  add[%132] -> add[%133] : 32
  nn.relu[%114] -> add[%133] : 32
--------
nn.relu[%134]: [8, 32]
  add[%133] -> nn.relu[%134] : 32
--------
nn.conv2d[%136]: [32]
  nn.relu[%134] -> nn.conv2d[%136] : 8
  constant[%135] -> nn.conv2d[%136] : 8
--------
add[%138]: [32]
  nn.conv2d[%136] -> add[%138] : 32
  constant[%137] -> add[%138] : 32
--------
add[%140]: [32]
  add[%138] -> add[%140] : 32
  constant[%139] -> add[%140] : 32
--------
nn.relu[%141]: [8]
  add[%140] -> nn.relu[%141] : 32
--------
nn.conv2d[%143]: [32]
  nn.relu[%141] -> nn.conv2d[%143] : 8
  constant[%142] -> nn.conv2d[%143] : 8
--------
add[%145]: [32]
  nn.conv2d[%143] -> add[%145] : 32
  constant[%144] -> add[%145] : 32
--------
nn.relu[%146]: [8]
  add[%145] -> nn.relu[%146] : 32
--------
nn.conv2d[%148]: [32]
  nn.relu[%146] -> nn.conv2d[%148] : 8
  constant[%147] -> nn.conv2d[%148] : 8
--------
add[%150]: [32]
  nn.conv2d[%148] -> add[%150] : 32
  constant[%149] -> add[%150] : 32
--------
add[%152]: [32]
  add[%150] -> add[%152] : 32
  constant[%151] -> add[%152] : 32
--------
add[%153]: [32]
  add[%152] -> add[%153] : 32
  nn.relu[%134] -> add[%153] : 32
--------
nn.relu[%154]: [8, 8]
  add[%153] -> nn.relu[%154] : 32
--------
nn.conv2d[%156]: [32]
  nn.relu[%154] -> nn.conv2d[%156] : 8
  constant[%155] -> nn.conv2d[%156] : 8
--------
add[%158]: [32]
  nn.conv2d[%156] -> add[%158] : 32
  constant[%157] -> add[%158] : 32
--------
add[%160]: [32]
  add[%158] -> add[%160] : 32
  constant[%159] -> add[%160] : 32
--------
nn.relu[%161]: [8]
  add[%160] -> nn.relu[%161] : 32
--------
nn.conv2d[%163]: [32]
  nn.relu[%161] -> nn.conv2d[%163] : 8
  constant[%162] -> nn.conv2d[%163] : 8
--------
add[%165]: [32]
  nn.conv2d[%163] -> add[%165] : 32
  constant[%164] -> add[%165] : 32
--------
nn.relu[%166]: [8]
  add[%165] -> nn.relu[%166] : 32
--------
nn.conv2d[%168]: [32]
  nn.relu[%166] -> nn.conv2d[%168] : 8
  constant[%167] -> nn.conv2d[%168] : 8
--------
add[%170]: [32]
  nn.conv2d[%168] -> add[%170] : 32
  constant[%169] -> add[%170] : 32
--------
add[%172]: [32]
  add[%170] -> add[%172] : 32
  constant[%171] -> add[%172] : 32
--------
nn.conv2d[%174]: [32]
  nn.relu[%154] -> nn.conv2d[%174] : 8
  constant[%173] -> nn.conv2d[%174] : 8
--------
add[%176]: [32]
  nn.conv2d[%174] -> add[%176] : 32
  constant[%175] -> add[%176] : 32
--------
add[%177]: [32]
  add[%172] -> add[%177] : 32
  add[%176] -> add[%177] : 32
--------
nn.relu[%178]: [8, 32]
  add[%177] -> nn.relu[%178] : 32
--------
nn.conv2d[%180]: [32]
  nn.relu[%178] -> nn.conv2d[%180] : 8
  constant[%179] -> nn.conv2d[%180] : 8
--------
add[%182]: [32]
  nn.conv2d[%180] -> add[%182] : 32
  constant[%181] -> add[%182] : 32
--------
add[%184]: [32]
  add[%182] -> add[%184] : 32
  constant[%183] -> add[%184] : 32
--------
nn.relu[%185]: [8]
  add[%184] -> nn.relu[%185] : 32
--------
nn.conv2d[%187]: [32]
  nn.relu[%185] -> nn.conv2d[%187] : 8
  constant[%186] -> nn.conv2d[%187] : 8
--------
add[%189]: [32]
  nn.conv2d[%187] -> add[%189] : 32
  constant[%188] -> add[%189] : 32
--------
nn.relu[%190]: [8]
  add[%189] -> nn.relu[%190] : 32
--------
nn.conv2d[%192]: [32]
  nn.relu[%190] -> nn.conv2d[%192] : 8
  constant[%191] -> nn.conv2d[%192] : 8
--------
add[%194]: [32]
  nn.conv2d[%192] -> add[%194] : 32
  constant[%193] -> add[%194] : 32
--------
add[%196]: [32]
  add[%194] -> add[%196] : 32
  constant[%195] -> add[%196] : 32
--------
add[%197]: [32]
  add[%196] -> add[%197] : 32
  nn.relu[%178] -> add[%197] : 32
--------
nn.relu[%198]: [8, 32]
  add[%197] -> nn.relu[%198] : 32
--------
nn.conv2d[%200]: [32]
  nn.relu[%198] -> nn.conv2d[%200] : 8
  constant[%199] -> nn.conv2d[%200] : 8
--------
add[%202]: [32]
  nn.conv2d[%200] -> add[%202] : 32
  constant[%201] -> add[%202] : 32
--------
add[%204]: [32]
  add[%202] -> add[%204] : 32
  constant[%203] -> add[%204] : 32
--------
nn.relu[%205]: [8]
  add[%204] -> nn.relu[%205] : 32
--------
nn.conv2d[%207]: [32]
  nn.relu[%205] -> nn.conv2d[%207] : 8
  constant[%206] -> nn.conv2d[%207] : 8
--------
add[%209]: [32]
  nn.conv2d[%207] -> add[%209] : 32
  constant[%208] -> add[%209] : 32
--------
nn.relu[%210]: [8]
  add[%209] -> nn.relu[%210] : 32
--------
nn.conv2d[%212]: [32]
  nn.relu[%210] -> nn.conv2d[%212] : 8
  constant[%211] -> nn.conv2d[%212] : 8
--------
add[%214]: [32]
  nn.conv2d[%212] -> add[%214] : 32
  constant[%213] -> add[%214] : 32
--------
add[%216]: [32]
  add[%214] -> add[%216] : 32
  constant[%215] -> add[%216] : 32
--------
add[%217]: [32]
  add[%216] -> add[%217] : 32
  nn.relu[%198] -> add[%217] : 32
--------
nn.relu[%218]: [8, 32]
  add[%217] -> nn.relu[%218] : 32
--------
nn.conv2d[%220]: [32]
  nn.relu[%218] -> nn.conv2d[%220] : 8
  constant[%219] -> nn.conv2d[%220] : 8
--------
add[%222]: [32]
  nn.conv2d[%220] -> add[%222] : 32
  constant[%221] -> add[%222] : 32
--------
add[%224]: [32]
  add[%222] -> add[%224] : 32
  constant[%223] -> add[%224] : 32
--------
nn.relu[%225]: [8]
  add[%224] -> nn.relu[%225] : 32
--------
nn.conv2d[%227]: [32]
  nn.relu[%225] -> nn.conv2d[%227] : 8
  constant[%226] -> nn.conv2d[%227] : 8
--------
add[%229]: [32]
  nn.conv2d[%227] -> add[%229] : 32
  constant[%228] -> add[%229] : 32
--------
nn.relu[%230]: [8]
  add[%229] -> nn.relu[%230] : 32
--------
nn.conv2d[%232]: [32]
  nn.relu[%230] -> nn.conv2d[%232] : 8
  constant[%231] -> nn.conv2d[%232] : 8
--------
add[%234]: [32]
  nn.conv2d[%232] -> add[%234] : 32
  constant[%233] -> add[%234] : 32
--------
add[%236]: [32]
  add[%234] -> add[%236] : 32
  constant[%235] -> add[%236] : 32
--------
add[%237]: [32]
  add[%236] -> add[%237] : 32
  nn.relu[%218] -> add[%237] : 32
--------
nn.relu[%238]: [8, 32]
  add[%237] -> nn.relu[%238] : 32
--------
nn.conv2d[%240]: [32]
  nn.relu[%238] -> nn.conv2d[%240] : 8
  constant[%239] -> nn.conv2d[%240] : 8
--------
add[%242]: [32]
  nn.conv2d[%240] -> add[%242] : 32
  constant[%241] -> add[%242] : 32
--------
add[%244]: [32]
  add[%242] -> add[%244] : 32
  constant[%243] -> add[%244] : 32
--------
nn.relu[%245]: [8]
  add[%244] -> nn.relu[%245] : 32
--------
nn.conv2d[%247]: [32]
  nn.relu[%245] -> nn.conv2d[%247] : 8
  constant[%246] -> nn.conv2d[%247] : 8
--------
add[%249]: [32]
  nn.conv2d[%247] -> add[%249] : 32
  constant[%248] -> add[%249] : 32
--------
nn.relu[%250]: [8]
  add[%249] -> nn.relu[%250] : 32
--------
nn.conv2d[%252]: [32]
  nn.relu[%250] -> nn.conv2d[%252] : 8
  constant[%251] -> nn.conv2d[%252] : 8
--------
add[%254]: [32]
  nn.conv2d[%252] -> add[%254] : 32
  constant[%253] -> add[%254] : 32
--------
add[%256]: [32]
  add[%254] -> add[%256] : 32
  constant[%255] -> add[%256] : 32
--------
add[%257]: [32]
  add[%256] -> add[%257] : 32
  nn.relu[%238] -> add[%257] : 32
--------
nn.relu[%258]: [8, 32]
  add[%257] -> nn.relu[%258] : 32
--------
nn.conv2d[%260]: [32]
  nn.relu[%258] -> nn.conv2d[%260] : 8
  constant[%259] -> nn.conv2d[%260] : 8
--------
add[%262]: [32]
  nn.conv2d[%260] -> add[%262] : 32
  constant[%261] -> add[%262] : 32
--------
add[%264]: [32]
  add[%262] -> add[%264] : 32
  constant[%263] -> add[%264] : 32
--------
nn.relu[%265]: [8]
  add[%264] -> nn.relu[%265] : 32
--------
nn.conv2d[%267]: [32]
  nn.relu[%265] -> nn.conv2d[%267] : 8
  constant[%266] -> nn.conv2d[%267] : 8
--------
add[%269]: [32]
  nn.conv2d[%267] -> add[%269] : 32
  constant[%268] -> add[%269] : 32
--------
nn.relu[%270]: [8]
  add[%269] -> nn.relu[%270] : 32
--------
nn.conv2d[%272]: [32]
  nn.relu[%270] -> nn.conv2d[%272] : 8
  constant[%271] -> nn.conv2d[%272] : 8
--------
add[%274]: [32]
  nn.conv2d[%272] -> add[%274] : 32
  constant[%273] -> add[%274] : 32
--------
add[%276]: [32]
  add[%274] -> add[%276] : 32
  constant[%275] -> add[%276] : 32
--------
add[%277]: [32]
  add[%276] -> add[%277] : 32
  nn.relu[%258] -> add[%277] : 32
--------
nn.relu[%278]: [8, 8]
  add[%277] -> nn.relu[%278] : 32
--------
nn.conv2d[%280]: [32]
  nn.relu[%278] -> nn.conv2d[%280] : 8
  constant[%279] -> nn.conv2d[%280] : 8
--------
add[%282]: [32]
  nn.conv2d[%280] -> add[%282] : 32
  constant[%281] -> add[%282] : 32
--------
add[%284]: [32]
  add[%282] -> add[%284] : 32
  constant[%283] -> add[%284] : 32
--------
nn.relu[%285]: [8]
  add[%284] -> nn.relu[%285] : 32
--------
nn.conv2d[%287]: [32]
  nn.relu[%285] -> nn.conv2d[%287] : 8
  constant[%286] -> nn.conv2d[%287] : 8
--------
add[%289]: [32]
  nn.conv2d[%287] -> add[%289] : 32
  constant[%288] -> add[%289] : 32
--------
nn.relu[%290]: [8]
  add[%289] -> nn.relu[%290] : 32
--------
nn.conv2d[%292]: [32]
  nn.relu[%290] -> nn.conv2d[%292] : 8
  constant[%291] -> nn.conv2d[%292] : 8
--------
add[%294]: [32]
  nn.conv2d[%292] -> add[%294] : 32
  constant[%293] -> add[%294] : 32
--------
add[%296]: [32]
  add[%294] -> add[%296] : 32
  constant[%295] -> add[%296] : 32
--------
nn.conv2d[%298]: [32]
  nn.relu[%278] -> nn.conv2d[%298] : 8
  constant[%297] -> nn.conv2d[%298] : 8
--------
add[%300]: [32]
  nn.conv2d[%298] -> add[%300] : 32
  constant[%299] -> add[%300] : 32
--------
add[%301]: [32]
  add[%296] -> add[%301] : 32
  add[%300] -> add[%301] : 32
--------
nn.relu[%302]: [8, 32]
  add[%301] -> nn.relu[%302] : 32
--------
nn.conv2d[%304]: [32]
  nn.relu[%302] -> nn.conv2d[%304] : 8
  constant[%303] -> nn.conv2d[%304] : 8
--------
add[%306]: [32]
  nn.conv2d[%304] -> add[%306] : 32
  constant[%305] -> add[%306] : 32
--------
add[%308]: [32]
  add[%306] -> add[%308] : 32
  constant[%307] -> add[%308] : 32
--------
nn.relu[%309]: [8]
  add[%308] -> nn.relu[%309] : 32
--------
nn.conv2d[%311]: [32]
  nn.relu[%309] -> nn.conv2d[%311] : 8
  constant[%310] -> nn.conv2d[%311] : 8
--------
add[%313]: [32]
  nn.conv2d[%311] -> add[%313] : 32
  constant[%312] -> add[%313] : 32
--------
nn.relu[%314]: [8]
  add[%313] -> nn.relu[%314] : 32
--------
nn.conv2d[%316]: [32]
  nn.relu[%314] -> nn.conv2d[%316] : 8
  constant[%315] -> nn.conv2d[%316] : 8
--------
add[%318]: [32]
  nn.conv2d[%316] -> add[%318] : 32
  constant[%317] -> add[%318] : 32
--------
add[%320]: [32]
  add[%318] -> add[%320] : 32
  constant[%319] -> add[%320] : 32
--------
add[%321]: [32]
  add[%320] -> add[%321] : 32
  nn.relu[%302] -> add[%321] : 32
--------
nn.relu[%322]: [8, 32]
  add[%321] -> nn.relu[%322] : 32
--------
nn.conv2d[%324]: [32]
  nn.relu[%322] -> nn.conv2d[%324] : 8
  constant[%323] -> nn.conv2d[%324] : 8
--------
add[%326]: [32]
  nn.conv2d[%324] -> add[%326] : 32
  constant[%325] -> add[%326] : 32
--------
add[%328]: [32]
  add[%326] -> add[%328] : 32
  constant[%327] -> add[%328] : 32
--------
nn.relu[%329]: [8]
  add[%328] -> nn.relu[%329] : 32
--------
nn.conv2d[%331]: [32]
  nn.relu[%329] -> nn.conv2d[%331] : 8
  constant[%330] -> nn.conv2d[%331] : 8
--------
add[%333]: [32]
  nn.conv2d[%331] -> add[%333] : 32
  constant[%332] -> add[%333] : 32
--------
nn.relu[%334]: [8]
  add[%333] -> nn.relu[%334] : 32
--------
nn.conv2d[%336]: [32]
  nn.relu[%334] -> nn.conv2d[%336] : 8
  constant[%335] -> nn.conv2d[%336] : 8
--------
add[%338]: [32]
  nn.conv2d[%336] -> add[%338] : 32
  constant[%337] -> add[%338] : 32
--------
add[%340]: [32]
  add[%338] -> add[%340] : 32
  constant[%339] -> add[%340] : 32
--------
add[%341]: [32]
  add[%340] -> add[%341] : 32
  nn.relu[%322] -> add[%341] : 32
--------
nn.relu[%342]: [None]
  add[%341] -> nn.relu[%342] : 32
--------
nn.global_avg_pool2d[%343]: [None]
  nn.relu[%342] -> nn.global_avg_pool2d[%343] : None
--------
nn.batch_flatten[%344]: [None]
  nn.global_avg_pool2d[%343] -> nn.batch_flatten[%344] : None
--------
nn.dense[%346]: [None]
  nn.batch_flatten[%344] -> nn.dense[%346] : None
  constant[%345] -> nn.dense[%346] : None
--------
add[%348]: []
  nn.dense[%346] -> add[%348] : None
  constant[%347] -> add[%348] : None
ops in graph:
{'nn.conv2d', 'nn.dense', 'nn.relu', 'nn.max_pool2d', 'nn.global_avg_pool2d', 'nn.batch_flatten', 'add'}
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
original acc: 0.8125
data
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
nn.global_avg_pool2d
nn.batch_flatten
constant
nn.dense
constant
add
data -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.global_avg_pool2d
nn.global_avg_pool2d -> nn.batch_flatten
nn.batch_flatten -> nn.dense
constant -> nn.dense
nn.dense -> add
constant -> add
analyzed condition
node_conds: [False, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, False, False, False, False, False]
edge_conds: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]
INFO:root:collecting statistics for calibration...
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation dense_tensorcore.cuda for op nn.dense
INFO:root:statistics collected

select descriptor
---------
nn.conv2d[%2]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%4]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%5]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%6]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%8]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%10]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%12]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%13]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%15]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%17]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%18]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%20]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%22]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%24]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%26]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%28]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%29]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%30]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%32]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%34]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%36]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%37]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%39]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%41]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%42]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%44]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%46]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%48]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%49]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%50]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%52]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%54]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%56]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%57]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%59]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%61]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%62]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%64]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%66]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%68]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%69]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%70]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%72]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%74]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%76]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%77]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%79]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%81]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%82]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%84]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%86]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%88]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%90]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%92]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%93]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%94]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%96]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%98]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%100]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%101]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%103]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%105]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%106]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%108]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%110]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%112]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%113]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%114]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%116]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%118]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%120]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%121]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%123]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%125]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%126]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%128]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%130]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%132]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%133]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%134]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%136]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%138]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%140]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%141]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%143]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%145]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%146]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%148]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%150]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%152]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%153]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%154]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%156]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%158]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%160]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%161]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%163]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%165]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%166]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%168]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%170]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%172]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%174]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%176]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%177]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%178]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%180]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%182]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%184]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%185]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%187]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%189]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%190]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%192]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%194]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%196]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%197]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%198]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%200]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%202]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%204]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%205]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%207]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%209]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%210]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%212]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%214]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%216]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%217]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%218]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%220]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%222]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%224]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%225]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%227]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%229]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%230]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%232]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%234]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%236]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%237]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%238]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%240]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%242]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%244]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%245]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%247]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%249]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%250]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%252]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%254]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%256]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%257]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%258]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%260]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%262]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%264]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%265]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%267]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%269]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%270]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%272]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%274]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%276]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%277]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%278]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%280]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%282]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%284]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%285]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%287]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%289]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%290]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%292]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%294]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%296]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%298]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%300]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%301]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%302]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%304]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%306]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%308]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%309]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%311]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%313]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%314]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%316]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%318]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%320]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%321]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%322]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%324]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%326]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%328]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%329]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%331]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%333]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%334]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%336]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%338]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%340]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%341]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%342]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %4 = nn.conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(64, 64, 1, 1), float32] */ /* ty=Tensor[(64, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %5 = add(%4, meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %6 = add(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%7, meta[relay.Constant][5] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %9 = add(%8, meta[relay.Constant][6] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.relu(%9) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.conv2d(%10, meta[relay.Constant][7] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %12 = add(%11, meta[relay.Constant][8] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %13 = add(%12, meta[relay.Constant][9] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %14 = nn.conv2d(%3, meta[relay.Constant][10] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %15 = add(%14, meta[relay.Constant][11] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %16 = add(%13, %15) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %17 = nn.relu(%16) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %18 = nn.conv2d(%17, meta[relay.Constant][12] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %19 = add(%18, meta[relay.Constant][13] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %20 = add(%19, meta[relay.Constant][14] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %21 = nn.relu(%20) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, meta[relay.Constant][15] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %23 = add(%22, meta[relay.Constant][16] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %24 = nn.relu(%23) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %25 = nn.conv2d(%24, meta[relay.Constant][17] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %26 = add(%25, meta[relay.Constant][18] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %27 = add(%26, meta[relay.Constant][19] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %28 = add(%27, %17) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %29 = nn.relu(%28) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %30 = nn.conv2d(%29, meta[relay.Constant][20] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %31 = add(%30, meta[relay.Constant][21] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %32 = add(%31, meta[relay.Constant][22] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %34 = nn.conv2d(%33, meta[relay.Constant][23] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %35 = add(%34, meta[relay.Constant][24] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %36 = nn.relu(%35) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %37 = nn.conv2d(%36, meta[relay.Constant][25] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %38 = add(%37, meta[relay.Constant][26] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %39 = add(%38, meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %40 = add(%39, %29) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %42 = nn.conv2d(%41, meta[relay.Constant][28] /* ty=Tensor[(128, 256, 1, 1), float32] */ /* ty=Tensor[(128, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %43 = add(%42, meta[relay.Constant][29] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %44 = add(%43, meta[relay.Constant][30] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %45 = nn.relu(%44) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %46 = nn.conv2d(%45, meta[relay.Constant][31] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %47 = add(%46, meta[relay.Constant][32] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %48 = nn.relu(%47) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %49 = nn.conv2d(%48, meta[relay.Constant][33] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %50 = add(%49, meta[relay.Constant][34] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %51 = add(%50, meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %52 = nn.conv2d(%41, meta[relay.Constant][36] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %53 = add(%52, meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %54 = add(%51, %53) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %55 = nn.relu(%54) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %56 = nn.conv2d(%55, meta[relay.Constant][38] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %57 = add(%56, meta[relay.Constant][39] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %58 = add(%57, meta[relay.Constant][40] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %59 = nn.relu(%58) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %60 = nn.conv2d(%59, meta[relay.Constant][41] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %61 = add(%60, meta[relay.Constant][42] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %63 = nn.conv2d(%62, meta[relay.Constant][43] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %64 = add(%63, meta[relay.Constant][44] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %65 = add(%64, meta[relay.Constant][45] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %66 = add(%65, %55) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %68 = nn.conv2d(%67, meta[relay.Constant][46] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %69 = add(%68, meta[relay.Constant][47] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %70 = add(%69, meta[relay.Constant][48] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %71 = nn.relu(%70) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %72 = nn.conv2d(%71, meta[relay.Constant][49] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %73 = add(%72, meta[relay.Constant][50] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %74 = nn.relu(%73) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %75 = nn.conv2d(%74, meta[relay.Constant][51] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %76 = add(%75, meta[relay.Constant][52] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %77 = add(%76, meta[relay.Constant][53] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %78 = add(%77, %67) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %79 = nn.relu(%78) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %80 = nn.conv2d(%79, meta[relay.Constant][54] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %81 = add(%80, meta[relay.Constant][55] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %82 = add(%81, meta[relay.Constant][56] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %83 = nn.relu(%82) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %84 = nn.conv2d(%83, meta[relay.Constant][57] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %85 = add(%84, meta[relay.Constant][58] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %86 = nn.relu(%85) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %87 = nn.conv2d(%86, meta[relay.Constant][59] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %88 = add(%87, meta[relay.Constant][60] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %89 = add(%88, meta[relay.Constant][61] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %90 = add(%89, %79) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %92 = nn.conv2d(%91, meta[relay.Constant][62] /* ty=Tensor[(256, 512, 1, 1), float32] */ /* ty=Tensor[(256, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %93 = add(%92, meta[relay.Constant][63] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %94 = add(%93, meta[relay.Constant][64] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %95 = nn.relu(%94) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %96 = nn.conv2d(%95, meta[relay.Constant][65] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %97 = add(%96, meta[relay.Constant][66] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %98 = nn.relu(%97) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %99 = nn.conv2d(%98, meta[relay.Constant][67] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %100 = add(%99, meta[relay.Constant][68] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %101 = add(%100, meta[relay.Constant][69] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %102 = nn.conv2d(%91, meta[relay.Constant][70] /* ty=Tensor[(1024, 512, 1, 1), float32] */ /* ty=Tensor[(1024, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %103 = add(%102, meta[relay.Constant][71] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %104 = add(%101, %103) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %105 = nn.relu(%104) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %106 = nn.conv2d(%105, meta[relay.Constant][72] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %107 = add(%106, meta[relay.Constant][73] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %108 = add(%107, meta[relay.Constant][74] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %109 = nn.relu(%108) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %110 = nn.conv2d(%109, meta[relay.Constant][75] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %111 = add(%110, meta[relay.Constant][76] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %112 = nn.relu(%111) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %113 = nn.conv2d(%112, meta[relay.Constant][77] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %114 = add(%113, meta[relay.Constant][78] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %115 = add(%114, meta[relay.Constant][79] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %116 = add(%115, %105) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %117 = nn.relu(%116) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %118 = nn.conv2d(%117, meta[relay.Constant][80] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %119 = add(%118, meta[relay.Constant][81] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %120 = add(%119, meta[relay.Constant][82] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %121 = nn.relu(%120) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %122 = nn.conv2d(%121, meta[relay.Constant][83] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %123 = add(%122, meta[relay.Constant][84] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %125 = nn.conv2d(%124, meta[relay.Constant][85] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %126 = add(%125, meta[relay.Constant][86] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %127 = add(%126, meta[relay.Constant][87] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %128 = add(%127, %117) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %129 = nn.relu(%128) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %130 = nn.conv2d(%129, meta[relay.Constant][88] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %131 = add(%130, meta[relay.Constant][89] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %132 = add(%131, meta[relay.Constant][90] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %133 = nn.relu(%132) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %134 = nn.conv2d(%133, meta[relay.Constant][91] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %135 = add(%134, meta[relay.Constant][92] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %136 = nn.relu(%135) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %137 = nn.conv2d(%136, meta[relay.Constant][93] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %138 = add(%137, meta[relay.Constant][94] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %139 = add(%138, meta[relay.Constant][95] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %140 = add(%139, %129) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %141 = nn.relu(%140) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %142 = nn.conv2d(%141, meta[relay.Constant][96] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %143 = add(%142, meta[relay.Constant][97] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %144 = add(%143, meta[relay.Constant][98] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %145 = nn.relu(%144) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %146 = nn.conv2d(%145, meta[relay.Constant][99] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %147 = add(%146, meta[relay.Constant][100] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %148 = nn.relu(%147) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %149 = nn.conv2d(%148, meta[relay.Constant][101] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %150 = add(%149, meta[relay.Constant][102] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %151 = add(%150, meta[relay.Constant][103] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %152 = add(%151, %141) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %153 = nn.relu(%152) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %154 = nn.conv2d(%153, meta[relay.Constant][104] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %155 = add(%154, meta[relay.Constant][105] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %156 = add(%155, meta[relay.Constant][106] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %158 = nn.conv2d(%157, meta[relay.Constant][107] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %159 = add(%158, meta[relay.Constant][108] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %161 = nn.conv2d(%160, meta[relay.Constant][109] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %162 = add(%161, meta[relay.Constant][110] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %163 = add(%162, meta[relay.Constant][111] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %164 = add(%163, %153) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %165 = nn.relu(%164) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %166 = nn.conv2d(%165, meta[relay.Constant][112] /* ty=Tensor[(512, 1024, 1, 1), float32] */ /* ty=Tensor[(512, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %167 = add(%166, meta[relay.Constant][113] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %168 = add(%167, meta[relay.Constant][114] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %169 = nn.relu(%168) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %170 = nn.conv2d(%169, meta[relay.Constant][115] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %171 = add(%170, meta[relay.Constant][116] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %173 = nn.conv2d(%172, meta[relay.Constant][117] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %174 = add(%173, meta[relay.Constant][118] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %175 = add(%174, meta[relay.Constant][119] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %176 = nn.conv2d(%165, meta[relay.Constant][120] /* ty=Tensor[(2048, 1024, 1, 1), float32] */ /* ty=Tensor[(2048, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %177 = add(%176, meta[relay.Constant][121] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %178 = add(%175, %177) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %180 = nn.conv2d(%179, meta[relay.Constant][122] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %181 = add(%180, meta[relay.Constant][123] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %182 = add(%181, meta[relay.Constant][124] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %183 = nn.relu(%182) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %184 = nn.conv2d(%183, meta[relay.Constant][125] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %185 = add(%184, meta[relay.Constant][126] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %186 = nn.relu(%185) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %187 = nn.conv2d(%186, meta[relay.Constant][127] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %188 = add(%187, meta[relay.Constant][128] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %189 = add(%188, meta[relay.Constant][129] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %190 = add(%189, %179) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %191 = nn.relu(%190) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %192 = nn.conv2d(%191, meta[relay.Constant][130] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %193 = add(%192, meta[relay.Constant][131] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %194 = add(%193, meta[relay.Constant][132] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %195 = nn.relu(%194) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %196 = nn.conv2d(%195, meta[relay.Constant][133] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %197 = add(%196, meta[relay.Constant][134] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %198 = nn.relu(%197) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %199 = nn.conv2d(%198, meta[relay.Constant][135] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %200 = add(%199, meta[relay.Constant][136] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %201 = add(%200, meta[relay.Constant][137] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %202 = add(%201, %191) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %203 = nn.relu(%202) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %204 = nn.global_avg_pool2d(%203) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %205 = nn.batch_flatten(%204) /* ty=Tensor[(32, 2048), float32] */;
  %206 = nn.dense(%205, meta[relay.Constant][138] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%206, meta[relay.Constant][139] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
Simulated graph fn (%data: Tensor[(32, 3, 224, 224), float32], %in_scale0: float32, %out_scale0: float32, %clip_min0: float32, %clip_max0: float32, %in_scale1: float32, %out_scale1: float32, %clip_min1: float32, %clip_max1: float32, %in_scale2: float32, %out_scale2: float32, %clip_min2: float32, %clip_max2: float32, %in_scale3: float32, %out_scale3: float32, %clip_min3: float32, %clip_max3: float32, %in_scale4: float32, %out_scale4: float32, %clip_min4: float32, %clip_max4: float32, %in_scale5: float32, %out_scale5: float32, %clip_min5: float32, %clip_max5: float32, %in_scale6: float32, %out_scale6: float32, %clip_min6: float32, %clip_max6: float32, %in_scale7: float32, %out_scale7: float32, %clip_min7: float32, %clip_max7: float32, %in_scale8: float32, %out_scale8: float32, %clip_min8: float32, %clip_max8: float32, %in_scale9: float32, %out_scale9: float32, %clip_min9: float32, %clip_max9: float32, %in_scale10: float32, %out_scale10: float32, %clip_min10: float32, %clip_max10: float32, %in_scale11: float32, %out_scale11: float32, %clip_min11: float32, %clip_max11: float32, %in_scale12: float32, %out_scale12: float32, %clip_min12: float32, %clip_max12: float32, %in_scale13: float32, %out_scale13: float32, %clip_min13: float32, %clip_max13: float32, %in_scale14: float32, %out_scale14: float32, %clip_min14: float32, %clip_max14: float32, %in_scale15: float32, %out_scale15: float32, %clip_min15: float32, %clip_max15: float32, %in_scale16: float32, %out_scale16: float32, %clip_min16: float32, %clip_max16: float32, %in_scale17: float32, %out_scale17: float32, %clip_min17: float32, %clip_max17: float32, %in_scale18: float32, %out_scale18: float32, %clip_min18: float32, %clip_max18: float32, %in_scale19: float32, %out_scale19: float32, %clip_min19: float32, %clip_max19: float32, %in_scale20: float32, %out_scale20: float32, %clip_min20: float32, %clip_max20: float32, %in_scale21: float32, %out_scale21: float32, %clip_min21: float32, %clip_max21: float32, %in_scale22: float32, %out_scale22: float32, %clip_min22: float32, %clip_max22: float32, %in_scale23: float32, %out_scale23: float32, %clip_min23: float32, %clip_max23: float32, %in_scale28: float32, %out_scale28: float32, %clip_min28: float32, %clip_max28: float32, %in_scale24: float32, %out_scale24: float32, %clip_min24: float32, %clip_max24: float32, %in_scale25: float32, %out_scale25: float32, %clip_min25: float32, %clip_max25: float32, %in_scale26: float32, %out_scale26: float32, %clip_min26: float32, %clip_max26: float32, %in_scale27: float32, %out_scale27: float32, %clip_min27: float32, %clip_max27: float32, %in_scale29: float32, %out_scale29: float32, %clip_min29: float32, %clip_max29: float32, %in_scale30: float32, %out_scale30: float32, %clip_min30: float32, %clip_max30: float32, %in_scale31: float32, %out_scale31: float32, %clip_min31: float32, %clip_max31: float32, %in_scale32: float32, %out_scale32: float32, %clip_min32: float32, %clip_max32: float32, %in_scale33: float32, %out_scale33: float32, %clip_min33: float32, %clip_max33: float32, %in_scale34: float32, %out_scale34: float32, %clip_min34: float32, %clip_max34: float32, %in_scale35: float32, %out_scale35: float32, %clip_min35: float32, %clip_max35: float32, %in_scale36: float32, %out_scale36: float32, %clip_min36: float32, %clip_max36: float32, %in_scale37: float32, %out_scale37: float32, %clip_min37: float32, %clip_max37: float32, %in_scale38: float32, %out_scale38: float32, %clip_min38: float32, %clip_max38: float32, %in_scale39: float32, %out_scale39: float32, %clip_min39: float32, %clip_max39: float32, %in_scale40: float32, %out_scale40: float32, %clip_min40: float32, %clip_max40: float32, %in_scale41: float32, %out_scale41: float32, %clip_min41: float32, %clip_max41: float32, %in_scale42: float32, %out_scale42: float32, %clip_min42: float32, %clip_max42: float32, %in_scale43: float32, %out_scale43: float32, %clip_min43: float32, %clip_max43: float32, %in_scale44: float32, %out_scale44: float32, %clip_min44: float32, %clip_max44: float32, %in_scale45: float32, %out_scale45: float32, %clip_min45: float32, %clip_max45: float32, %in_scale46: float32, %out_scale46: float32, %clip_min46: float32, %clip_max46: float32, %in_scale47: float32, %out_scale47: float32, %clip_min47: float32, %clip_max47: float32, %in_scale48: float32, %out_scale48: float32, %clip_min48: float32, %clip_max48: float32, %in_scale49: float32, %out_scale49: float32, %clip_min49: float32, %clip_max49: float32, %in_scale50: float32, %out_scale50: float32, %clip_min50: float32, %clip_max50: float32, %in_scale51: float32, %out_scale51: float32, %clip_min51: float32, %clip_max51: float32, %in_scale52: float32, %out_scale52: float32, %clip_min52: float32, %clip_max52: float32, %in_scale53: float32, %out_scale53: float32, %clip_min53: float32, %clip_max53: float32, %in_scale54: float32, %out_scale54: float32, %clip_min54: float32, %clip_max54: float32, %in_scale55: float32, %out_scale55: float32, %clip_min55: float32, %clip_max55: float32, %in_scale56: float32, %out_scale56: float32, %clip_min56: float32, %clip_max56: float32, %in_scale57: float32, %out_scale57: float32, %clip_min57: float32, %clip_max57: float32, %in_scale58: float32, %out_scale58: float32, %clip_min58: float32, %clip_max58: float32, %in_scale59: float32, %out_scale59: float32, %clip_min59: float32, %clip_max59: float32, %in_scale60: float32, %out_scale60: float32, %clip_min60: float32, %clip_max60: float32, %in_scale61: float32, %out_scale61: float32, %clip_min61: float32, %clip_max61: float32, %in_scale62: float32, %out_scale62: float32, %clip_min62: float32, %clip_max62: float32, %in_scale63: float32, %out_scale63: float32, %clip_min63: float32, %clip_max63: float32, %in_scale64: float32, %out_scale64: float32, %clip_min64: float32, %clip_max64: float32, %in_scale65: float32, %out_scale65: float32, %clip_min65: float32, %clip_max65: float32, %in_scale66: float32, %out_scale66: float32, %clip_min66: float32, %clip_max66: float32, %in_scale67: float32, %out_scale67: float32, %clip_min67: float32, %clip_max67: float32, %in_scale68: float32, %out_scale68: float32, %clip_min68: float32, %clip_max68: float32, %in_scale69: float32, %out_scale69: float32, %clip_min69: float32, %clip_max69: float32, %in_scale70: float32, %out_scale70: float32, %clip_min70: float32, %clip_max70: float32, %in_scale71: float32, %out_scale71: float32, %clip_min71: float32, %clip_max71: float32, %in_scale72: float32, %out_scale72: float32, %clip_min72: float32, %clip_max72: float32, %in_scale73: float32, %out_scale73: float32, %clip_min73: float32, %clip_max73: float32, %in_scale74: float32, %out_scale74: float32, %clip_min74: float32, %clip_max74: float32, %in_scale75: float32, %out_scale75: float32, %clip_min75: float32, %clip_max75: float32, %in_scale76: float32, %out_scale76: float32, %clip_min76: float32, %clip_max76: float32, %in_scale77: float32, %out_scale77: float32, %clip_min77: float32, %clip_max77: float32, %in_scale78: float32, %out_scale78: float32, %clip_min78: float32, %clip_max78: float32, %in_scale79: float32, %out_scale79: float32, %clip_min79: float32, %clip_max79: float32, %in_scale80: float32, %out_scale80: float32, %clip_min80: float32, %clip_max80: float32, %in_scale81: float32, %out_scale81: float32, %clip_min81: float32, %clip_max81: float32, %in_scale82: float32, %out_scale82: float32, %clip_min82: float32, %clip_max82: float32, %in_scale83: float32, %out_scale83: float32, %clip_min83: float32, %clip_max83: float32, %in_scale84: float32, %out_scale84: float32, %clip_min84: float32, %clip_max84: float32, %in_scale85: float32, %out_scale85: float32, %clip_min85: float32, %clip_max85: float32, %in_scale86: float32, %out_scale86: float32, %clip_min86: float32, %clip_max86: float32, %in_scale87: float32, %out_scale87: float32, %clip_min87: float32, %clip_max87: float32, %in_scale88: float32, %out_scale88: float32, %clip_min88: float32, %clip_max88: float32, %in_scale89: float32, %out_scale89: float32, %clip_min89: float32, %clip_max89: float32, %in_scale90: float32, %out_scale90: float32, %clip_min90: float32, %clip_max90: float32, %in_scale95: float32, %out_scale95: float32, %clip_min95: float32, %clip_max95: float32, %in_scale91: float32, %out_scale91: float32, %clip_min91: float32, %clip_max91: float32, %in_scale92: float32, %out_scale92: float32, %clip_min92: float32, %clip_max92: float32, %in_scale93: float32, %out_scale93: float32, %clip_min93: float32, %clip_max93: float32, %in_scale94: float32, %out_scale94: float32, %clip_min94: float32, %clip_max94: float32, %in_scale96: float32, %out_scale96: float32, %clip_min96: float32, %clip_max96: float32, %in_scale97: float32, %out_scale97: float32, %clip_min97: float32, %clip_max97: float32, %in_scale98: float32, %out_scale98: float32, %clip_min98: float32, %clip_max98: float32, %in_scale99: float32, %out_scale99: float32, %clip_min99: float32, %clip_max99: float32, %in_scale100: float32, %out_scale100: float32, %clip_min100: float32, %clip_max100: float32, %in_scale101: float32, %out_scale101: float32, %clip_min101: float32, %clip_max101: float32, %in_scale102: float32, %out_scale102: float32, %clip_min102: float32, %clip_max102: float32, %in_scale103: float32, %out_scale103: float32, %clip_min103: float32, %clip_max103: float32, %in_scale104: float32, %out_scale104: float32, %clip_min104: float32, %clip_max104: float32, %in_scale105: float32, %out_scale105: float32, %clip_min105: float32, %clip_max105: float32, %in_scale106: float32, %out_scale106: float32, %clip_min106: float32, %clip_max106: float32, %in_scale107: float32, %out_scale107: float32, %clip_min107: float32, %clip_max107: float32, %in_scale108: float32, %out_scale108: float32, %clip_min108: float32, %clip_max108: float32, %in_scale109: float32, %out_scale109: float32, %clip_min109: float32, %clip_max109: float32, %in_scale110: float32, %out_scale110: float32, %clip_min110: float32, %clip_max110: float32, %in_scale111: float32, %out_scale111: float32, %clip_min111: float32, %clip_max111: float32, %in_scale112: float32, %out_scale112: float32, %clip_min112: float32, %clip_max112: float32, %in_scale113: float32, %out_scale113: float32, %clip_min113: float32, %clip_max113: float32, %in_scale114: float32, %out_scale114: float32, %clip_min114: float32, %clip_max114: float32, %in_scale115: float32, %out_scale115: float32, %clip_min115: float32, %clip_max115: float32, %in_scale116: float32, %out_scale116: float32, %clip_min116: float32, %clip_max116: float32, %in_scale117: float32, %out_scale117: float32, %clip_min117: float32, %clip_max117: float32, %in_scale118: float32, %out_scale118: float32, %clip_min118: float32, %clip_max118: float32, %in_scale119: float32, %out_scale119: float32, %clip_min119: float32, %clip_max119: float32, %in_scale120: float32, %out_scale120: float32, %clip_min120: float32, %clip_max120: float32, %in_scale121: float32, %out_scale121: float32, %clip_min121: float32, %clip_max121: float32, %in_scale122: float32, %out_scale122: float32, %clip_min122: float32, %clip_max122: float32, %in_scale123: float32, %out_scale123: float32, %clip_min123: float32, %clip_max123: float32, %in_scale124: float32, %out_scale124: float32, %clip_min124: float32, %clip_max124: float32, %in_scale125: float32, %out_scale125: float32, %clip_min125: float32, %clip_max125: float32, %in_scale126: float32, %out_scale126: float32, %clip_min126: float32, %clip_max126: float32, %in_scale127: float32, %out_scale127: float32, %clip_min127: float32, %clip_max127: float32, %in_scale128: float32, %out_scale128: float32, %clip_min128: float32, %clip_max128: float32, %in_scale129: float32, %out_scale129: float32, %clip_min129: float32, %clip_max129: float32, %in_scale130: float32, %out_scale130: float32, %clip_min130: float32, %clip_max130: float32, %in_scale131: float32, %out_scale131: float32, %clip_min131: float32, %clip_max131: float32, %in_scale132: float32, %out_scale132: float32, %clip_min132: float32, %clip_max132: float32, %in_scale133: float32, %out_scale133: float32, %clip_min133: float32, %clip_max133: float32, %in_scale134: float32, %out_scale134: float32, %clip_min134: float32, %clip_max134: float32, %in_scale135: float32, %out_scale135: float32, %clip_min135: float32, %clip_max135: float32, %in_scale136: float32, %out_scale136: float32, %clip_min136: float32, %clip_max136: float32, %in_scale137: float32, %out_scale137: float32, %clip_min137: float32, %clip_max137: float32, %in_scale138: float32, %out_scale138: float32, %clip_min138: float32, %clip_max138: float32, %in_scale139: float32, %out_scale139: float32, %clip_min139: float32, %clip_max139: float32, %in_scale140: float32, %out_scale140: float32, %clip_min140: float32, %clip_max140: float32, %in_scale141: float32, %out_scale141: float32, %clip_min141: float32, %clip_max141: float32, %in_scale142: float32, %out_scale142: float32, %clip_min142: float32, %clip_max142: float32, %in_scale143: float32, %out_scale143: float32, %clip_min143: float32, %clip_max143: float32, %in_scale144: float32, %out_scale144: float32, %clip_min144: float32, %clip_max144: float32, %in_scale145: float32, %out_scale145: float32, %clip_min145: float32, %clip_max145: float32, %in_scale146: float32, %out_scale146: float32, %clip_min146: float32, %clip_max146: float32, %in_scale147: float32, %out_scale147: float32, %clip_min147: float32, %clip_max147: float32, %in_scale148: float32, %out_scale148: float32, %clip_min148: float32, %clip_max148: float32, %in_scale149: float32, %out_scale149: float32, %clip_min149: float32, %clip_max149: float32, %in_scale150: float32, %out_scale150: float32, %clip_min150: float32, %clip_max150: float32, %in_scale151: float32, %out_scale151: float32, %clip_min151: float32, %clip_max151: float32, %in_scale152: float32, %out_scale152: float32, %clip_min152: float32, %clip_max152: float32, %in_scale153: float32, %out_scale153: float32, %clip_min153: float32, %clip_max153: float32, %in_scale154: float32, %out_scale154: float32, %clip_min154: float32, %clip_max154: float32, %in_scale155: float32, %out_scale155: float32, %clip_min155: float32, %clip_max155: float32, %in_scale156: float32, %out_scale156: float32, %clip_min156: float32, %clip_max156: float32, %in_scale157: float32, %out_scale157: float32, %clip_min157: float32, %clip_max157: float32, %in_scale158: float32, %out_scale158: float32, %clip_min158: float32, %clip_max158: float32, %in_scale159: float32, %out_scale159: float32, %clip_min159: float32, %clip_max159: float32, %in_scale160: float32, %out_scale160: float32, %clip_min160: float32, %clip_max160: float32, %in_scale161: float32, %out_scale161: float32, %clip_min161: float32, %clip_max161: float32, %in_scale162: float32, %out_scale162: float32, %clip_min162: float32, %clip_max162: float32, %in_scale163: float32, %out_scale163: float32, %clip_min163: float32, %clip_max163: float32, %in_scale164: float32, %out_scale164: float32, %clip_min164: float32, %clip_max164: float32, %in_scale165: float32, %out_scale165: float32, %clip_min165: float32, %clip_max165: float32, %in_scale166: float32, %out_scale166: float32, %clip_min166: float32, %clip_max166: float32, %in_scale167: float32, %out_scale167: float32, %clip_min167: float32, %clip_max167: float32, %in_scale168: float32, %out_scale168: float32, %clip_min168: float32, %clip_max168: float32, %in_scale169: float32, %out_scale169: float32, %clip_min169: float32, %clip_max169: float32, %in_scale170: float32, %out_scale170: float32, %clip_min170: float32, %clip_max170: float32, %in_scale171: float32, %out_scale171: float32, %clip_min171: float32, %clip_max171: float32, %in_scale172: float32, %out_scale172: float32, %clip_min172: float32, %clip_max172: float32, %in_scale173: float32, %out_scale173: float32, %clip_min173: float32, %clip_max173: float32, %in_scale174: float32, %out_scale174: float32, %clip_min174: float32, %clip_max174: float32, %in_scale175: float32, %out_scale175: float32, %clip_min175: float32, %clip_max175: float32, %in_scale176: float32, %out_scale176: float32, %clip_min176: float32, %clip_max176: float32, %in_scale177: float32, %out_scale177: float32, %clip_min177: float32, %clip_max177: float32, %in_scale178: float32, %out_scale178: float32, %clip_min178: float32, %clip_max178: float32, %in_scale183: float32, %out_scale183: float32, %clip_min183: float32, %clip_max183: float32, %in_scale179: float32, %out_scale179: float32, %clip_min179: float32, %clip_max179: float32, %in_scale180: float32, %out_scale180: float32, %clip_min180: float32, %clip_max180: float32, %in_scale181: float32, %out_scale181: float32, %clip_min181: float32, %clip_max181: float32, %in_scale182: float32, %out_scale182: float32, %clip_min182: float32, %clip_max182: float32, %in_scale184: float32, %out_scale184: float32, %clip_min184: float32, %clip_max184: float32, %in_scale185: float32, %out_scale185: float32, %clip_min185: float32, %clip_max185: float32, %in_scale186: float32, %out_scale186: float32, %clip_min186: float32, %clip_max186: float32, %in_scale187: float32, %out_scale187: float32, %clip_min187: float32, %clip_max187: float32, %in_scale188: float32, %out_scale188: float32, %clip_min188: float32, %clip_max188: float32, %in_scale189: float32, %out_scale189: float32, %clip_min189: float32, %clip_max189: float32, %in_scale190: float32, %out_scale190: float32, %clip_min190: float32, %clip_max190: float32, %in_scale191: float32, %out_scale191: float32, %clip_min191: float32, %clip_max191: float32, %in_scale192: float32, %out_scale192: float32, %clip_min192: float32, %clip_max192: float32, %in_scale193: float32, %out_scale193: float32, %clip_min193: float32, %clip_max193: float32, %in_scale194: float32, %out_scale194: float32, %clip_min194: float32, %clip_max194: float32, %in_scale195: float32, %out_scale195: float32, %clip_min195: float32, %clip_max195: float32, %in_scale196: float32, %out_scale196: float32, %clip_min196: float32, %clip_max196: float32, %in_scale197: float32, %out_scale197: float32, %clip_min197: float32, %clip_max197: float32, %in_scale198: float32, %out_scale198: float32, %clip_min198: float32, %clip_max198: float32, %in_scale199: float32, %out_scale199: float32, %clip_min199: float32, %clip_max199: float32, %in_scale200: float32, %out_scale200: float32, %clip_min200: float32, %clip_max200: float32, %in_scale201: float32, %out_scale201: float32, %clip_min201: float32, %clip_max201: float32, %in_scale202: float32, %out_scale202: float32, %clip_min202: float32, %clip_max202: float32, %in_scale203: float32, %out_scale203: float32, %clip_min203: float32, %clip_max203: float32, %in_scale204: float32, %out_scale204: float32, %clip_min204: float32, %clip_max204: float32, %in_scale205: float32, %out_scale205: float32, %clip_min205: float32, %clip_max205: float32, %in_scale206: float32, %out_scale206: float32, %clip_min206: float32, %clip_max206: float32, %in_scale207: float32, %out_scale207: float32, %clip_min207: float32, %clip_max207: float32, %in_scale208: float32, %out_scale208: float32, %clip_min208: float32, %clip_max208: float32, %in_scale209: float32, %out_scale209: float32, %clip_min209: float32, %clip_max209: float32, %in_scale210: float32, %out_scale210: float32, %clip_min210: float32, %clip_max210: float32, %in_scale211: float32, %out_scale211: float32, %clip_min211: float32, %clip_max211: float32, %in_scale212: float32, %out_scale212: float32, %clip_min212: float32, %clip_max212: float32, %in_scale213: float32, %out_scale213: float32, %clip_min213: float32, %clip_max213: float32, %in_scale214: float32, %out_scale214: float32, %clip_min214: float32, %clip_max214: float32, %in_scale215: float32, %out_scale215: float32, %clip_min215: float32, %clip_max215: float32, %in_scale216: float32, %out_scale216: float32, %clip_min216: float32, %clip_max216: float32, %in_scale217: float32, %out_scale217: float32, %clip_min217: float32, %clip_max217: float32, %in_scale218: float32, %out_scale218: float32, %clip_min218: float32, %clip_max218: float32, %in_scale219: float32, %out_scale219: float32, %clip_min219: float32, %clip_max219: float32, %in_scale220: float32, %out_scale220: float32, %clip_min220: float32, %clip_max220: float32, %in_scale221: float32, %out_scale221: float32, %clip_min221: float32, %clip_max221: float32, %in_scale222: float32, %out_scale222: float32, %clip_min222: float32, %clip_max222: float32, %in_scale223: float32, %out_scale223: float32, %clip_min223: float32, %clip_max223: float32, %in_scale224: float32, %out_scale224: float32, %clip_min224: float32, %clip_max224: float32, %in_scale225: float32, %out_scale225: float32, %clip_min225: float32, %clip_max225: float32, %in_scale226: float32, %out_scale226: float32, %clip_min226: float32, %clip_max226: float32, %in_scale227: float32, %out_scale227: float32, %clip_min227: float32, %clip_max227: float32, %in_scale228: float32, %out_scale228: float32, %clip_min228: float32, %clip_max228: float32, %in_scale229: float32, %out_scale229: float32, %clip_min229: float32, %clip_max229: float32, %in_scale230: float32, %out_scale230: float32, %clip_min230: float32, %clip_max230: float32, %in_scale231: float32, %out_scale231: float32, %clip_min231: float32, %clip_max231: float32, %in_scale232: float32, %out_scale232: float32, %clip_min232: float32, %clip_max232: float32, %in_scale233: float32, %out_scale233: float32, %clip_min233: float32, %clip_max233: float32, %in_scale234: float32, %out_scale234: float32, %clip_min234: float32, %clip_max234: float32, %in_scale235: float32, %out_scale235: float32, %clip_min235: float32, %clip_max235: float32, %in_scale236: float32, %out_scale236: float32, %clip_min236: float32, %clip_max236: float32, %in_scale237: float32, %out_scale237: float32, %clip_min237: float32, %clip_max237: float32, %in_scale238: float32, %out_scale238: float32, %clip_min238: float32, %clip_max238: float32, %in_scale239: float32, %out_scale239: float32, %clip_min239: float32, %clip_max239: float32, %in_scale240: float32, %out_scale240: float32, %clip_min240: float32, %clip_max240: float32, %in_scale241: float32, %out_scale241: float32, %clip_min241: float32, %clip_max241: float32, %in_scale242: float32, %out_scale242: float32, %clip_min242: float32, %clip_max242: float32, %in_scale243: float32, %out_scale243: float32, %clip_min243: float32, %clip_max243: float32, %in_scale244: float32, %out_scale244: float32, %clip_min244: float32, %clip_max244: float32, %in_scale245: float32, %out_scale245: float32, %clip_min245: float32, %clip_max245: float32, %in_scale246: float32, %out_scale246: float32, %clip_min246: float32, %clip_max246: float32, %in_scale247: float32, %out_scale247: float32, %clip_min247: float32, %clip_max247: float32, %in_scale248: float32, %out_scale248: float32, %clip_min248: float32, %clip_max248: float32, %in_scale249: float32, %out_scale249: float32, %clip_min249: float32, %clip_max249: float32, %in_scale250: float32, %out_scale250: float32, %clip_min250: float32, %clip_max250: float32, %in_scale251: float32, %out_scale251: float32, %clip_min251: float32, %clip_max251: float32, %in_scale252: float32, %out_scale252: float32, %clip_min252: float32, %clip_max252: float32, %in_scale253: float32, %out_scale253: float32, %clip_min253: float32, %clip_max253: float32, %in_scale254: float32, %out_scale254: float32, %clip_min254: float32, %clip_max254: float32, %in_scale255: float32, %out_scale255: float32, %clip_min255: float32, %clip_max255: float32, %in_scale256: float32, %out_scale256: float32, %clip_min256: float32, %clip_max256: float32, %in_scale257: float32, %out_scale257: float32, %clip_min257: float32, %clip_max257: float32, %in_scale258: float32, %out_scale258: float32, %clip_min258: float32, %clip_max258: float32, %in_scale259: float32, %out_scale259: float32, %clip_min259: float32, %clip_max259: float32, %in_scale260: float32, %out_scale260: float32, %clip_min260: float32, %clip_max260: float32, %in_scale261: float32, %out_scale261: float32, %clip_min261: float32, %clip_max261: float32, %in_scale262: float32, %out_scale262: float32, %clip_min262: float32, %clip_max262: float32, %in_scale263: float32, %out_scale263: float32, %clip_min263: float32, %clip_max263: float32, %in_scale264: float32, %out_scale264: float32, %clip_min264: float32, %clip_max264: float32, %in_scale265: float32, %out_scale265: float32, %clip_min265: float32, %clip_max265: float32, %in_scale266: float32, %out_scale266: float32, %clip_min266: float32, %clip_max266: float32, %in_scale267: float32, %out_scale267: float32, %clip_min267: float32, %clip_max267: float32, %in_scale268: float32, %out_scale268: float32, %clip_min268: float32, %clip_max268: float32, %in_scale269: float32, %out_scale269: float32, %clip_min269: float32, %clip_max269: float32, %in_scale270: float32, %out_scale270: float32, %clip_min270: float32, %clip_max270: float32, %in_scale271: float32, %out_scale271: float32, %clip_min271: float32, %clip_max271: float32, %in_scale272: float32, %out_scale272: float32, %clip_min272: float32, %clip_max272: float32, %in_scale273: float32, %out_scale273: float32, %clip_min273: float32, %clip_max273: float32, %in_scale274: float32, %out_scale274: float32, %clip_min274: float32, %clip_max274: float32, %in_scale275: float32, %out_scale275: float32, %clip_min275: float32, %clip_max275: float32, %in_scale276: float32, %out_scale276: float32, %clip_min276: float32, %clip_max276: float32, %in_scale277: float32, %out_scale277: float32, %clip_min277: float32, %clip_max277: float32, %in_scale278: float32, %out_scale278: float32, %clip_min278: float32, %clip_max278: float32, %in_scale279: float32, %out_scale279: float32, %clip_min279: float32, %clip_max279: float32, %in_scale280: float32, %out_scale280: float32, %clip_min280: float32, %clip_max280: float32, %in_scale281: float32, %out_scale281: float32, %clip_min281: float32, %clip_max281: float32, %in_scale282: float32, %out_scale282: float32, %clip_min282: float32, %clip_max282: float32, %in_scale283: float32, %out_scale283: float32, %clip_min283: float32, %clip_max283: float32, %in_scale284: float32, %out_scale284: float32, %clip_min284: float32, %clip_max284: float32, %in_scale285: float32, %out_scale285: float32, %clip_min285: float32, %clip_max285: float32, %in_scale286: float32, %out_scale286: float32, %clip_min286: float32, %clip_max286: float32, %in_scale287: float32, %out_scale287: float32, %clip_min287: float32, %clip_max287: float32, %in_scale288: float32, %out_scale288: float32, %clip_min288: float32, %clip_max288: float32, %in_scale289: float32, %out_scale289: float32, %clip_min289: float32, %clip_max289: float32, %in_scale290: float32, %out_scale290: float32, %clip_min290: float32, %clip_max290: float32, %in_scale291: float32, %out_scale291: float32, %clip_min291: float32, %clip_max291: float32, %in_scale292: float32, %out_scale292: float32, %clip_min292: float32, %clip_max292: float32, %in_scale293: float32, %out_scale293: float32, %clip_min293: float32, %clip_max293: float32, %in_scale294: float32, %out_scale294: float32, %clip_min294: float32, %clip_max294: float32, %in_scale295: float32, %out_scale295: float32, %clip_min295: float32, %clip_max295: float32, %in_scale296: float32, %out_scale296: float32, %clip_min296: float32, %clip_max296: float32, %in_scale297: float32, %out_scale297: float32, %clip_min297: float32, %clip_max297: float32, %in_scale298: float32, %out_scale298: float32, %clip_min298: float32, %clip_max298: float32, %in_scale299: float32, %out_scale299: float32, %clip_min299: float32, %clip_max299: float32, %in_scale300: float32, %out_scale300: float32, %clip_min300: float32, %clip_max300: float32, %in_scale301: float32, %out_scale301: float32, %clip_min301: float32, %clip_max301: float32, %in_scale302: float32, %out_scale302: float32, %clip_min302: float32, %clip_max302: float32, %in_scale303: float32, %out_scale303: float32, %clip_min303: float32, %clip_max303: float32, %in_scale304: float32, %out_scale304: float32, %clip_min304: float32, %clip_max304: float32, %in_scale305: float32, %out_scale305: float32, %clip_min305: float32, %clip_max305: float32, %in_scale306: float32, %out_scale306: float32, %clip_min306: float32, %clip_max306: float32, %in_scale307: float32, %out_scale307: float32, %clip_min307: float32, %clip_max307: float32, %in_scale308: float32, %out_scale308: float32, %clip_min308: float32, %clip_max308: float32, %in_scale313: float32, %out_scale313: float32, %clip_min313: float32, %clip_max313: float32, %in_scale309: float32, %out_scale309: float32, %clip_min309: float32, %clip_max309: float32, %in_scale310: float32, %out_scale310: float32, %clip_min310: float32, %clip_max310: float32, %in_scale311: float32, %out_scale311: float32, %clip_min311: float32, %clip_max311: float32, %in_scale312: float32, %out_scale312: float32, %clip_min312: float32, %clip_max312: float32, %in_scale314: float32, %out_scale314: float32, %clip_min314: float32, %clip_max314: float32, %in_scale315: float32, %out_scale315: float32, %clip_min315: float32, %clip_max315: float32, %in_scale316: float32, %out_scale316: float32, %clip_min316: float32, %clip_max316: float32, %in_scale317: float32, %out_scale317: float32, %clip_min317: float32, %clip_max317: float32, %in_scale318: float32, %out_scale318: float32, %clip_min318: float32, %clip_max318: float32, %in_scale319: float32, %out_scale319: float32, %clip_min319: float32, %clip_max319: float32, %in_scale320: float32, %out_scale320: float32, %clip_min320: float32, %clip_max320: float32, %in_scale321: float32, %out_scale321: float32, %clip_min321: float32, %clip_max321: float32, %in_scale322: float32, %out_scale322: float32, %clip_min322: float32, %clip_max322: float32, %in_scale323: float32, %out_scale323: float32, %clip_min323: float32, %clip_max323: float32, %in_scale324: float32, %out_scale324: float32, %clip_min324: float32, %clip_max324: float32, %in_scale325: float32, %out_scale325: float32, %clip_min325: float32, %clip_max325: float32, %in_scale326: float32, %out_scale326: float32, %clip_min326: float32, %clip_max326: float32, %in_scale327: float32, %out_scale327: float32, %clip_min327: float32, %clip_max327: float32, %in_scale328: float32, %out_scale328: float32, %clip_min328: float32, %clip_max328: float32, %in_scale329: float32, %out_scale329: float32, %clip_min329: float32, %clip_max329: float32, %in_scale330: float32, %out_scale330: float32, %clip_min330: float32, %clip_max330: float32, %in_scale331: float32, %out_scale331: float32, %clip_min331: float32, %clip_max331: float32, %in_scale332: float32, %out_scale332: float32, %clip_min332: float32, %clip_max332: float32, %in_scale333: float32, %out_scale333: float32, %clip_min333: float32, %clip_max333: float32, %in_scale334: float32, %out_scale334: float32, %clip_min334: float32, %clip_max334: float32, %in_scale335: float32, %out_scale335: float32, %clip_min335: float32, %clip_max335: float32, %in_scale336: float32, %out_scale336: float32, %clip_min336: float32, %clip_max336: float32, %in_scale337: float32, %out_scale337: float32, %clip_min337: float32, %clip_max337: float32, %in_scale338: float32, %out_scale338: float32, %clip_min338: float32, %clip_max338: float32, %in_scale339: float32, %out_scale339: float32, %clip_min339: float32, %clip_max339: float32, %in_scale340: float32, %out_scale340: float32, %clip_min340: float32, %clip_max340: float32, %in_scale341: float32, %out_scale341: float32, %clip_min341: float32, %clip_max341: float32, %in_scale342: float32, %out_scale342: float32, %clip_min342: float32, %clip_max342: float32, %in_scale343: float32, %out_scale343: float32, %clip_min343: float32, %clip_max343: float32, %in_scale344: float32, %out_scale344: float32, %clip_min344: float32, %clip_max344: float32, %in_scale345: float32, %out_scale345: float32, %clip_min345: float32, %clip_max345: float32, %in_scale346: float32, %out_scale346: float32, %clip_min346: float32, %clip_max346: float32, %in_scale347: float32, %out_scale347: float32, %clip_min347: float32, %clip_max347: float32, %in_scale348: float32, %out_scale348: float32, %clip_min348: float32, %clip_max348: float32, %in_scale349: float32, %out_scale349: float32, %clip_min349: float32, %clip_max349: float32, %in_scale350: float32, %out_scale350: float32, %clip_min350: float32, %clip_max350: float32, %in_scale351: float32, %out_scale351: float32, %clip_min351: float32, %clip_max351: float32, %in_scale352: float32, %out_scale352: float32, %clip_min352: float32, %clip_max352: float32, %in_scale353: float32, %out_scale353: float32, %clip_min353: float32, %clip_max353: float32, %in_scale354: float32, %out_scale354: float32, %clip_min354: float32, %clip_max354: float32, %in_scale355: float32, %out_scale355: float32, %clip_min355: float32, %clip_max355: float32, %in_scale356: float32, %out_scale356: float32, %clip_min356: float32, %clip_max356: float32, %in_scale357: float32, %out_scale357: float32, %clip_min357: float32, %clip_max357: float32, %in_scale358: float32, %out_scale358: float32, %clip_min358: float32, %clip_max358: float32, %in_scale359: float32, %out_scale359: float32, %clip_min359: float32, %clip_max359: float32, %in_scale360: float32, %out_scale360: float32, %clip_min360: float32, %clip_max360: float32, %in_scale361: float32, %out_scale361: float32, %clip_min361: float32, %clip_max361: float32, %in_scale362: float32, %out_scale362: float32, %clip_min362: float32, %clip_max362: float32, %in_scale363: float32, %out_scale363: float32, %clip_min363: float32, %clip_max363: float32, %in_scale364: float32, %out_scale364: float32, %clip_min364: float32, %clip_max364: float32) -> Tensor[(32, 1000), float32] {
  %0 = nn.simulated_quantize(%data, %in_scale0, %out_scale0, %clip_min0, %clip_max0, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 3, 224, 224), float32] */;
  %1 = nn.simulated_quantize(meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, %in_scale1, %out_scale1, %clip_min1, %clip_max1, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %2 = nn.conv2d(%0, %1, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.simulated_quantize(%2, %in_scale2, %out_scale2, %clip_min2, %clip_max2, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %4 = nn.simulated_quantize(meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale3, %out_scale3, %clip_min3, %clip_max3, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %5 = add(%3, %4) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %6 = nn.simulated_quantize(%5, %in_scale4, %out_scale4, %clip_min4, %clip_max4, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %8 = nn.simulated_quantize(%7, %in_scale5, %out_scale5, %clip_min5, %clip_max5, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.simulated_quantize(%9, %in_scale6, %out_scale6, %clip_min6, %clip_max6, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.simulated_quantize(meta[relay.Constant][2] /* ty=Tensor[(64, 64, 1, 1), float32] */ /* ty=Tensor[(64, 64, 1, 1), float32] */, %in_scale7, %out_scale7, %clip_min7, %clip_max7, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %12 = nn.conv2d(%10, %11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %13 = nn.simulated_quantize(%12, %in_scale8, %out_scale8, %clip_min8, %clip_max8, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %14 = nn.simulated_quantize(meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale9, %out_scale9, %clip_min9, %clip_max9, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %15 = add(%13, %14) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %16 = nn.simulated_quantize(%15, %in_scale10, %out_scale10, %clip_min10, %clip_max10, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %17 = nn.simulated_quantize(meta[relay.Constant][4] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale11, %out_scale11, %clip_min11, %clip_max11, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %18 = add(%16, %17) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %19 = nn.simulated_quantize(%18, %in_scale12, %out_scale12, %clip_min12, %clip_max12, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %20 = nn.relu(%19) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %21 = nn.simulated_quantize(%20, %in_scale13, %out_scale13, %clip_min13, %clip_max13, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %22 = nn.simulated_quantize(meta[relay.Constant][5] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, %in_scale14, %out_scale14, %clip_min14, %clip_max14, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %23 = nn.conv2d(%21, %22, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %24 = nn.simulated_quantize(%23, %in_scale15, %out_scale15, %clip_min15, %clip_max15, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %25 = nn.simulated_quantize(meta[relay.Constant][6] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale16, %out_scale16, %clip_min16, %clip_max16, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %26 = add(%24, %25) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %27 = nn.simulated_quantize(%26, %in_scale17, %out_scale17, %clip_min17, %clip_max17, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %29 = nn.simulated_quantize(%28, %in_scale18, %out_scale18, %clip_min18, %clip_max18, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %30 = nn.simulated_quantize(meta[relay.Constant][7] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, %in_scale19, %out_scale19, %clip_min19, %clip_max19, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 64, 1, 1), float32] */;
  %31 = nn.conv2d(%29, %30, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %32 = nn.simulated_quantize(%31, %in_scale20, %out_scale20, %clip_min20, %clip_max20, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %33 = nn.simulated_quantize(meta[relay.Constant][8] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale21, %out_scale21, %clip_min21, %clip_max21, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %34 = add(%32, %33) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %35 = nn.simulated_quantize(%34, %in_scale22, %out_scale22, %clip_min22, %clip_max22, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %36 = nn.simulated_quantize(meta[relay.Constant][9] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale23, %out_scale23, %clip_min23, %clip_max23, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %37 = add(%35, %36) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %38 = nn.simulated_quantize(%37, %in_scale28, %out_scale28, %clip_min28, %clip_max28, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %39 = nn.simulated_quantize(%9, %in_scale24, %out_scale24, %clip_min24, %clip_max24, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %40 = nn.simulated_quantize(meta[relay.Constant][10] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, %in_scale25, %out_scale25, %clip_min25, %clip_max25, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 64, 1, 1), float32] */;
  %41 = nn.conv2d(%39, %40, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %42 = nn.simulated_quantize(%41, %in_scale26, %out_scale26, %clip_min26, %clip_max26, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %43 = nn.simulated_quantize(meta[relay.Constant][11] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale27, %out_scale27, %clip_min27, %clip_max27, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %44 = add(%42, %43) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %45 = nn.simulated_quantize(%44, %in_scale29, %out_scale29, %clip_min29, %clip_max29, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %46 = add(%38, %45) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %47 = nn.simulated_quantize(%46, %in_scale30, %out_scale30, %clip_min30, %clip_max30, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %48 = nn.relu(%47) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %49 = nn.simulated_quantize(%48, %in_scale31, %out_scale31, %clip_min31, %clip_max31, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %50 = nn.simulated_quantize(meta[relay.Constant][12] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, %in_scale32, %out_scale32, %clip_min32, %clip_max32, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %51 = nn.conv2d(%49, %50, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %52 = nn.simulated_quantize(%51, %in_scale33, %out_scale33, %clip_min33, %clip_max33, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %53 = nn.simulated_quantize(meta[relay.Constant][13] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale34, %out_scale34, %clip_min34, %clip_max34, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %55 = nn.simulated_quantize(%54, %in_scale35, %out_scale35, %clip_min35, %clip_max35, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %56 = nn.simulated_quantize(meta[relay.Constant][14] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale36, %out_scale36, %clip_min36, %clip_max36, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %57 = add(%55, %56) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %58 = nn.simulated_quantize(%57, %in_scale37, %out_scale37, %clip_min37, %clip_max37, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %59 = nn.relu(%58) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %60 = nn.simulated_quantize(%59, %in_scale38, %out_scale38, %clip_min38, %clip_max38, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %61 = nn.simulated_quantize(meta[relay.Constant][15] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, %in_scale39, %out_scale39, %clip_min39, %clip_max39, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %62 = nn.conv2d(%60, %61, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %63 = nn.simulated_quantize(%62, %in_scale40, %out_scale40, %clip_min40, %clip_max40, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %64 = nn.simulated_quantize(meta[relay.Constant][16] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale41, %out_scale41, %clip_min41, %clip_max41, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %65 = add(%63, %64) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %66 = nn.simulated_quantize(%65, %in_scale42, %out_scale42, %clip_min42, %clip_max42, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %68 = nn.simulated_quantize(%67, %in_scale43, %out_scale43, %clip_min43, %clip_max43, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %69 = nn.simulated_quantize(meta[relay.Constant][17] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, %in_scale44, %out_scale44, %clip_min44, %clip_max44, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 64, 1, 1), float32] */;
  %70 = nn.conv2d(%68, %69, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %71 = nn.simulated_quantize(%70, %in_scale45, %out_scale45, %clip_min45, %clip_max45, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %72 = nn.simulated_quantize(meta[relay.Constant][18] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale46, %out_scale46, %clip_min46, %clip_max46, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %74 = nn.simulated_quantize(%73, %in_scale47, %out_scale47, %clip_min47, %clip_max47, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %75 = nn.simulated_quantize(meta[relay.Constant][19] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale48, %out_scale48, %clip_min48, %clip_max48, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %76 = add(%74, %75) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %77 = nn.simulated_quantize(%76, %in_scale49, %out_scale49, %clip_min49, %clip_max49, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %78 = nn.simulated_quantize(%48, %in_scale50, %out_scale50, %clip_min50, %clip_max50, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %79 = add(%77, %78) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %80 = nn.simulated_quantize(%79, %in_scale51, %out_scale51, %clip_min51, %clip_max51, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %81 = nn.relu(%80) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %82 = nn.simulated_quantize(%81, %in_scale52, %out_scale52, %clip_min52, %clip_max52, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %83 = nn.simulated_quantize(meta[relay.Constant][20] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, %in_scale53, %out_scale53, %clip_min53, %clip_max53, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %84 = nn.conv2d(%82, %83, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %85 = nn.simulated_quantize(%84, %in_scale54, %out_scale54, %clip_min54, %clip_max54, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %86 = nn.simulated_quantize(meta[relay.Constant][21] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale55, %out_scale55, %clip_min55, %clip_max55, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %87 = add(%85, %86) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %88 = nn.simulated_quantize(%87, %in_scale56, %out_scale56, %clip_min56, %clip_max56, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %89 = nn.simulated_quantize(meta[relay.Constant][22] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale57, %out_scale57, %clip_min57, %clip_max57, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %90 = add(%88, %89) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %91 = nn.simulated_quantize(%90, %in_scale58, %out_scale58, %clip_min58, %clip_max58, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %92 = nn.relu(%91) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %93 = nn.simulated_quantize(%92, %in_scale59, %out_scale59, %clip_min59, %clip_max59, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %94 = nn.simulated_quantize(meta[relay.Constant][23] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, %in_scale60, %out_scale60, %clip_min60, %clip_max60, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %95 = nn.conv2d(%93, %94, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %96 = nn.simulated_quantize(%95, %in_scale61, %out_scale61, %clip_min61, %clip_max61, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %97 = nn.simulated_quantize(meta[relay.Constant][24] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale62, %out_scale62, %clip_min62, %clip_max62, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %98 = add(%96, %97) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %99 = nn.simulated_quantize(%98, %in_scale63, %out_scale63, %clip_min63, %clip_max63, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %100 = nn.relu(%99) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %101 = nn.simulated_quantize(%100, %in_scale64, %out_scale64, %clip_min64, %clip_max64, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %102 = nn.simulated_quantize(meta[relay.Constant][25] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, %in_scale65, %out_scale65, %clip_min65, %clip_max65, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 64, 1, 1), float32] */;
  %103 = nn.conv2d(%101, %102, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %104 = nn.simulated_quantize(%103, %in_scale66, %out_scale66, %clip_min66, %clip_max66, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %105 = nn.simulated_quantize(meta[relay.Constant][26] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale67, %out_scale67, %clip_min67, %clip_max67, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %106 = add(%104, %105) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %107 = nn.simulated_quantize(%106, %in_scale68, %out_scale68, %clip_min68, %clip_max68, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %108 = nn.simulated_quantize(meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale69, %out_scale69, %clip_min69, %clip_max69, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %109 = add(%107, %108) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %110 = nn.simulated_quantize(%109, %in_scale70, %out_scale70, %clip_min70, %clip_max70, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %111 = nn.simulated_quantize(%81, %in_scale71, %out_scale71, %clip_min71, %clip_max71, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %112 = add(%110, %111) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %113 = nn.simulated_quantize(%112, %in_scale72, %out_scale72, %clip_min72, %clip_max72, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %114 = nn.relu(%113) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %115 = nn.simulated_quantize(%114, %in_scale73, %out_scale73, %clip_min73, %clip_max73, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %116 = nn.simulated_quantize(meta[relay.Constant][28] /* ty=Tensor[(128, 256, 1, 1), float32] */ /* ty=Tensor[(128, 256, 1, 1), float32] */, %in_scale74, %out_scale74, %clip_min74, %clip_max74, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 256, 1, 1), float32] */;
  %117 = nn.conv2d(%115, %116, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %118 = nn.simulated_quantize(%117, %in_scale75, %out_scale75, %clip_min75, %clip_max75, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %119 = nn.simulated_quantize(meta[relay.Constant][29] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale76, %out_scale76, %clip_min76, %clip_max76, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %120 = add(%118, %119) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %121 = nn.simulated_quantize(%120, %in_scale77, %out_scale77, %clip_min77, %clip_max77, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %122 = nn.simulated_quantize(meta[relay.Constant][30] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale78, %out_scale78, %clip_min78, %clip_max78, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %123 = add(%121, %122) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %124 = nn.simulated_quantize(%123, %in_scale79, %out_scale79, %clip_min79, %clip_max79, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %125 = nn.relu(%124) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %126 = nn.simulated_quantize(%125, %in_scale80, %out_scale80, %clip_min80, %clip_max80, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %127 = nn.simulated_quantize(meta[relay.Constant][31] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, %in_scale81, %out_scale81, %clip_min81, %clip_max81, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %129 = nn.simulated_quantize(%128, %in_scale82, %out_scale82, %clip_min82, %clip_max82, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %130 = nn.simulated_quantize(meta[relay.Constant][32] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale83, %out_scale83, %clip_min83, %clip_max83, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %131 = add(%129, %130) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %132 = nn.simulated_quantize(%131, %in_scale84, %out_scale84, %clip_min84, %clip_max84, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %133 = nn.relu(%132) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %134 = nn.simulated_quantize(%133, %in_scale85, %out_scale85, %clip_min85, %clip_max85, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %135 = nn.simulated_quantize(meta[relay.Constant][33] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, %in_scale86, %out_scale86, %clip_min86, %clip_max86, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 128, 1, 1), float32] */;
  %136 = nn.conv2d(%134, %135, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %137 = nn.simulated_quantize(%136, %in_scale87, %out_scale87, %clip_min87, %clip_max87, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %138 = nn.simulated_quantize(meta[relay.Constant][34] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale88, %out_scale88, %clip_min88, %clip_max88, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %139 = add(%137, %138) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %140 = nn.simulated_quantize(%139, %in_scale89, %out_scale89, %clip_min89, %clip_max89, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %141 = nn.simulated_quantize(meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale90, %out_scale90, %clip_min90, %clip_max90, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %142 = add(%140, %141) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %143 = nn.simulated_quantize(%142, %in_scale95, %out_scale95, %clip_min95, %clip_max95, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %144 = nn.simulated_quantize(%114, %in_scale91, %out_scale91, %clip_min91, %clip_max91, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %145 = nn.simulated_quantize(meta[relay.Constant][36] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, %in_scale92, %out_scale92, %clip_min92, %clip_max92, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %146 = nn.conv2d(%144, %145, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %147 = nn.simulated_quantize(%146, %in_scale93, %out_scale93, %clip_min93, %clip_max93, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %148 = nn.simulated_quantize(meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale94, %out_scale94, %clip_min94, %clip_max94, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %149 = add(%147, %148) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %150 = nn.simulated_quantize(%149, %in_scale96, %out_scale96, %clip_min96, %clip_max96, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %151 = add(%143, %150) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %152 = nn.simulated_quantize(%151, %in_scale97, %out_scale97, %clip_min97, %clip_max97, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %153 = nn.relu(%152) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %154 = nn.simulated_quantize(%153, %in_scale98, %out_scale98, %clip_min98, %clip_max98, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %155 = nn.simulated_quantize(meta[relay.Constant][38] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, %in_scale99, %out_scale99, %clip_min99, %clip_max99, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 512, 1, 1), float32] */;
  %156 = nn.conv2d(%154, %155, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %157 = nn.simulated_quantize(%156, %in_scale100, %out_scale100, %clip_min100, %clip_max100, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %158 = nn.simulated_quantize(meta[relay.Constant][39] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale101, %out_scale101, %clip_min101, %clip_max101, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %159 = add(%157, %158) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %160 = nn.simulated_quantize(%159, %in_scale102, %out_scale102, %clip_min102, %clip_max102, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %161 = nn.simulated_quantize(meta[relay.Constant][40] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale103, %out_scale103, %clip_min103, %clip_max103, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %162 = add(%160, %161) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %163 = nn.simulated_quantize(%162, %in_scale104, %out_scale104, %clip_min104, %clip_max104, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %164 = nn.relu(%163) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %165 = nn.simulated_quantize(%164, %in_scale105, %out_scale105, %clip_min105, %clip_max105, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %166 = nn.simulated_quantize(meta[relay.Constant][41] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, %in_scale106, %out_scale106, %clip_min106, %clip_max106, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %167 = nn.conv2d(%165, %166, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %168 = nn.simulated_quantize(%167, %in_scale107, %out_scale107, %clip_min107, %clip_max107, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %169 = nn.simulated_quantize(meta[relay.Constant][42] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale108, %out_scale108, %clip_min108, %clip_max108, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %170 = add(%168, %169) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %171 = nn.simulated_quantize(%170, %in_scale109, %out_scale109, %clip_min109, %clip_max109, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %173 = nn.simulated_quantize(%172, %in_scale110, %out_scale110, %clip_min110, %clip_max110, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %174 = nn.simulated_quantize(meta[relay.Constant][43] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, %in_scale111, %out_scale111, %clip_min111, %clip_max111, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 128, 1, 1), float32] */;
  %175 = nn.conv2d(%173, %174, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %176 = nn.simulated_quantize(%175, %in_scale112, %out_scale112, %clip_min112, %clip_max112, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %177 = nn.simulated_quantize(meta[relay.Constant][44] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale113, %out_scale113, %clip_min113, %clip_max113, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %178 = add(%176, %177) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %179 = nn.simulated_quantize(%178, %in_scale114, %out_scale114, %clip_min114, %clip_max114, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %180 = nn.simulated_quantize(meta[relay.Constant][45] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale115, %out_scale115, %clip_min115, %clip_max115, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %181 = add(%179, %180) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %182 = nn.simulated_quantize(%181, %in_scale116, %out_scale116, %clip_min116, %clip_max116, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %183 = nn.simulated_quantize(%153, %in_scale117, %out_scale117, %clip_min117, %clip_max117, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %184 = add(%182, %183) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %185 = nn.simulated_quantize(%184, %in_scale118, %out_scale118, %clip_min118, %clip_max118, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %186 = nn.relu(%185) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %187 = nn.simulated_quantize(%186, %in_scale119, %out_scale119, %clip_min119, %clip_max119, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %188 = nn.simulated_quantize(meta[relay.Constant][46] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, %in_scale120, %out_scale120, %clip_min120, %clip_max120, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 512, 1, 1), float32] */;
  %189 = nn.conv2d(%187, %188, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %190 = nn.simulated_quantize(%189, %in_scale121, %out_scale121, %clip_min121, %clip_max121, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %191 = nn.simulated_quantize(meta[relay.Constant][47] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale122, %out_scale122, %clip_min122, %clip_max122, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %192 = add(%190, %191) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %193 = nn.simulated_quantize(%192, %in_scale123, %out_scale123, %clip_min123, %clip_max123, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %194 = nn.simulated_quantize(meta[relay.Constant][48] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale124, %out_scale124, %clip_min124, %clip_max124, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %195 = add(%193, %194) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %196 = nn.simulated_quantize(%195, %in_scale125, %out_scale125, %clip_min125, %clip_max125, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %197 = nn.relu(%196) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %198 = nn.simulated_quantize(%197, %in_scale126, %out_scale126, %clip_min126, %clip_max126, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %199 = nn.simulated_quantize(meta[relay.Constant][49] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, %in_scale127, %out_scale127, %clip_min127, %clip_max127, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %200 = nn.conv2d(%198, %199, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %201 = nn.simulated_quantize(%200, %in_scale128, %out_scale128, %clip_min128, %clip_max128, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %202 = nn.simulated_quantize(meta[relay.Constant][50] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale129, %out_scale129, %clip_min129, %clip_max129, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %203 = add(%201, %202) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %204 = nn.simulated_quantize(%203, %in_scale130, %out_scale130, %clip_min130, %clip_max130, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %206 = nn.simulated_quantize(%205, %in_scale131, %out_scale131, %clip_min131, %clip_max131, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %207 = nn.simulated_quantize(meta[relay.Constant][51] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, %in_scale132, %out_scale132, %clip_min132, %clip_max132, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 128, 1, 1), float32] */;
  %208 = nn.conv2d(%206, %207, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %209 = nn.simulated_quantize(%208, %in_scale133, %out_scale133, %clip_min133, %clip_max133, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %210 = nn.simulated_quantize(meta[relay.Constant][52] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale134, %out_scale134, %clip_min134, %clip_max134, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %211 = add(%209, %210) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %212 = nn.simulated_quantize(%211, %in_scale135, %out_scale135, %clip_min135, %clip_max135, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %213 = nn.simulated_quantize(meta[relay.Constant][53] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale136, %out_scale136, %clip_min136, %clip_max136, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %214 = add(%212, %213) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %215 = nn.simulated_quantize(%214, %in_scale137, %out_scale137, %clip_min137, %clip_max137, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %216 = nn.simulated_quantize(%186, %in_scale138, %out_scale138, %clip_min138, %clip_max138, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %217 = add(%215, %216) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %218 = nn.simulated_quantize(%217, %in_scale139, %out_scale139, %clip_min139, %clip_max139, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %219 = nn.relu(%218) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %220 = nn.simulated_quantize(%219, %in_scale140, %out_scale140, %clip_min140, %clip_max140, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %221 = nn.simulated_quantize(meta[relay.Constant][54] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, %in_scale141, %out_scale141, %clip_min141, %clip_max141, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 512, 1, 1), float32] */;
  %222 = nn.conv2d(%220, %221, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %223 = nn.simulated_quantize(%222, %in_scale142, %out_scale142, %clip_min142, %clip_max142, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %224 = nn.simulated_quantize(meta[relay.Constant][55] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale143, %out_scale143, %clip_min143, %clip_max143, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %225 = add(%223, %224) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %226 = nn.simulated_quantize(%225, %in_scale144, %out_scale144, %clip_min144, %clip_max144, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %227 = nn.simulated_quantize(meta[relay.Constant][56] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale145, %out_scale145, %clip_min145, %clip_max145, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %228 = add(%226, %227) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %229 = nn.simulated_quantize(%228, %in_scale146, %out_scale146, %clip_min146, %clip_max146, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %230 = nn.relu(%229) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %231 = nn.simulated_quantize(%230, %in_scale147, %out_scale147, %clip_min147, %clip_max147, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %232 = nn.simulated_quantize(meta[relay.Constant][57] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, %in_scale148, %out_scale148, %clip_min148, %clip_max148, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %233 = nn.conv2d(%231, %232, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %234 = nn.simulated_quantize(%233, %in_scale149, %out_scale149, %clip_min149, %clip_max149, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %235 = nn.simulated_quantize(meta[relay.Constant][58] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale150, %out_scale150, %clip_min150, %clip_max150, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %236 = add(%234, %235) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %237 = nn.simulated_quantize(%236, %in_scale151, %out_scale151, %clip_min151, %clip_max151, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %238 = nn.relu(%237) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %239 = nn.simulated_quantize(%238, %in_scale152, %out_scale152, %clip_min152, %clip_max152, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %240 = nn.simulated_quantize(meta[relay.Constant][59] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, %in_scale153, %out_scale153, %clip_min153, %clip_max153, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 128, 1, 1), float32] */;
  %241 = nn.conv2d(%239, %240, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %242 = nn.simulated_quantize(%241, %in_scale154, %out_scale154, %clip_min154, %clip_max154, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %243 = nn.simulated_quantize(meta[relay.Constant][60] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale155, %out_scale155, %clip_min155, %clip_max155, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %244 = add(%242, %243) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %245 = nn.simulated_quantize(%244, %in_scale156, %out_scale156, %clip_min156, %clip_max156, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %246 = nn.simulated_quantize(meta[relay.Constant][61] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale157, %out_scale157, %clip_min157, %clip_max157, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %247 = add(%245, %246) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %248 = nn.simulated_quantize(%247, %in_scale158, %out_scale158, %clip_min158, %clip_max158, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %249 = nn.simulated_quantize(%219, %in_scale159, %out_scale159, %clip_min159, %clip_max159, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %250 = add(%248, %249) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %251 = nn.simulated_quantize(%250, %in_scale160, %out_scale160, %clip_min160, %clip_max160, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %252 = nn.relu(%251) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %253 = nn.simulated_quantize(%252, %in_scale161, %out_scale161, %clip_min161, %clip_max161, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %254 = nn.simulated_quantize(meta[relay.Constant][62] /* ty=Tensor[(256, 512, 1, 1), float32] */ /* ty=Tensor[(256, 512, 1, 1), float32] */, %in_scale162, %out_scale162, %clip_min162, %clip_max162, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 512, 1, 1), float32] */;
  %255 = nn.conv2d(%253, %254, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %256 = nn.simulated_quantize(%255, %in_scale163, %out_scale163, %clip_min163, %clip_max163, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %257 = nn.simulated_quantize(meta[relay.Constant][63] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale164, %out_scale164, %clip_min164, %clip_max164, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %258 = add(%256, %257) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %259 = nn.simulated_quantize(%258, %in_scale165, %out_scale165, %clip_min165, %clip_max165, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %260 = nn.simulated_quantize(meta[relay.Constant][64] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale166, %out_scale166, %clip_min166, %clip_max166, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %261 = add(%259, %260) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %262 = nn.simulated_quantize(%261, %in_scale167, %out_scale167, %clip_min167, %clip_max167, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %263 = nn.relu(%262) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %264 = nn.simulated_quantize(%263, %in_scale168, %out_scale168, %clip_min168, %clip_max168, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %265 = nn.simulated_quantize(meta[relay.Constant][65] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, %in_scale169, %out_scale169, %clip_min169, %clip_max169, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %266 = nn.conv2d(%264, %265, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %267 = nn.simulated_quantize(%266, %in_scale170, %out_scale170, %clip_min170, %clip_max170, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %268 = nn.simulated_quantize(meta[relay.Constant][66] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale171, %out_scale171, %clip_min171, %clip_max171, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %269 = add(%267, %268) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %270 = nn.simulated_quantize(%269, %in_scale172, %out_scale172, %clip_min172, %clip_max172, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %271 = nn.relu(%270) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %272 = nn.simulated_quantize(%271, %in_scale173, %out_scale173, %clip_min173, %clip_max173, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %273 = nn.simulated_quantize(meta[relay.Constant][67] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, %in_scale174, %out_scale174, %clip_min174, %clip_max174, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %274 = nn.conv2d(%272, %273, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %275 = nn.simulated_quantize(%274, %in_scale175, %out_scale175, %clip_min175, %clip_max175, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %276 = nn.simulated_quantize(meta[relay.Constant][68] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale176, %out_scale176, %clip_min176, %clip_max176, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %277 = add(%275, %276) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %278 = nn.simulated_quantize(%277, %in_scale177, %out_scale177, %clip_min177, %clip_max177, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %279 = nn.simulated_quantize(meta[relay.Constant][69] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale178, %out_scale178, %clip_min178, %clip_max178, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %280 = add(%278, %279) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %281 = nn.simulated_quantize(%280, %in_scale183, %out_scale183, %clip_min183, %clip_max183, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %282 = nn.simulated_quantize(%252, %in_scale179, %out_scale179, %clip_min179, %clip_max179, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %283 = nn.simulated_quantize(meta[relay.Constant][70] /* ty=Tensor[(1024, 512, 1, 1), float32] */ /* ty=Tensor[(1024, 512, 1, 1), float32] */, %in_scale180, %out_scale180, %clip_min180, %clip_max180, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 512, 1, 1), float32] */;
  %284 = nn.conv2d(%282, %283, strides=[2, 2], padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %285 = nn.simulated_quantize(%284, %in_scale181, %out_scale181, %clip_min181, %clip_max181, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %286 = nn.simulated_quantize(meta[relay.Constant][71] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale182, %out_scale182, %clip_min182, %clip_max182, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %287 = add(%285, %286) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %288 = nn.simulated_quantize(%287, %in_scale184, %out_scale184, %clip_min184, %clip_max184, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %289 = add(%281, %288) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %290 = nn.simulated_quantize(%289, %in_scale185, %out_scale185, %clip_min185, %clip_max185, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %291 = nn.relu(%290) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %292 = nn.simulated_quantize(%291, %in_scale186, %out_scale186, %clip_min186, %clip_max186, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %293 = nn.simulated_quantize(meta[relay.Constant][72] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, %in_scale187, %out_scale187, %clip_min187, %clip_max187, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %294 = nn.conv2d(%292, %293, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %295 = nn.simulated_quantize(%294, %in_scale188, %out_scale188, %clip_min188, %clip_max188, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %296 = nn.simulated_quantize(meta[relay.Constant][73] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale189, %out_scale189, %clip_min189, %clip_max189, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %297 = add(%295, %296) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %298 = nn.simulated_quantize(%297, %in_scale190, %out_scale190, %clip_min190, %clip_max190, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %299 = nn.simulated_quantize(meta[relay.Constant][74] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale191, %out_scale191, %clip_min191, %clip_max191, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %300 = add(%298, %299) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %301 = nn.simulated_quantize(%300, %in_scale192, %out_scale192, %clip_min192, %clip_max192, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %302 = nn.relu(%301) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %303 = nn.simulated_quantize(%302, %in_scale193, %out_scale193, %clip_min193, %clip_max193, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %304 = nn.simulated_quantize(meta[relay.Constant][75] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, %in_scale194, %out_scale194, %clip_min194, %clip_max194, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %305 = nn.conv2d(%303, %304, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %306 = nn.simulated_quantize(%305, %in_scale195, %out_scale195, %clip_min195, %clip_max195, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %307 = nn.simulated_quantize(meta[relay.Constant][76] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale196, %out_scale196, %clip_min196, %clip_max196, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %308 = add(%306, %307) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %309 = nn.simulated_quantize(%308, %in_scale197, %out_scale197, %clip_min197, %clip_max197, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %310 = nn.relu(%309) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %311 = nn.simulated_quantize(%310, %in_scale198, %out_scale198, %clip_min198, %clip_max198, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %312 = nn.simulated_quantize(meta[relay.Constant][77] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, %in_scale199, %out_scale199, %clip_min199, %clip_max199, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %313 = nn.conv2d(%311, %312, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %314 = nn.simulated_quantize(%313, %in_scale200, %out_scale200, %clip_min200, %clip_max200, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %315 = nn.simulated_quantize(meta[relay.Constant][78] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale201, %out_scale201, %clip_min201, %clip_max201, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %316 = add(%314, %315) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %317 = nn.simulated_quantize(%316, %in_scale202, %out_scale202, %clip_min202, %clip_max202, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %318 = nn.simulated_quantize(meta[relay.Constant][79] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale203, %out_scale203, %clip_min203, %clip_max203, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %319 = add(%317, %318) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %320 = nn.simulated_quantize(%319, %in_scale204, %out_scale204, %clip_min204, %clip_max204, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %321 = nn.simulated_quantize(%291, %in_scale205, %out_scale205, %clip_min205, %clip_max205, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %322 = add(%320, %321) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %323 = nn.simulated_quantize(%322, %in_scale206, %out_scale206, %clip_min206, %clip_max206, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %324 = nn.relu(%323) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %325 = nn.simulated_quantize(%324, %in_scale207, %out_scale207, %clip_min207, %clip_max207, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %326 = nn.simulated_quantize(meta[relay.Constant][80] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, %in_scale208, %out_scale208, %clip_min208, %clip_max208, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %327 = nn.conv2d(%325, %326, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %328 = nn.simulated_quantize(%327, %in_scale209, %out_scale209, %clip_min209, %clip_max209, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %329 = nn.simulated_quantize(meta[relay.Constant][81] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale210, %out_scale210, %clip_min210, %clip_max210, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %330 = add(%328, %329) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %331 = nn.simulated_quantize(%330, %in_scale211, %out_scale211, %clip_min211, %clip_max211, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %332 = nn.simulated_quantize(meta[relay.Constant][82] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale212, %out_scale212, %clip_min212, %clip_max212, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %333 = add(%331, %332) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %334 = nn.simulated_quantize(%333, %in_scale213, %out_scale213, %clip_min213, %clip_max213, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %335 = nn.relu(%334) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %336 = nn.simulated_quantize(%335, %in_scale214, %out_scale214, %clip_min214, %clip_max214, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %337 = nn.simulated_quantize(meta[relay.Constant][83] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, %in_scale215, %out_scale215, %clip_min215, %clip_max215, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %338 = nn.conv2d(%336, %337, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %339 = nn.simulated_quantize(%338, %in_scale216, %out_scale216, %clip_min216, %clip_max216, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %340 = nn.simulated_quantize(meta[relay.Constant][84] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale217, %out_scale217, %clip_min217, %clip_max217, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %341 = add(%339, %340) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %342 = nn.simulated_quantize(%341, %in_scale218, %out_scale218, %clip_min218, %clip_max218, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %343 = nn.relu(%342) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %344 = nn.simulated_quantize(%343, %in_scale219, %out_scale219, %clip_min219, %clip_max219, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %345 = nn.simulated_quantize(meta[relay.Constant][85] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, %in_scale220, %out_scale220, %clip_min220, %clip_max220, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %346 = nn.conv2d(%344, %345, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %347 = nn.simulated_quantize(%346, %in_scale221, %out_scale221, %clip_min221, %clip_max221, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %348 = nn.simulated_quantize(meta[relay.Constant][86] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale222, %out_scale222, %clip_min222, %clip_max222, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %349 = add(%347, %348) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %350 = nn.simulated_quantize(%349, %in_scale223, %out_scale223, %clip_min223, %clip_max223, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %351 = nn.simulated_quantize(meta[relay.Constant][87] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale224, %out_scale224, %clip_min224, %clip_max224, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %352 = add(%350, %351) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %353 = nn.simulated_quantize(%352, %in_scale225, %out_scale225, %clip_min225, %clip_max225, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %354 = nn.simulated_quantize(%324, %in_scale226, %out_scale226, %clip_min226, %clip_max226, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %355 = add(%353, %354) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %356 = nn.simulated_quantize(%355, %in_scale227, %out_scale227, %clip_min227, %clip_max227, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %357 = nn.relu(%356) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %358 = nn.simulated_quantize(%357, %in_scale228, %out_scale228, %clip_min228, %clip_max228, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %359 = nn.simulated_quantize(meta[relay.Constant][88] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, %in_scale229, %out_scale229, %clip_min229, %clip_max229, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %360 = nn.conv2d(%358, %359, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %361 = nn.simulated_quantize(%360, %in_scale230, %out_scale230, %clip_min230, %clip_max230, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %362 = nn.simulated_quantize(meta[relay.Constant][89] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale231, %out_scale231, %clip_min231, %clip_max231, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %363 = add(%361, %362) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %364 = nn.simulated_quantize(%363, %in_scale232, %out_scale232, %clip_min232, %clip_max232, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %365 = nn.simulated_quantize(meta[relay.Constant][90] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale233, %out_scale233, %clip_min233, %clip_max233, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %366 = add(%364, %365) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %367 = nn.simulated_quantize(%366, %in_scale234, %out_scale234, %clip_min234, %clip_max234, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %368 = nn.relu(%367) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %369 = nn.simulated_quantize(%368, %in_scale235, %out_scale235, %clip_min235, %clip_max235, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %370 = nn.simulated_quantize(meta[relay.Constant][91] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, %in_scale236, %out_scale236, %clip_min236, %clip_max236, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %371 = nn.conv2d(%369, %370, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %372 = nn.simulated_quantize(%371, %in_scale237, %out_scale237, %clip_min237, %clip_max237, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %373 = nn.simulated_quantize(meta[relay.Constant][92] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale238, %out_scale238, %clip_min238, %clip_max238, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %374 = add(%372, %373) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %375 = nn.simulated_quantize(%374, %in_scale239, %out_scale239, %clip_min239, %clip_max239, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %376 = nn.relu(%375) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %377 = nn.simulated_quantize(%376, %in_scale240, %out_scale240, %clip_min240, %clip_max240, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %378 = nn.simulated_quantize(meta[relay.Constant][93] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, %in_scale241, %out_scale241, %clip_min241, %clip_max241, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %379 = nn.conv2d(%377, %378, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %380 = nn.simulated_quantize(%379, %in_scale242, %out_scale242, %clip_min242, %clip_max242, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %381 = nn.simulated_quantize(meta[relay.Constant][94] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale243, %out_scale243, %clip_min243, %clip_max243, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %382 = add(%380, %381) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %383 = nn.simulated_quantize(%382, %in_scale244, %out_scale244, %clip_min244, %clip_max244, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %384 = nn.simulated_quantize(meta[relay.Constant][95] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale245, %out_scale245, %clip_min245, %clip_max245, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %385 = add(%383, %384) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %386 = nn.simulated_quantize(%385, %in_scale246, %out_scale246, %clip_min246, %clip_max246, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %387 = nn.simulated_quantize(%357, %in_scale247, %out_scale247, %clip_min247, %clip_max247, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %388 = add(%386, %387) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %389 = nn.simulated_quantize(%388, %in_scale248, %out_scale248, %clip_min248, %clip_max248, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %390 = nn.relu(%389) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %391 = nn.simulated_quantize(%390, %in_scale249, %out_scale249, %clip_min249, %clip_max249, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %392 = nn.simulated_quantize(meta[relay.Constant][96] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, %in_scale250, %out_scale250, %clip_min250, %clip_max250, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %393 = nn.conv2d(%391, %392, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %394 = nn.simulated_quantize(%393, %in_scale251, %out_scale251, %clip_min251, %clip_max251, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %395 = nn.simulated_quantize(meta[relay.Constant][97] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale252, %out_scale252, %clip_min252, %clip_max252, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %396 = add(%394, %395) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %397 = nn.simulated_quantize(%396, %in_scale253, %out_scale253, %clip_min253, %clip_max253, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %398 = nn.simulated_quantize(meta[relay.Constant][98] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale254, %out_scale254, %clip_min254, %clip_max254, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %399 = add(%397, %398) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %400 = nn.simulated_quantize(%399, %in_scale255, %out_scale255, %clip_min255, %clip_max255, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %401 = nn.relu(%400) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %402 = nn.simulated_quantize(%401, %in_scale256, %out_scale256, %clip_min256, %clip_max256, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %403 = nn.simulated_quantize(meta[relay.Constant][99] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, %in_scale257, %out_scale257, %clip_min257, %clip_max257, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %404 = nn.conv2d(%402, %403, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %405 = nn.simulated_quantize(%404, %in_scale258, %out_scale258, %clip_min258, %clip_max258, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %406 = nn.simulated_quantize(meta[relay.Constant][100] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale259, %out_scale259, %clip_min259, %clip_max259, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %407 = add(%405, %406) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %408 = nn.simulated_quantize(%407, %in_scale260, %out_scale260, %clip_min260, %clip_max260, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %409 = nn.relu(%408) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %410 = nn.simulated_quantize(%409, %in_scale261, %out_scale261, %clip_min261, %clip_max261, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %411 = nn.simulated_quantize(meta[relay.Constant][101] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, %in_scale262, %out_scale262, %clip_min262, %clip_max262, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %412 = nn.conv2d(%410, %411, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %413 = nn.simulated_quantize(%412, %in_scale263, %out_scale263, %clip_min263, %clip_max263, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %414 = nn.simulated_quantize(meta[relay.Constant][102] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale264, %out_scale264, %clip_min264, %clip_max264, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %415 = add(%413, %414) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %416 = nn.simulated_quantize(%415, %in_scale265, %out_scale265, %clip_min265, %clip_max265, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %417 = nn.simulated_quantize(meta[relay.Constant][103] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale266, %out_scale266, %clip_min266, %clip_max266, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %418 = add(%416, %417) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %419 = nn.simulated_quantize(%418, %in_scale267, %out_scale267, %clip_min267, %clip_max267, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %420 = nn.simulated_quantize(%390, %in_scale268, %out_scale268, %clip_min268, %clip_max268, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %421 = add(%419, %420) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %422 = nn.simulated_quantize(%421, %in_scale269, %out_scale269, %clip_min269, %clip_max269, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %423 = nn.relu(%422) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %424 = nn.simulated_quantize(%423, %in_scale270, %out_scale270, %clip_min270, %clip_max270, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %425 = nn.simulated_quantize(meta[relay.Constant][104] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, %in_scale271, %out_scale271, %clip_min271, %clip_max271, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %426 = nn.conv2d(%424, %425, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %427 = nn.simulated_quantize(%426, %in_scale272, %out_scale272, %clip_min272, %clip_max272, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %428 = nn.simulated_quantize(meta[relay.Constant][105] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale273, %out_scale273, %clip_min273, %clip_max273, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %429 = add(%427, %428) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %430 = nn.simulated_quantize(%429, %in_scale274, %out_scale274, %clip_min274, %clip_max274, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %431 = nn.simulated_quantize(meta[relay.Constant][106] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale275, %out_scale275, %clip_min275, %clip_max275, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %432 = add(%430, %431) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %433 = nn.simulated_quantize(%432, %in_scale276, %out_scale276, %clip_min276, %clip_max276, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %434 = nn.relu(%433) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %435 = nn.simulated_quantize(%434, %in_scale277, %out_scale277, %clip_min277, %clip_max277, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %436 = nn.simulated_quantize(meta[relay.Constant][107] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, %in_scale278, %out_scale278, %clip_min278, %clip_max278, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %437 = nn.conv2d(%435, %436, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %438 = nn.simulated_quantize(%437, %in_scale279, %out_scale279, %clip_min279, %clip_max279, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %439 = nn.simulated_quantize(meta[relay.Constant][108] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, %in_scale280, %out_scale280, %clip_min280, %clip_max280, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %440 = add(%438, %439) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %441 = nn.simulated_quantize(%440, %in_scale281, %out_scale281, %clip_min281, %clip_max281, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %442 = nn.relu(%441) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %443 = nn.simulated_quantize(%442, %in_scale282, %out_scale282, %clip_min282, %clip_max282, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %444 = nn.simulated_quantize(meta[relay.Constant][109] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, %in_scale283, %out_scale283, %clip_min283, %clip_max283, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %445 = nn.conv2d(%443, %444, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %446 = nn.simulated_quantize(%445, %in_scale284, %out_scale284, %clip_min284, %clip_max284, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %447 = nn.simulated_quantize(meta[relay.Constant][110] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale285, %out_scale285, %clip_min285, %clip_max285, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %448 = add(%446, %447) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %449 = nn.simulated_quantize(%448, %in_scale286, %out_scale286, %clip_min286, %clip_max286, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %450 = nn.simulated_quantize(meta[relay.Constant][111] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, %in_scale287, %out_scale287, %clip_min287, %clip_max287, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %451 = add(%449, %450) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %452 = nn.simulated_quantize(%451, %in_scale288, %out_scale288, %clip_min288, %clip_max288, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %453 = nn.simulated_quantize(%423, %in_scale289, %out_scale289, %clip_min289, %clip_max289, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %454 = add(%452, %453) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %455 = nn.simulated_quantize(%454, %in_scale290, %out_scale290, %clip_min290, %clip_max290, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %456 = nn.relu(%455) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %457 = nn.simulated_quantize(%456, %in_scale291, %out_scale291, %clip_min291, %clip_max291, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %458 = nn.simulated_quantize(meta[relay.Constant][112] /* ty=Tensor[(512, 1024, 1, 1), float32] */ /* ty=Tensor[(512, 1024, 1, 1), float32] */, %in_scale292, %out_scale292, %clip_min292, %clip_max292, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 1024, 1, 1), float32] */;
  %459 = nn.conv2d(%457, %458, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %460 = nn.simulated_quantize(%459, %in_scale293, %out_scale293, %clip_min293, %clip_max293, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %461 = nn.simulated_quantize(meta[relay.Constant][113] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale294, %out_scale294, %clip_min294, %clip_max294, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %462 = add(%460, %461) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %463 = nn.simulated_quantize(%462, %in_scale295, %out_scale295, %clip_min295, %clip_max295, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %464 = nn.simulated_quantize(meta[relay.Constant][114] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale296, %out_scale296, %clip_min296, %clip_max296, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %465 = add(%463, %464) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %466 = nn.simulated_quantize(%465, %in_scale297, %out_scale297, %clip_min297, %clip_max297, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %467 = nn.relu(%466) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %468 = nn.simulated_quantize(%467, %in_scale298, %out_scale298, %clip_min298, %clip_max298, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %469 = nn.simulated_quantize(meta[relay.Constant][115] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, %in_scale299, %out_scale299, %clip_min299, %clip_max299, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %470 = nn.conv2d(%468, %469, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %471 = nn.simulated_quantize(%470, %in_scale300, %out_scale300, %clip_min300, %clip_max300, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %472 = nn.simulated_quantize(meta[relay.Constant][116] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale301, %out_scale301, %clip_min301, %clip_max301, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %473 = add(%471, %472) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %474 = nn.simulated_quantize(%473, %in_scale302, %out_scale302, %clip_min302, %clip_max302, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %475 = nn.relu(%474) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %476 = nn.simulated_quantize(%475, %in_scale303, %out_scale303, %clip_min303, %clip_max303, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %477 = nn.simulated_quantize(meta[relay.Constant][117] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, %in_scale304, %out_scale304, %clip_min304, %clip_max304, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(2048, 512, 1, 1), float32] */;
  %478 = nn.conv2d(%476, %477, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %479 = nn.simulated_quantize(%478, %in_scale305, %out_scale305, %clip_min305, %clip_max305, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %480 = nn.simulated_quantize(meta[relay.Constant][118] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, %in_scale306, %out_scale306, %clip_min306, %clip_max306, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %481 = add(%479, %480) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %482 = nn.simulated_quantize(%481, %in_scale307, %out_scale307, %clip_min307, %clip_max307, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %483 = nn.simulated_quantize(meta[relay.Constant][119] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, %in_scale308, %out_scale308, %clip_min308, %clip_max308, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %484 = add(%482, %483) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %485 = nn.simulated_quantize(%484, %in_scale313, %out_scale313, %clip_min313, %clip_max313, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %486 = nn.simulated_quantize(%456, %in_scale309, %out_scale309, %clip_min309, %clip_max309, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %487 = nn.simulated_quantize(meta[relay.Constant][120] /* ty=Tensor[(2048, 1024, 1, 1), float32] */ /* ty=Tensor[(2048, 1024, 1, 1), float32] */, %in_scale310, %out_scale310, %clip_min310, %clip_max310, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(2048, 1024, 1, 1), float32] */;
  %488 = nn.conv2d(%486, %487, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %489 = nn.simulated_quantize(%488, %in_scale311, %out_scale311, %clip_min311, %clip_max311, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %490 = nn.simulated_quantize(meta[relay.Constant][121] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, %in_scale312, %out_scale312, %clip_min312, %clip_max312, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %491 = add(%489, %490) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %492 = nn.simulated_quantize(%491, %in_scale314, %out_scale314, %clip_min314, %clip_max314, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %493 = add(%485, %492) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %494 = nn.simulated_quantize(%493, %in_scale315, %out_scale315, %clip_min315, %clip_max315, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %495 = nn.relu(%494) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %496 = nn.simulated_quantize(%495, %in_scale316, %out_scale316, %clip_min316, %clip_max316, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %497 = nn.simulated_quantize(meta[relay.Constant][122] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, %in_scale317, %out_scale317, %clip_min317, %clip_max317, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 2048, 1, 1), float32] */;
  %498 = nn.conv2d(%496, %497, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %499 = nn.simulated_quantize(%498, %in_scale318, %out_scale318, %clip_min318, %clip_max318, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %500 = nn.simulated_quantize(meta[relay.Constant][123] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale319, %out_scale319, %clip_min319, %clip_max319, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %501 = add(%499, %500) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %502 = nn.simulated_quantize(%501, %in_scale320, %out_scale320, %clip_min320, %clip_max320, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %503 = nn.simulated_quantize(meta[relay.Constant][124] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale321, %out_scale321, %clip_min321, %clip_max321, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %504 = add(%502, %503) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %505 = nn.simulated_quantize(%504, %in_scale322, %out_scale322, %clip_min322, %clip_max322, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %506 = nn.relu(%505) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %507 = nn.simulated_quantize(%506, %in_scale323, %out_scale323, %clip_min323, %clip_max323, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %508 = nn.simulated_quantize(meta[relay.Constant][125] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, %in_scale324, %out_scale324, %clip_min324, %clip_max324, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %509 = nn.conv2d(%507, %508, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %510 = nn.simulated_quantize(%509, %in_scale325, %out_scale325, %clip_min325, %clip_max325, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %511 = nn.simulated_quantize(meta[relay.Constant][126] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale326, %out_scale326, %clip_min326, %clip_max326, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %512 = add(%510, %511) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %513 = nn.simulated_quantize(%512, %in_scale327, %out_scale327, %clip_min327, %clip_max327, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %514 = nn.relu(%513) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %515 = nn.simulated_quantize(%514, %in_scale328, %out_scale328, %clip_min328, %clip_max328, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %516 = nn.simulated_quantize(meta[relay.Constant][127] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, %in_scale329, %out_scale329, %clip_min329, %clip_max329, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(2048, 512, 1, 1), float32] */;
  %517 = nn.conv2d(%515, %516, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %518 = nn.simulated_quantize(%517, %in_scale330, %out_scale330, %clip_min330, %clip_max330, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %519 = nn.simulated_quantize(meta[relay.Constant][128] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, %in_scale331, %out_scale331, %clip_min331, %clip_max331, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %520 = add(%518, %519) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %521 = nn.simulated_quantize(%520, %in_scale332, %out_scale332, %clip_min332, %clip_max332, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %522 = nn.simulated_quantize(meta[relay.Constant][129] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, %in_scale333, %out_scale333, %clip_min333, %clip_max333, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %523 = add(%521, %522) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %524 = nn.simulated_quantize(%523, %in_scale334, %out_scale334, %clip_min334, %clip_max334, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %525 = nn.simulated_quantize(%495, %in_scale335, %out_scale335, %clip_min335, %clip_max335, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %526 = add(%524, %525) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %527 = nn.simulated_quantize(%526, %in_scale336, %out_scale336, %clip_min336, %clip_max336, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %528 = nn.relu(%527) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %529 = nn.simulated_quantize(%528, %in_scale337, %out_scale337, %clip_min337, %clip_max337, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %530 = nn.simulated_quantize(meta[relay.Constant][130] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, %in_scale338, %out_scale338, %clip_min338, %clip_max338, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 2048, 1, 1), float32] */;
  %531 = nn.conv2d(%529, %530, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %532 = nn.simulated_quantize(%531, %in_scale339, %out_scale339, %clip_min339, %clip_max339, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %533 = nn.simulated_quantize(meta[relay.Constant][131] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale340, %out_scale340, %clip_min340, %clip_max340, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %534 = add(%532, %533) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %535 = nn.simulated_quantize(%534, %in_scale341, %out_scale341, %clip_min341, %clip_max341, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %536 = nn.simulated_quantize(meta[relay.Constant][132] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale342, %out_scale342, %clip_min342, %clip_max342, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %537 = add(%535, %536) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %538 = nn.simulated_quantize(%537, %in_scale343, %out_scale343, %clip_min343, %clip_max343, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %539 = nn.relu(%538) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %540 = nn.simulated_quantize(%539, %in_scale344, %out_scale344, %clip_min344, %clip_max344, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %541 = nn.simulated_quantize(meta[relay.Constant][133] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, %in_scale345, %out_scale345, %clip_min345, %clip_max345, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %542 = nn.conv2d(%540, %541, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %543 = nn.simulated_quantize(%542, %in_scale346, %out_scale346, %clip_min346, %clip_max346, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %544 = nn.simulated_quantize(meta[relay.Constant][134] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, %in_scale347, %out_scale347, %clip_min347, %clip_max347, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %545 = add(%543, %544) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %546 = nn.simulated_quantize(%545, %in_scale348, %out_scale348, %clip_min348, %clip_max348, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %547 = nn.relu(%546) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %548 = nn.simulated_quantize(%547, %in_scale349, %out_scale349, %clip_min349, %clip_max349, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %549 = nn.simulated_quantize(meta[relay.Constant][135] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, %in_scale350, %out_scale350, %clip_min350, %clip_max350, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(2048, 512, 1, 1), float32] */;
  %550 = nn.conv2d(%548, %549, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %551 = nn.simulated_quantize(%550, %in_scale351, %out_scale351, %clip_min351, %clip_max351, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %552 = nn.simulated_quantize(meta[relay.Constant][136] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, %in_scale352, %out_scale352, %clip_min352, %clip_max352, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %553 = add(%551, %552) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %554 = nn.simulated_quantize(%553, %in_scale353, %out_scale353, %clip_min353, %clip_max353, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %555 = nn.simulated_quantize(meta[relay.Constant][137] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, %in_scale354, %out_scale354, %clip_min354, %clip_max354, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %556 = add(%554, %555) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %557 = nn.simulated_quantize(%556, %in_scale355, %out_scale355, %clip_min355, %clip_max355, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %558 = nn.simulated_quantize(%528, %in_scale356, %out_scale356, %clip_min356, %clip_max356, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %559 = add(%557, %558) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %560 = nn.simulated_quantize(%559, %in_scale357, %out_scale357, %clip_min357, %clip_max357, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %561 = nn.relu(%560) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %562 = nn.simulated_quantize(%561, %in_scale358, %out_scale358, %clip_min358, %clip_max358, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %563 = nn.global_avg_pool2d(%562) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %564 = nn.simulated_quantize(%563, %in_scale359, %out_scale359, %clip_min359, %clip_max359, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %565 = nn.batch_flatten(%564) /* ty=Tensor[(32, 2048), float32] */;
  %566 = nn.simulated_quantize(%565, %in_scale360, %out_scale360, %clip_min360, %clip_max360, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048), float32] */;
  %567 = nn.simulated_quantize(meta[relay.Constant][138] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, %in_scale361, %out_scale361, %clip_min361, %clip_max361, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000, 2048), float32] */;
  %568 = nn.dense(%566, %567, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  %569 = nn.simulated_quantize(%568, %in_scale362, %out_scale362, %clip_min362, %clip_max362, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */;
  %570 = nn.simulated_quantize(meta[relay.Constant][139] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */, %in_scale363, %out_scale363, %clip_min363, %clip_max363, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000), float32] */;
  %571 = add(%569, %570) /* ty=Tensor[(32, 1000), float32] */;
  nn.simulated_quantize(%571, %in_scale364, %out_scale364, %clip_min364, %clip_max364, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
calculating threshold...
threshold method:
avg_range
thresholds: [2.6328714, 0.41522968, 6.714379, 1.1478554, 6.954009, 6.954009, 6.954009, 0.78061384, 6.3522167, 3.6093483e-05, 6.3521976, 1.1276937, 6.103249, 2.3370872, 0.53458947, 3.981303, 1.2620064, 2.719297, 2.719297, 1.9482002, 3.9516606, 0.0005308822, 3.9520955, 1.3787345, 2.9409864, 1.010842, 4.7123833, 0.9353685, 4.4218016, 4.721756, 3.2503595, 0.6211404, 2.8185155, 0.00014911669, 2.8184586, 0.8660013, 2.6623101, 2.1526558, 0.44724932, 2.9045482, 0.6904638, 2.8222704, 2.8222704, 1.9035805, 3.8936703, 0.00045458402, 3.8934772, 0.79138994, 3.684328, 3.684328, 3.539911, 0.39742854, 1.983106, 8.732147e-05, 1.9830396, 0.49259102, 2.0954366, 2.0954366, 0.44861734, 1.9672, 0.50629807, 2.0664215, 2.0664215, 1.8476616, 3.3126287, 0.00038401576, 3.312307, 0.7631658, 2.5921829, 3.5736158, 3.5736158, 0.367324, 2.2459788, 2.010915e-05, 2.2459972, 0.93146646, 1.8265529, 1.8265529, 0.42026186, 2.106414, 1.2143239, 2.2599823, 1.8708947, 1.3626825, 2.7788725, 0.0001041114, 2.7788293, 0.7123275, 2.6818674, 0.9827033, 3.1742241, 0.7738579, 2.983016, 3.1103504, 3.1103504, 0.5742223, 2.518482, 1.8604049e-05, 2.518494, 0.77043146, 1.7646878, 1.7646878, 0.47330764, 1.9731548, 0.7233969, 1.9288862, 1.5150695, 1.1412117, 2.086682, 0.00010310195, 2.086776, 0.6999181, 1.7131221, 3.1264992, 3.1264992, 0.5724043, 2.6635237, 2.223524e-05, 2.6635382, 0.38112956, 2.611788, 2.611788, 0.49697554, 2.311939, 0.64697045, 2.345185, 2.345185, 1.0766618, 2.1367214, 9.102974e-05, 2.136788, 0.5087188, 2.2201262, 3.3671715, 3.3671715, 0.50546974, 4.3405805, 1.9119912e-05, 4.3405967, 0.53639877, 4.0526896, 4.0526896, 0.38693497, 2.4639087, 0.30612683, 2.4880092, 2.4880092, 1.2972152, 2.581595, 9.0521935e-05, 2.5816677, 0.9778615, 2.6804838, 4.0864034, 4.0864034, 0.45961964, 1.5258698, 9.595744e-06, 1.525877, 0.41842383, 1.4605732, 1.4605732, 0.3036466, 2.1970677, 0.777757, 2.1624837, 1.4219792, 1.4316025, 2.0762205, 8.25451e-05, 2.0761893, 0.6372677, 1.6626203, 0.59356225, 2.6671197, 0.6732668, 2.5934587, 3.030861, 3.030861, 0.37425265, 1.9474415, 9.153559e-06, 1.9474498, 0.50566626, 1.7014549, 1.7014549, 0.33455488, 1.8411896, 0.46148568, 1.7053759, 1.6479821, 1.0857713, 1.5720549, 9.363891e-05, 1.5719872, 0.26860678, 1.5411997, 2.7829463, 2.7829463, 0.48393372, 1.9945893, 1.1808401e-05, 1.9945985, 0.26229534, 1.8306634, 1.8306634, 0.36937764, 1.8478255, 0.43430302, 1.8622922, 1.8622922, 1.5946774, 2.3158317, 0.0001144038, 2.3159091, 0.31168014, 2.2071755, 2.8605494, 2.8605494, 0.5596972, 2.41848, 1.3071515e-05, 2.418491, 0.4030789, 2.40925, 2.40925, 0.54295754, 2.2186012, 0.43280634, 2.1644316, 2.0097506, 1.2114336, 2.3796365, 0.00010832246, 2.3796954, 0.47925773, 2.4099863, 3.1041389, 3.1041389, 0.67249614, 3.011554, 1.9492714e-05, 3.0115657, 0.3605199, 2.9795713, 2.9795713, 0.8316316, 5.3954625, 0.36735648, 5.028106, 5.028106, 1.1028225, 2.3709157, 7.4814016e-05, 2.3709655, 0.66150963, 2.5238578, 4.179229, 4.179229, 0.6628884, 2.9265056, 1.2748129e-05, 2.9265149, 0.43896747, 2.8095806, 2.8095806, 0.47430062, 2.1837752, 0.48615795, 2.1920586, 2.1920586, 1.11515, 2.6620708, 8.815713e-05, 2.6620083, 0.655451, 2.7143784, 4.210725, 4.210725, 0.49684635, 1.8963063, 1.5281469e-07, 1.896306, 0.3577969, 1.9261687, 1.9261687, 0.31854755, 1.6194868, 0.46971455, 1.5406055, 1.2875314, 1.1781112, 1.3878286, 8.0427066e-07, 1.3878284, 0.41873372, 1.2506301, 0.51612294, 1.9967124, 0.44553894, 1.8714364, 2.130772, 1.954454, 0.5491331, 2.2441487, 3.6671034e-07, 2.2441485, 0.5107688, 2.2612875, 2.2612875, 0.34910858, 2.251326, 0.6767496, 2.2335868, 2.2335868, 0.9454169, 1.674724, 1.1228644e-06, 1.6747242, 0.68842053, 1.6762054, 2.6619513, 2.6619513, 0.88175744, 3.9012733, 6.118338e-07, 3.9012733, 0.8181827, 3.656603, 3.656603, 0.2999038, 3.0785038, 0.23350382, 3.1558542, 3.1558542, 21.305336, 31.219112, 6.985712e-06, 31.219116, 0.7500615, 31.379152, 31.645468, 31.645468, 10.443441, 10.443441, 0.6911725, 27.762865, 0.04205477, 27.765003]

calculate parameters
---------
data[%0] -> nn.conv2d[%2]
  bit=8, threshold=2.632871389389038
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.02056930772960186, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%1] -> nn.conv2d[%2]
  bit=8, threshold=0.4152296781539917
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00324398186057806, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%2] -> add[%4]
  bit=32, threshold=1.1478554010391235
  SimulatedQuantizeParams(in_scale=6.672646e-05, out_scale=3.1266262912055254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%3] -> add[%4]
  bit=32, threshold=1.1478554010391235
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.1266262912055254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%4] -> nn.relu[%5]
  bit=32, threshold=6.954009056091309
  SimulatedQuantizeParams(in_scale=3.1266263e-09, out_scale=3.1266262912055254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%5] -> nn.max_pool2d[%6]
  bit=32, threshold=6.954009056091309
  SimulatedQuantizeParams(in_scale=3.1266263e-09, out_scale=3.2382128090091555e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%6] -> nn.conv2d[%8]
  bit=8, threshold=6.954009056091309
  SimulatedQuantizeParams(in_scale=3.2382128e-09, out_scale=0.05432819575071335, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%7] -> nn.conv2d[%8]
  bit=8, threshold=0.7806138396263123
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0060985456220805645, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%8] -> add[%10]
  bit=32, threshold=3.609348277677782e-05
  SimulatedQuantizeParams(in_scale=0.00033132298, out_scale=2.9579814153635198e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%9] -> add[%10]
  bit=32, threshold=3.609348277677782e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.9579814153635198e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%10] -> add[%12]
  bit=32, threshold=1.1276936531066895
  SimulatedQuantizeParams(in_scale=2.9579814e-09, out_scale=2.9579725335793228e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%11] -> add[%12]
  bit=32, threshold=1.1276936531066895
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.9579725335793228e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%12] -> nn.relu[%13]
  bit=32, threshold=6.1032490730285645
  SimulatedQuantizeParams(in_scale=2.9579725e-09, out_scale=2.9579725335793228e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%13] -> nn.conv2d[%15]
  bit=8, threshold=2.3370871543884277
  SimulatedQuantizeParams(in_scale=2.9579725e-09, out_scale=0.01825849339365959, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%14] -> nn.conv2d[%15]
  bit=8, threshold=0.5345894694328308
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004176480229943991, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%15] -> add[%17]
  bit=32, threshold=1.262006402015686
  SimulatedQuantizeParams(in_scale=7.6256234e-05, out_scale=1.853938669249544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%16] -> add[%17]
  bit=32, threshold=1.262006402015686
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.853938669249544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%17] -> nn.relu[%18]
  bit=32, threshold=2.719296932220459
  SimulatedQuantizeParams(in_scale=1.8539387e-09, out_scale=1.853938669249544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%18] -> nn.conv2d[%20]
  bit=8, threshold=2.719296932220459
  SimulatedQuantizeParams(in_scale=1.8539387e-09, out_scale=0.021244507282972336, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%19] -> nn.conv2d[%20]
  bit=8, threshold=1.9482002258300781
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.015220314264297485, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%20] -> add[%22]
  bit=32, threshold=0.0005308822146616876
  SimulatedQuantizeParams(in_scale=0.00032334807, out_scale=1.840135377406682e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%21] -> add[%22]
  bit=32, threshold=0.0005308822146616876
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.840135377406682e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%22] -> add[%24]
  bit=32, threshold=1.3787344694137573
  SimulatedQuantizeParams(in_scale=1.8401354e-09, out_scale=1.8403378820863736e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%23] -> add[%24]
  bit=32, threshold=1.3787344694137573
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.8403378820863736e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.max_pool2d[%6] -> nn.conv2d[%26]
  bit=8, threshold=6.954009056091309
  SimulatedQuantizeParams(in_scale=3.2382128e-09, out_scale=0.05432819575071335, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%25] -> nn.conv2d[%26]
  bit=8, threshold=1.010841965675354
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007897202856838703, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%26] -> add[%28]
  bit=32, threshold=0.9353684782981873
  SimulatedQuantizeParams(in_scale=0.00042904078, out_scale=2.1943744599184356e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%27] -> add[%28]
  bit=32, threshold=0.9353684782981873
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.1943744599184356e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%24] -> add[%29]
  bit=32, threshold=4.421801567077637
  SimulatedQuantizeParams(in_scale=1.8403379e-09, out_scale=2.059061809944751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%28] -> add[%29]
  bit=32, threshold=4.421801567077637
  SimulatedQuantizeParams(in_scale=2.1943745e-09, out_scale=2.059061809944751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%29] -> nn.relu[%30]
  bit=32, threshold=4.7217559814453125
  SimulatedQuantizeParams(in_scale=2.0590618e-09, out_scale=2.059061809944751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%30] -> nn.conv2d[%32]
  bit=8, threshold=3.250359535217285
  SimulatedQuantizeParams(in_scale=2.0590618e-09, out_scale=0.02539343386888504, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%31] -> nn.conv2d[%32]
  bit=8, threshold=0.6211404204368591
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004852659534662962, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%32] -> add[%34]
  bit=32, threshold=0.0001491166913183406
  SimulatedQuantizeParams(in_scale=0.00012322569, out_scale=1.3124735742664484e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%33] -> add[%34]
  bit=32, threshold=0.0001491166913183406
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3124735742664484e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%34] -> add[%36]
  bit=32, threshold=0.866001307964325
  SimulatedQuantizeParams(in_scale=1.3124736e-09, out_scale=1.3124470399361599e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%35] -> add[%36]
  bit=32, threshold=0.866001307964325
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3124470399361599e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%36] -> nn.relu[%37]
  bit=32, threshold=2.6623101234436035
  SimulatedQuantizeParams(in_scale=1.312447e-09, out_scale=1.3124470399361599e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%37] -> nn.conv2d[%39]
  bit=8, threshold=2.152655839920044
  SimulatedQuantizeParams(in_scale=1.312447e-09, out_scale=0.016817623749375343, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%38] -> nn.conv2d[%39]
  bit=8, threshold=0.44724932312965393
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0034941353369504213, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%39] -> add[%41]
  bit=32, threshold=0.6904637813568115
  SimulatedQuantizeParams(in_scale=5.8763053e-05, out_scale=1.3525356390431398e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%40] -> add[%41]
  bit=32, threshold=0.6904637813568115
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3525356390431398e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%41] -> nn.relu[%42]
  bit=32, threshold=2.822270393371582
  SimulatedQuantizeParams(in_scale=1.3525356e-09, out_scale=1.3525356390431398e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%42] -> nn.conv2d[%44]
  bit=8, threshold=2.822270393371582
  SimulatedQuantizeParams(in_scale=1.3525356e-09, out_scale=0.022048987448215485, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%43] -> nn.conv2d[%44]
  bit=8, threshold=1.9035805463790894
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.014871723018586636, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%44] -> add[%46]
  bit=32, threshold=0.00045458402018994093
  SimulatedQuantizeParams(in_scale=0.00032790643, out_scale=1.8131315338010268e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%45] -> add[%46]
  bit=32, threshold=0.00045458402018994093
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.8131315338010268e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%46] -> add[%48]
  bit=32, threshold=0.7913899421691895
  SimulatedQuantizeParams(in_scale=1.8131315e-09, out_scale=1.8130416057360321e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%47] -> add[%48]
  bit=32, threshold=0.7913899421691895
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.8130416057360321e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%48] -> add[%49]
  bit=32, threshold=3.250359535217285
  SimulatedQuantizeParams(in_scale=1.8130416e-09, out_scale=1.7156489562353272e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%30] -> add[%49]
  bit=32, threshold=3.250359535217285
  SimulatedQuantizeParams(in_scale=2.0590618e-09, out_scale=1.7156489562353272e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%49] -> nn.relu[%50]
  bit=32, threshold=3.684328079223633
  SimulatedQuantizeParams(in_scale=1.715649e-09, out_scale=1.7156489562353272e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%50] -> nn.conv2d[%52]
  bit=8, threshold=3.5399110317230225
  SimulatedQuantizeParams(in_scale=1.715649e-09, out_scale=0.027655554935336113, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%51] -> nn.conv2d[%52]
  bit=8, threshold=0.3974285423755646
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0031049104873090982, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%52] -> add[%54]
  bit=32, threshold=8.732146670809016e-05
  SimulatedQuantizeParams(in_scale=8.586802e-05, out_scale=9.234557007964384e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%53] -> add[%54]
  bit=32, threshold=8.732146670809016e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.234557007964384e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%54] -> add[%56]
  bit=32, threshold=0.4925910234451294
  SimulatedQuantizeParams(in_scale=9.234557e-10, out_scale=9.234247810852025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%55] -> add[%56]
  bit=32, threshold=0.4925910234451294
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.234247810852025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%56] -> nn.relu[%57]
  bit=32, threshold=2.0954365730285645
  SimulatedQuantizeParams(in_scale=9.234248e-10, out_scale=9.234247810852025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%57] -> nn.conv2d[%59]
  bit=8, threshold=2.0954365730285645
  SimulatedQuantizeParams(in_scale=9.234248e-10, out_scale=0.01637059822678566, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%58] -> nn.conv2d[%59]
  bit=8, threshold=0.4486173391342163
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003504822961986065, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%59] -> add[%61]
  bit=32, threshold=0.5062980651855469
  SimulatedQuantizeParams(in_scale=5.7376048e-05, out_scale=9.160489033988028e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%60] -> add[%61]
  bit=32, threshold=0.5062980651855469
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.160489033988028e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%61] -> nn.relu[%62]
  bit=32, threshold=2.0664215087890625
  SimulatedQuantizeParams(in_scale=9.160489e-10, out_scale=9.160489033988028e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%62] -> nn.conv2d[%64]
  bit=8, threshold=2.0664215087890625
  SimulatedQuantizeParams(in_scale=9.160489e-10, out_scale=0.01614391803741455, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%63] -> nn.conv2d[%64]
  bit=8, threshold=1.8476616144180298
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.014434856362640858, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%64] -> add[%66]
  bit=32, threshold=0.0003840157587546855
  SimulatedQuantizeParams(in_scale=0.00023303514, out_scale=1.542562966250216e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%65] -> add[%66]
  bit=32, threshold=0.0003840157587546855
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.542562966250216e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%66] -> add[%68]
  bit=32, threshold=0.7631657719612122
  SimulatedQuantizeParams(in_scale=1.542563e-09, out_scale=1.5424130861418917e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%67] -> add[%68]
  bit=32, threshold=0.7631657719612122
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.5424130861418917e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%68] -> add[%69]
  bit=32, threshold=3.5399110317230225
  SimulatedQuantizeParams(in_scale=1.5424131e-09, out_scale=1.64839952798701e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%50] -> add[%69]
  bit=32, threshold=3.5399110317230225
  SimulatedQuantizeParams(in_scale=1.715649e-09, out_scale=1.64839952798701e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%69] -> nn.relu[%70]
  bit=32, threshold=3.573615789413452
  SimulatedQuantizeParams(in_scale=1.6483995e-09, out_scale=1.64839952798701e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%70] -> nn.conv2d[%72]
  bit=8, threshold=3.573615789413452
  SimulatedQuantizeParams(in_scale=1.6483995e-09, out_scale=0.027918873354792595, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%71] -> nn.conv2d[%72]
  bit=8, threshold=0.36732399463653564
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0028697187080979347, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%72] -> add[%74]
  bit=32, threshold=2.0109149772906676e-05
  SimulatedQuantizeParams(in_scale=8.011932e-05, out_scale=1.0458653942890805e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%73] -> add[%74]
  bit=32, threshold=2.0109149772906676e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0458653942890805e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%74] -> add[%76]
  bit=32, threshold=0.9314664602279663
  SimulatedQuantizeParams(in_scale=1.0458654e-09, out_scale=1.0458739430063702e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%75] -> add[%76]
  bit=32, threshold=0.9314664602279663
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0458739430063702e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%76] -> nn.relu[%77]
  bit=32, threshold=1.8265528678894043
  SimulatedQuantizeParams(in_scale=1.0458739e-09, out_scale=1.0458739430063702e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%77] -> nn.conv2d[%79]
  bit=8, threshold=1.8265528678894043
  SimulatedQuantizeParams(in_scale=1.0458739e-09, out_scale=0.014269944280385971, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%78] -> nn.conv2d[%79]
  bit=8, threshold=0.42026185989379883
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0032832957804203033, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%79] -> add[%81]
  bit=32, threshold=1.214323878288269
  SimulatedQuantizeParams(in_scale=4.685245e-05, out_scale=9.808754919404805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%80] -> add[%81]
  bit=32, threshold=1.214323878288269
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.808754919404805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%81] -> nn.relu[%82]
  bit=32, threshold=2.2599823474884033
  SimulatedQuantizeParams(in_scale=9.808755e-10, out_scale=9.808754919404805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%82] -> nn.conv2d[%84]
  bit=8, threshold=1.8708946704864502
  SimulatedQuantizeParams(in_scale=9.808755e-10, out_scale=0.014616364613175392, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%83] -> nn.conv2d[%84]
  bit=8, threshold=1.3626824617385864
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.010645956732332706, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%84] -> add[%86]
  bit=32, threshold=0.0001041114010149613
  SimulatedQuantizeParams(in_scale=0.00015560519, out_scale=1.2940133409244936e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%85] -> add[%86]
  bit=32, threshold=0.0001041114010149613
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2940133409244936e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%86] -> add[%88]
  bit=32, threshold=0.7123274803161621
  SimulatedQuantizeParams(in_scale=1.2940133e-09, out_scale=1.2939932458877479e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%87] -> add[%88]
  bit=32, threshold=0.7123274803161621
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2939932458877479e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%70] -> nn.conv2d[%90]
  bit=8, threshold=3.573615789413452
  SimulatedQuantizeParams(in_scale=1.6483995e-09, out_scale=0.027918873354792595, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%89] -> nn.conv2d[%90]
  bit=8, threshold=0.9827033281326294
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007677369751036167, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%90] -> add[%92]
  bit=32, threshold=0.7738578915596008
  SimulatedQuantizeParams(in_scale=0.00021434351, out_scale=1.4781132984253986e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%91] -> add[%92]
  bit=32, threshold=0.7738578915596008
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4781132984253986e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%88] -> add[%93]
  bit=32, threshold=2.983016014099121
  SimulatedQuantizeParams(in_scale=1.2939932e-09, out_scale=1.389075077184998e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%92] -> add[%93]
  bit=32, threshold=2.983016014099121
  SimulatedQuantizeParams(in_scale=1.4781133e-09, out_scale=1.389075077184998e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%93] -> nn.relu[%94]
  bit=32, threshold=3.1103503704071045
  SimulatedQuantizeParams(in_scale=1.3890751e-09, out_scale=1.389075077184998e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%94] -> nn.conv2d[%96]
  bit=8, threshold=3.1103503704071045
  SimulatedQuantizeParams(in_scale=1.3890751e-09, out_scale=0.024299612268805504, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%95] -> nn.conv2d[%96]
  bit=8, threshold=0.5742223262786865
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0044861119240522385, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%96] -> add[%98]
  bit=32, threshold=1.86040488188155e-05
  SimulatedQuantizeParams(in_scale=0.00010901078, out_scale=1.1727595561339399e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%97] -> add[%98]
  bit=32, threshold=1.86040488188155e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1727595561339399e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%98] -> add[%100]
  bit=32, threshold=0.7704314589500427
  SimulatedQuantizeParams(in_scale=1.1727596e-09, out_scale=1.172765107249063e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%99] -> add[%100]
  bit=32, threshold=0.7704314589500427
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.172765107249063e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%100] -> nn.relu[%101]
  bit=32, threshold=1.7646877765655518
  SimulatedQuantizeParams(in_scale=1.1727651e-09, out_scale=1.172765107249063e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%101] -> nn.conv2d[%103]
  bit=8, threshold=1.7646877765655518
  SimulatedQuantizeParams(in_scale=1.1727651e-09, out_scale=0.013786623254418373, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%102] -> nn.conv2d[%103]
  bit=8, threshold=0.47330763936042786
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0036977159325033426, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%103] -> add[%105]
  bit=32, threshold=0.723396897315979
  SimulatedQuantizeParams(in_scale=5.0979015e-05, out_scale=9.188217964251066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%104] -> add[%105]
  bit=32, threshold=0.723396897315979
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.188217964251066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%105] -> nn.relu[%106]
  bit=32, threshold=1.9288861751556396
  SimulatedQuantizeParams(in_scale=9.188218e-10, out_scale=9.188217964251066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%106] -> nn.conv2d[%108]
  bit=8, threshold=1.5150694847106934
  SimulatedQuantizeParams(in_scale=9.188218e-10, out_scale=0.011836480349302292, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%107] -> nn.conv2d[%108]
  bit=8, threshold=1.141211748123169
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008915716782212257, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%108] -> add[%110]
  bit=32, threshold=0.00010310194920748472
  SimulatedQuantizeParams(in_scale=0.00010553071, out_scale=9.716870641440778e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%109] -> add[%110]
  bit=32, threshold=0.00010310194920748472
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.716870641440778e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%110] -> add[%112]
  bit=32, threshold=0.6999180912971497
  SimulatedQuantizeParams(in_scale=9.716871e-10, out_scale=9.71730806931248e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%111] -> add[%112]
  bit=32, threshold=0.6999180912971497
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.71730806931248e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%112] -> add[%113]
  bit=32, threshold=3.1103503704071045
  SimulatedQuantizeParams(in_scale=9.717308e-10, out_scale=1.448369757461876e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%94] -> add[%113]
  bit=32, threshold=3.1103503704071045
  SimulatedQuantizeParams(in_scale=1.3890751e-09, out_scale=1.448369757461876e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%113] -> nn.relu[%114]
  bit=32, threshold=3.1264991760253906
  SimulatedQuantizeParams(in_scale=1.4483698e-09, out_scale=1.448369757461876e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%114] -> nn.conv2d[%116]
  bit=8, threshold=3.1264991760253906
  SimulatedQuantizeParams(in_scale=1.4483698e-09, out_scale=0.024425774812698364, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%115] -> nn.conv2d[%116]
  bit=8, threshold=0.5724043250083923
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004471908789128065, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%116] -> add[%118]
  bit=32, threshold=2.223523915745318e-05
  SimulatedQuantizeParams(in_scale=0.00010922983, out_scale=1.2402998628147088e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%117] -> add[%118]
  bit=32, threshold=2.223523915745318e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2402998628147088e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%118] -> add[%120]
  bit=32, threshold=0.38112956285476685
  SimulatedQuantizeParams(in_scale=1.2402999e-09, out_scale=1.240306635175159e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%119] -> add[%120]
  bit=32, threshold=0.38112956285476685
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.240306635175159e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%120] -> nn.relu[%121]
  bit=32, threshold=2.611788034439087
  SimulatedQuantizeParams(in_scale=1.2403066e-09, out_scale=1.240306635175159e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%121] -> nn.conv2d[%123]
  bit=8, threshold=2.611788034439087
  SimulatedQuantizeParams(in_scale=1.2403066e-09, out_scale=0.020404594019055367, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%122] -> nn.conv2d[%123]
  bit=8, threshold=0.49697554111480713
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0038826214149594307, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%123] -> add[%125]
  bit=32, threshold=0.6469704508781433
  SimulatedQuantizeParams(in_scale=7.922331e-05, out_scale=1.0765804914214527e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%124] -> add[%125]
  bit=32, threshold=0.6469704508781433
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0765804914214527e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%125] -> nn.relu[%126]
  bit=32, threshold=2.3451850414276123
  SimulatedQuantizeParams(in_scale=1.0765805e-09, out_scale=1.0765804914214527e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%126] -> nn.conv2d[%128]
  bit=8, threshold=2.3451850414276123
  SimulatedQuantizeParams(in_scale=1.0765805e-09, out_scale=0.01832175813615322, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%127] -> nn.conv2d[%128]
  bit=8, threshold=1.0766618251800537
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00841142050921917, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%128] -> add[%130]
  bit=32, threshold=9.10297385416925e-05
  SimulatedQuantizeParams(in_scale=0.00015411201, out_scale=9.949884249849106e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%129] -> add[%130]
  bit=32, threshold=9.10297385416925e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.949884249849106e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%130] -> add[%132]
  bit=32, threshold=0.5087187886238098
  SimulatedQuantizeParams(in_scale=9.949884e-10, out_scale=9.950194002072976e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%131] -> add[%132]
  bit=32, threshold=0.5087187886238098
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.950194002072976e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%132] -> add[%133]
  bit=32, threshold=3.1264991760253906
  SimulatedQuantizeParams(in_scale=9.950194e-10, out_scale=1.4558896310745695e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%114] -> add[%133]
  bit=32, threshold=3.1264991760253906
  SimulatedQuantizeParams(in_scale=1.4483698e-09, out_scale=1.4558896310745695e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%133] -> nn.relu[%134]
  bit=32, threshold=3.3671715259552
  SimulatedQuantizeParams(in_scale=1.4558896e-09, out_scale=1.4558896310745695e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%134] -> nn.conv2d[%136]
  bit=8, threshold=3.3671715259552
  SimulatedQuantizeParams(in_scale=1.4558896e-09, out_scale=0.026306027546525, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%135] -> nn.conv2d[%136]
  bit=8, threshold=0.5054697394371033
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003948982339352369, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%136] -> add[%138]
  bit=32, threshold=1.91199123946717e-05
  SimulatedQuantizeParams(in_scale=0.00010388204, out_scale=2.0212402862540557e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%137] -> add[%138]
  bit=32, threshold=1.91199123946717e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.0212402862540557e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%138] -> add[%140]
  bit=32, threshold=0.5363987684249878
  SimulatedQuantizeParams(in_scale=2.0212403e-09, out_scale=2.021247835770623e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%139] -> add[%140]
  bit=32, threshold=0.5363987684249878
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.021247835770623e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%140] -> nn.relu[%141]
  bit=32, threshold=4.052689552307129
  SimulatedQuantizeParams(in_scale=2.0212478e-09, out_scale=2.021247835770623e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%141] -> nn.conv2d[%143]
  bit=8, threshold=4.052689552307129
  SimulatedQuantizeParams(in_scale=2.0212478e-09, out_scale=0.031661637127399445, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%142] -> nn.conv2d[%143]
  bit=8, threshold=0.38693496584892273
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003022929420694709, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%143] -> add[%145]
  bit=32, threshold=0.30612683296203613
  SimulatedQuantizeParams(in_scale=9.571089e-05, out_scale=1.1473468841671774e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%144] -> add[%145]
  bit=32, threshold=0.30612683296203613
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1473468841671774e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%145] -> nn.relu[%146]
  bit=32, threshold=2.488009214401245
  SimulatedQuantizeParams(in_scale=1.1473469e-09, out_scale=1.1473468841671774e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%146] -> nn.conv2d[%148]
  bit=8, threshold=2.488009214401245
  SimulatedQuantizeParams(in_scale=1.1473469e-09, out_scale=0.019437571987509727, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%147] -> nn.conv2d[%148]
  bit=8, threshold=1.297215223312378
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.010134493932127953, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%148] -> add[%150]
  bit=32, threshold=9.05219349078834e-05
  SimulatedQuantizeParams(in_scale=0.00019698996, out_scale=1.2021488249303047e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%149] -> add[%150]
  bit=32, threshold=9.05219349078834e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2021488249303047e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%150] -> add[%152]
  bit=32, threshold=0.9778615236282349
  SimulatedQuantizeParams(in_scale=1.2021488e-09, out_scale=1.2021826867325558e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%151] -> add[%152]
  bit=32, threshold=0.9778615236282349
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2021826867325558e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%152] -> add[%153]
  bit=32, threshold=3.3671715259552
  SimulatedQuantizeParams(in_scale=1.2021827e-09, out_scale=1.5679614273622633e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%134] -> add[%153]
  bit=32, threshold=3.3671715259552
  SimulatedQuantizeParams(in_scale=1.4558896e-09, out_scale=1.5679614273622633e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%153] -> nn.relu[%154]
  bit=32, threshold=4.0864033699035645
  SimulatedQuantizeParams(in_scale=1.5679614e-09, out_scale=1.5679614273622633e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%154] -> nn.conv2d[%156]
  bit=8, threshold=4.0864033699035645
  SimulatedQuantizeParams(in_scale=1.5679614e-09, out_scale=0.0319250263273716, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%155] -> nn.conv2d[%156]
  bit=8, threshold=0.4596196413040161
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003590778447687626, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%156] -> add[%158]
  bit=32, threshold=9.595743904355913e-06
  SimulatedQuantizeParams(in_scale=0.000114635695, out_scale=7.105385169126066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%157] -> add[%158]
  bit=32, threshold=9.595743904355913e-06
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.105385169126066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%158] -> add[%160]
  bit=32, threshold=0.4184238314628601
  SimulatedQuantizeParams(in_scale=7.105385e-10, out_scale=7.105418475816805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%159] -> add[%160]
  bit=32, threshold=0.4184238314628601
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.105418475816805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%160] -> nn.relu[%161]
  bit=32, threshold=1.4605731964111328
  SimulatedQuantizeParams(in_scale=7.1054185e-10, out_scale=7.105418475816805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%161] -> nn.conv2d[%163]
  bit=8, threshold=1.4605731964111328
  SimulatedQuantizeParams(in_scale=7.1054185e-10, out_scale=0.011410728096961975, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%162] -> nn.conv2d[%163]
  bit=8, threshold=0.30364659428596497
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0023722390178591013, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%163] -> add[%165]
  bit=32, threshold=0.7777569890022278
  SimulatedQuantizeParams(in_scale=2.7068974e-05, out_scale=1.0230893909835004e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%164] -> add[%165]
  bit=32, threshold=0.7777569890022278
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0230893909835004e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%165] -> nn.relu[%166]
  bit=32, threshold=2.1624836921691895
  SimulatedQuantizeParams(in_scale=1.0230894e-09, out_scale=1.0230893909835004e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%166] -> nn.conv2d[%168]
  bit=8, threshold=1.4219791889190674
  SimulatedQuantizeParams(in_scale=1.0230894e-09, out_scale=0.011109212413430214, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%167] -> nn.conv2d[%168]
  bit=8, threshold=1.4316024780273438
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.011184394359588623, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%168] -> add[%170]
  bit=32, threshold=8.254509884864092e-05
  SimulatedQuantizeParams(in_scale=0.00012424981, out_scale=9.66815516534325e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%169] -> add[%170]
  bit=32, threshold=8.254509884864092e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.66815516534325e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%170] -> add[%172]
  bit=32, threshold=0.6372677087783813
  SimulatedQuantizeParams(in_scale=9.668155e-10, out_scale=9.668009726127025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%171] -> add[%172]
  bit=32, threshold=0.6372677087783813
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.668009726127025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%154] -> nn.conv2d[%174]
  bit=8, threshold=4.0864033699035645
  SimulatedQuantizeParams(in_scale=1.5679614e-09, out_scale=0.0319250263273716, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%173] -> nn.conv2d[%174]
  bit=8, threshold=0.5935622453689575
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004637205041944981, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%174] -> add[%176]
  bit=32, threshold=0.6732668280601501
  SimulatedQuantizeParams(in_scale=0.00014804289, out_scale=1.241974412202751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%175] -> add[%176]
  bit=32, threshold=0.6732668280601501
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.241974412202751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%172] -> add[%177]
  bit=32, threshold=2.593458652496338
  SimulatedQuantizeParams(in_scale=9.66801e-10, out_scale=1.2076732947008395e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%176] -> add[%177]
  bit=32, threshold=2.593458652496338
  SimulatedQuantizeParams(in_scale=1.2419744e-09, out_scale=1.2076732947008395e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%177] -> nn.relu[%178]
  bit=32, threshold=3.0308609008789062
  SimulatedQuantizeParams(in_scale=1.2076733e-09, out_scale=1.2076732947008395e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%178] -> nn.conv2d[%180]
  bit=8, threshold=3.0308609008789062
  SimulatedQuantizeParams(in_scale=1.2076733e-09, out_scale=0.023678600788116455, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%179] -> nn.conv2d[%180]
  bit=8, threshold=0.37425264716148376
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.002923848805949092, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%180] -> add[%182]
  bit=32, threshold=9.153559403785039e-06
  SimulatedQuantizeParams(in_scale=6.923265e-05, out_scale=9.068480966156756e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%181] -> add[%182]
  bit=32, threshold=9.153559403785039e-06
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.068480966156756e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%182] -> add[%184]
  bit=32, threshold=0.5056662559509277
  SimulatedQuantizeParams(in_scale=9.068481e-10, out_scale=9.068519823962617e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%183] -> add[%184]
  bit=32, threshold=0.5056662559509277
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.068519823962617e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%184] -> nn.relu[%185]
  bit=32, threshold=1.7014548778533936
  SimulatedQuantizeParams(in_scale=9.06852e-10, out_scale=9.068519823962617e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%185] -> nn.conv2d[%187]
  bit=8, threshold=1.7014548778533936
  SimulatedQuantizeParams(in_scale=9.06852e-10, out_scale=0.013292616233229637, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%186] -> nn.conv2d[%187]
  bit=8, threshold=0.33455488085746765
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.002613710006698966, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%187] -> add[%189]
  bit=32, threshold=0.46148568391799927
  SimulatedQuantizeParams(in_scale=3.4743043e-05, out_scale=8.573707299674993e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%188] -> add[%189]
  bit=32, threshold=0.46148568391799927
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.573707299674993e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%189] -> nn.relu[%190]
  bit=32, threshold=1.7053759098052979
  SimulatedQuantizeParams(in_scale=8.5737073e-10, out_scale=8.573707299674993e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%190] -> nn.conv2d[%192]
  bit=8, threshold=1.647982120513916
  SimulatedQuantizeParams(in_scale=8.5737073e-10, out_scale=0.012874860316514969, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%191] -> nn.conv2d[%192]
  bit=8, threshold=1.0857713222503662
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008482588455080986, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%192] -> add[%194]
  bit=32, threshold=9.36389114940539e-05
  SimulatedQuantizeParams(in_scale=0.00010921214, out_scale=7.320450912118304e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%193] -> add[%194]
  bit=32, threshold=9.36389114940539e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.320450912118304e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%194] -> add[%196]
  bit=32, threshold=0.2686067819595337
  SimulatedQuantizeParams(in_scale=7.320451e-10, out_scale=7.32013560877931e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%195] -> add[%196]
  bit=32, threshold=0.2686067819595337
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.32013560877931e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%196] -> add[%197]
  bit=32, threshold=3.0308609008789062
  SimulatedQuantizeParams(in_scale=7.3201356e-10, out_scale=1.4113545887539658e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%178] -> add[%197]
  bit=32, threshold=3.0308609008789062
  SimulatedQuantizeParams(in_scale=1.2076733e-09, out_scale=1.4113545887539658e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%197] -> nn.relu[%198]
  bit=32, threshold=2.7829463481903076
  SimulatedQuantizeParams(in_scale=1.4113546e-09, out_scale=1.4113545887539658e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%198] -> nn.conv2d[%200]
  bit=8, threshold=2.7829463481903076
  SimulatedQuantizeParams(in_scale=1.4113546e-09, out_scale=0.021741768345236778, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%199] -> nn.conv2d[%200]
  bit=8, threshold=0.4839337170124054
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003780732164159417, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%200] -> add[%202]
  bit=32, threshold=1.1808400813606568e-05
  SimulatedQuantizeParams(in_scale=8.2199804e-05, out_scale=9.288030344833942e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%201] -> add[%202]
  bit=32, threshold=1.1808400813606568e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.288030344833942e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%202] -> add[%204]
  bit=32, threshold=0.26229533553123474
  SimulatedQuantizeParams(in_scale=9.2880303e-10, out_scale=9.28807308842039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%203] -> add[%204]
  bit=32, threshold=0.26229533553123474
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.28807308842039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%204] -> nn.relu[%205]
  bit=32, threshold=1.8306634426116943
  SimulatedQuantizeParams(in_scale=9.288073e-10, out_scale=9.28807308842039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%205] -> nn.conv2d[%207]
  bit=8, threshold=1.8306634426116943
  SimulatedQuantizeParams(in_scale=9.288073e-10, out_scale=0.014302058145403862, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%206] -> nn.conv2d[%207]
  bit=8, threshold=0.36937764286994934
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0028857628349214792, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%207] -> add[%209]
  bit=32, threshold=0.43430301547050476
  SimulatedQuantizeParams(in_scale=4.127235e-05, out_scale=8.604608137119385e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%208] -> add[%209]
  bit=32, threshold=0.43430301547050476
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.604608137119385e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%209] -> nn.relu[%210]
  bit=32, threshold=1.8622921705245972
  SimulatedQuantizeParams(in_scale=8.604608e-10, out_scale=8.604608137119385e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%210] -> nn.conv2d[%212]
  bit=8, threshold=1.8622921705245972
  SimulatedQuantizeParams(in_scale=8.604608e-10, out_scale=0.014549157582223415, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%211] -> nn.conv2d[%212]
  bit=8, threshold=1.594677448272705
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.012458417564630508, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%212] -> add[%214]
  bit=32, threshold=0.00011440380330896005
  SimulatedQuantizeParams(in_scale=0.00018125949, out_scale=1.0783931525537582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%213] -> add[%214]
  bit=32, threshold=0.00011440380330896005
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0783931525537582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%214] -> add[%216]
  bit=32, threshold=0.3116801381111145
  SimulatedQuantizeParams(in_scale=1.0783932e-09, out_scale=1.0784292348020585e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%215] -> add[%216]
  bit=32, threshold=0.3116801381111145
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0784292348020585e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%216] -> add[%217]
  bit=32, threshold=2.7829463481903076
  SimulatedQuantizeParams(in_scale=1.0784292e-09, out_scale=1.2959103790066706e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%198] -> add[%217]
  bit=32, threshold=2.7829463481903076
  SimulatedQuantizeParams(in_scale=1.4113546e-09, out_scale=1.2959103790066706e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%217] -> nn.relu[%218]
  bit=32, threshold=2.8605494499206543
  SimulatedQuantizeParams(in_scale=1.2959104e-09, out_scale=1.2959103790066706e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%218] -> nn.conv2d[%220]
  bit=8, threshold=2.8605494499206543
  SimulatedQuantizeParams(in_scale=1.2959104e-09, out_scale=0.02234804257750511, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%219] -> nn.conv2d[%220]
  bit=8, threshold=0.5596972107887268
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004372634459286928, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%220] -> add[%222]
  bit=32, threshold=1.3071515240881126e-05
  SimulatedQuantizeParams(in_scale=9.771982e-05, out_scale=1.1261924726113648e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%221] -> add[%222]
  bit=32, threshold=1.3071515240881126e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1261924726113648e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%222] -> add[%224]
  bit=32, threshold=0.40307891368865967
  SimulatedQuantizeParams(in_scale=1.1261925e-09, out_scale=1.126197579637278e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%223] -> add[%224]
  bit=32, threshold=0.40307891368865967
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.126197579637278e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%224] -> nn.relu[%225]
  bit=32, threshold=2.409250020980835
  SimulatedQuantizeParams(in_scale=1.1261976e-09, out_scale=1.126197579637278e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%225] -> nn.conv2d[%227]
  bit=8, threshold=2.409250020980835
  SimulatedQuantizeParams(in_scale=1.1261976e-09, out_scale=0.018822265788912773, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%226] -> nn.conv2d[%227]
  bit=8, threshold=0.5429575443267822
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004241855815052986, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%227] -> add[%229]
  bit=32, threshold=0.4328063428401947
  SimulatedQuantizeParams(in_scale=7.984134e-05, out_scale=1.0331167032973099e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%228] -> add[%229]
  bit=32, threshold=0.4328063428401947
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0331167032973099e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%229] -> nn.relu[%230]
  bit=32, threshold=2.164431571960449
  SimulatedQuantizeParams(in_scale=1.0331167e-09, out_scale=1.0331167032973099e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%230] -> nn.conv2d[%232]
  bit=8, threshold=2.0097506046295166
  SimulatedQuantizeParams(in_scale=1.0331167e-09, out_scale=0.0157011765986681, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%231] -> nn.conv2d[%232]
  bit=8, threshold=1.2114336490631104
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00946432538330555, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%232] -> add[%234]
  bit=32, threshold=0.00010832246334757656
  SimulatedQuantizeParams(in_scale=0.00014860104, out_scale=1.1081046080718693e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%233] -> add[%234]
  bit=32, threshold=0.00010832246334757656
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1081046080718693e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%234] -> add[%236]
  bit=32, threshold=0.479257732629776
  SimulatedQuantizeParams(in_scale=1.1081046e-09, out_scale=1.1081320305805775e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%235] -> add[%236]
  bit=32, threshold=0.479257732629776
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1081320305805775e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%236] -> add[%237]
  bit=32, threshold=2.8605494499206543
  SimulatedQuantizeParams(in_scale=1.108132e-09, out_scale=1.3320471392574973e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%218] -> add[%237]
  bit=32, threshold=2.8605494499206543
  SimulatedQuantizeParams(in_scale=1.2959104e-09, out_scale=1.3320471392574973e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%237] -> nn.relu[%238]
  bit=32, threshold=3.1041388511657715
  SimulatedQuantizeParams(in_scale=1.3320471e-09, out_scale=1.3320471392574973e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%238] -> nn.conv2d[%240]
  bit=8, threshold=3.1041388511657715
  SimulatedQuantizeParams(in_scale=1.3320471e-09, out_scale=0.02425108477473259, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%239] -> nn.conv2d[%240]
  bit=8, threshold=0.6724961400032043
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005253876093775034, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%240] -> add[%242]
  bit=32, threshold=1.9492714272928424e-05
  SimulatedQuantizeParams(in_scale=0.00012741219, out_scale=1.4023641137228537e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%241] -> add[%242]
  bit=32, threshold=1.9492714272928424e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4023641137228537e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%242] -> add[%244]
  bit=32, threshold=0.3605198860168457
  SimulatedQuantizeParams(in_scale=1.4023641e-09, out_scale=1.4023695538156744e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%243] -> add[%244]
  bit=32, threshold=0.3605198860168457
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4023695538156744e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%244] -> nn.relu[%245]
  bit=32, threshold=2.9795713424682617
  SimulatedQuantizeParams(in_scale=1.4023696e-09, out_scale=1.4023695538156744e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%245] -> nn.conv2d[%247]
  bit=8, threshold=2.9795713424682617
  SimulatedQuantizeParams(in_scale=1.4023696e-09, out_scale=0.023277901113033295, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%246] -> nn.conv2d[%247]
  bit=8, threshold=0.831631600856781
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006497121881693602, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%247] -> add[%249]
  bit=32, threshold=0.36735647916793823
  SimulatedQuantizeParams(in_scale=0.00015123936, out_scale=2.5124580194102464e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%248] -> add[%249]
  bit=32, threshold=0.36735647916793823
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.5124580194102464e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%249] -> nn.relu[%250]
  bit=32, threshold=5.028106212615967
  SimulatedQuantizeParams(in_scale=2.512458e-09, out_scale=2.5124580194102464e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%250] -> nn.conv2d[%252]
  bit=8, threshold=5.028106212615967
  SimulatedQuantizeParams(in_scale=2.512458e-09, out_scale=0.03928207978606224, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%251] -> nn.conv2d[%252]
  bit=8, threshold=1.1028225421905518
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008615801110863686, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%252] -> add[%254]
  bit=32, threshold=7.481401553377509e-05
  SimulatedQuantizeParams(in_scale=0.00033844658, out_scale=1.1040436342923954e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%253] -> add[%254]
  bit=32, threshold=7.481401553377509e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1040436342923954e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%254] -> add[%256]
  bit=32, threshold=0.66150963306427
  SimulatedQuantizeParams(in_scale=1.1040436e-09, out_scale=1.10406683795361e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%255] -> add[%256]
  bit=32, threshold=0.66150963306427
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.10406683795361e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%256] -> add[%257]
  bit=32, threshold=3.1041388511657715
  SimulatedQuantizeParams(in_scale=1.1040668e-09, out_scale=1.44547729341582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%238] -> add[%257]
  bit=32, threshold=3.1041388511657715
  SimulatedQuantizeParams(in_scale=1.3320471e-09, out_scale=1.44547729341582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%257] -> nn.relu[%258]
  bit=32, threshold=4.179228782653809
  SimulatedQuantizeParams(in_scale=1.4454773e-09, out_scale=1.44547729341582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%258] -> nn.conv2d[%260]
  bit=8, threshold=4.179228782653809
  SimulatedQuantizeParams(in_scale=1.4454773e-09, out_scale=0.03265022486448288, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%259] -> nn.conv2d[%260]
  bit=8, threshold=0.6628884077072144
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005178815685212612, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%260] -> add[%262]
  bit=32, threshold=1.2748128938255832e-05
  SimulatedQuantizeParams(in_scale=0.0001690895, out_scale=1.3627603490107276e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%261] -> add[%262]
  bit=32, threshold=1.2748128938255832e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3627603490107276e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%262] -> add[%264]
  bit=32, threshold=0.4389674663543701
  SimulatedQuantizeParams(in_scale=1.3627603e-09, out_scale=1.3627646788805237e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%263] -> add[%264]
  bit=32, threshold=0.4389674663543701
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3627646788805237e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%264] -> nn.relu[%265]
  bit=32, threshold=2.8095805644989014
  SimulatedQuantizeParams(in_scale=1.3627647e-09, out_scale=1.3627646788805237e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%265] -> nn.conv2d[%267]
  bit=8, threshold=2.8095805644989014
  SimulatedQuantizeParams(in_scale=1.3627647e-09, out_scale=0.021949848160147667, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%266] -> nn.conv2d[%267]
  bit=8, threshold=0.4743006229400635
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003705473616719246, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%267] -> add[%269]
  bit=32, threshold=0.48615795373916626
  SimulatedQuantizeParams(in_scale=8.133458e-05, out_scale=1.0168995645543077e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%268] -> add[%269]
  bit=32, threshold=0.48615795373916626
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0168995645543077e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%269] -> nn.relu[%270]
  bit=32, threshold=2.192058563232422
  SimulatedQuantizeParams(in_scale=1.0168996e-09, out_scale=1.0168995645543077e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%270] -> nn.conv2d[%272]
  bit=8, threshold=2.192058563232422
  SimulatedQuantizeParams(in_scale=1.0168996e-09, out_scale=0.017125457525253296, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%271] -> nn.conv2d[%272]
  bit=8, threshold=1.115149974822998
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008712109178304672, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%272] -> add[%274]
  bit=32, threshold=8.815713226795197e-05
  SimulatedQuantizeParams(in_scale=0.00014919885, out_scale=1.2396232929035023e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%273] -> add[%274]
  bit=32, threshold=8.815713226795197e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2396232929035023e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%274] -> add[%276]
  bit=32, threshold=0.6554509997367859
  SimulatedQuantizeParams(in_scale=1.2396233e-09, out_scale=1.239594205060257e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%275] -> add[%276]
  bit=32, threshold=0.6554509997367859
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.239594205060257e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%276] -> add[%277]
  bit=32, threshold=4.179228782653809
  SimulatedQuantizeParams(in_scale=1.2395942e-09, out_scale=1.9461050548841285e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%258] -> add[%277]
  bit=32, threshold=4.179228782653809
  SimulatedQuantizeParams(in_scale=1.4454773e-09, out_scale=1.9461050548841285e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%277] -> nn.relu[%278]
  bit=32, threshold=4.210724830627441
  SimulatedQuantizeParams(in_scale=1.946105e-09, out_scale=1.9461050548841285e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%278] -> nn.conv2d[%280]
  bit=8, threshold=4.210724830627441
  SimulatedQuantizeParams(in_scale=1.946105e-09, out_scale=0.032896287739276886, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%279] -> nn.conv2d[%280]
  bit=8, threshold=0.49684634804725647
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003881612094119191, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%280] -> add[%282]
  bit=32, threshold=1.528146924556495e-07
  SimulatedQuantizeParams(in_scale=0.00012769063, out_scale=8.830364217615738e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%281] -> add[%282]
  bit=32, threshold=1.528146924556495e-07
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.830364217615738e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%282] -> add[%284]
  bit=32, threshold=0.35779690742492676
  SimulatedQuantizeParams(in_scale=8.830364e-10, out_scale=8.830363107392714e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%283] -> add[%284]
  bit=32, threshold=0.35779690742492676
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.830363107392714e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%284] -> nn.relu[%285]
  bit=32, threshold=1.92616868019104
  SimulatedQuantizeParams(in_scale=8.830363e-10, out_scale=8.830363107392714e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%285] -> nn.conv2d[%287]
  bit=8, threshold=1.92616868019104
  SimulatedQuantizeParams(in_scale=8.830363e-10, out_scale=0.0150481928139925, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%286] -> nn.conv2d[%287]
  bit=8, threshold=0.3185475468635559
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0024886527098715305, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%287] -> add[%289]
  bit=32, threshold=0.46971455216407776
  SimulatedQuantizeParams(in_scale=3.7449725e-05, out_scale=7.541323121529331e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%288] -> add[%289]
  bit=32, threshold=0.46971455216407776
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.541323121529331e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%289] -> nn.relu[%290]
  bit=32, threshold=1.5406055450439453
  SimulatedQuantizeParams(in_scale=7.541323e-10, out_scale=7.541323121529331e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%290] -> nn.conv2d[%292]
  bit=8, threshold=1.2875313758850098
  SimulatedQuantizeParams(in_scale=7.541323e-10, out_scale=0.010058838874101639, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%291] -> nn.conv2d[%292]
  bit=8, threshold=1.17811119556427
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00920399371534586, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%292] -> add[%294]
  bit=32, threshold=8.042706554078904e-07
  SimulatedQuantizeParams(in_scale=9.258149e-05, out_scale=6.462580470767421e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%293] -> add[%294]
  bit=32, threshold=8.042706554078904e-07
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.462580470767421e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%294] -> add[%296]
  bit=32, threshold=0.41873371601104736
  SimulatedQuantizeParams(in_scale=6.4625805e-10, out_scale=6.462579360544396e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%295] -> add[%296]
  bit=32, threshold=0.41873371601104736
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.462579360544396e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%278] -> nn.conv2d[%298]
  bit=8, threshold=4.210724830627441
  SimulatedQuantizeParams(in_scale=1.946105e-09, out_scale=0.032896287739276886, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%297] -> nn.conv2d[%298]
  bit=8, threshold=0.5161229372024536
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004032210446894169, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%298] -> add[%300]
  bit=32, threshold=0.4455389380455017
  SimulatedQuantizeParams(in_scale=0.00013264475, out_scale=9.297916880868229e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%299] -> add[%300]
  bit=32, threshold=0.4455389380455017
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.297916880868229e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%296] -> add[%301]
  bit=32, threshold=1.871436357498169
  SimulatedQuantizeParams(in_scale=6.4625794e-10, out_scale=8.714554633471039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%300] -> add[%301]
  bit=32, threshold=1.871436357498169
  SimulatedQuantizeParams(in_scale=9.297917e-10, out_scale=8.714554633471039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%301] -> nn.relu[%302]
  bit=32, threshold=2.130772113800049
  SimulatedQuantizeParams(in_scale=8.7145546e-10, out_scale=8.714554633471039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%302] -> nn.conv2d[%304]
  bit=8, threshold=1.954453945159912
  SimulatedQuantizeParams(in_scale=8.7145546e-10, out_scale=0.015269171446561813, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%303] -> nn.conv2d[%304]
  bit=8, threshold=0.5491331219673157
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004290102515369654, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%304] -> add[%306]
  bit=32, threshold=3.667103385396331e-07
  SimulatedQuantizeParams(in_scale=6.550631e-05, out_scale=1.0450131870953783e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%305] -> add[%306]
  bit=32, threshold=3.667103385396331e-07
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0450131870953783e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%306] -> add[%308]
  bit=32, threshold=0.5107687711715698
  SimulatedQuantizeParams(in_scale=1.0450132e-09, out_scale=1.0450130760730758e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%307] -> add[%308]
  bit=32, threshold=0.5107687711715698
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0450130760730758e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%308] -> nn.relu[%309]
  bit=32, threshold=2.2612874507904053
  SimulatedQuantizeParams(in_scale=1.0450131e-09, out_scale=1.0450130760730758e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%309] -> nn.conv2d[%311]
  bit=8, threshold=2.2612874507904053
  SimulatedQuantizeParams(in_scale=1.0450131e-09, out_scale=0.01766630820930004, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%310] -> nn.conv2d[%311]
  bit=8, threshold=0.34910857677459717
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0027274107560515404, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%311] -> add[%313]
  bit=32, threshold=0.676749587059021
  SimulatedQuantizeParams(in_scale=4.818328e-05, out_scale=1.0483554024887098e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%312] -> add[%313]
  bit=32, threshold=0.676749587059021
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0483554024887098e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%313] -> nn.relu[%314]
  bit=32, threshold=2.2335867881774902
  SimulatedQuantizeParams(in_scale=1.0483554e-09, out_scale=1.0483554024887098e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%314] -> nn.conv2d[%316]
  bit=8, threshold=2.2335867881774902
  SimulatedQuantizeParams(in_scale=1.0483554e-09, out_scale=0.017449896782636642, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%315] -> nn.conv2d[%316]
  bit=8, threshold=0.9454169273376465
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007386069744825363, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%316] -> add[%318]
  bit=32, threshold=1.1228644325456116e-06
  SimulatedQuantizeParams(in_scale=0.00012888615, out_scale=7.798541257209024e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%317] -> add[%318]
  bit=32, threshold=1.1228644325456116e-06
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.798541257209024e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%318] -> add[%320]
  bit=32, threshold=0.6884205341339111
  SimulatedQuantizeParams(in_scale=7.798541e-10, out_scale=7.798542367432049e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%319] -> add[%320]
  bit=32, threshold=0.6884205341339111
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.798542367432049e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%320] -> add[%321]
  bit=32, threshold=1.954453945159912
  SimulatedQuantizeParams(in_scale=7.7985424e-10, out_scale=9.101135400868543e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%302] -> add[%321]
  bit=32, threshold=1.954453945159912
  SimulatedQuantizeParams(in_scale=8.7145546e-10, out_scale=9.101135400868543e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%321] -> nn.relu[%322]
  bit=32, threshold=2.6619513034820557
  SimulatedQuantizeParams(in_scale=9.1011354e-10, out_scale=9.101135400868543e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%322] -> nn.conv2d[%324]
  bit=8, threshold=2.6619513034820557
  SimulatedQuantizeParams(in_scale=9.1011354e-10, out_scale=0.02079649455845356, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%323] -> nn.conv2d[%324]
  bit=8, threshold=0.8817574381828308
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006888729985803366, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%324] -> add[%326]
  bit=32, threshold=6.118337978477939e-07
  SimulatedQuantizeParams(in_scale=0.00014326144, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%325] -> add[%326]
  bit=32, threshold=6.118337978477939e-07
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%326] -> add[%328]
  bit=32, threshold=0.8181827068328857
  SimulatedQuantizeParams(in_scale=1.8166719e-09, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%327] -> add[%328]
  bit=32, threshold=0.8181827068328857
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%328] -> nn.relu[%329]
  bit=32, threshold=3.6566030979156494
  SimulatedQuantizeParams(in_scale=1.8166719e-09, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%329] -> nn.conv2d[%331]
  bit=8, threshold=3.6566030979156494
  SimulatedQuantizeParams(in_scale=1.8166719e-09, out_scale=0.02856721170246601, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%330] -> nn.conv2d[%331]
  bit=8, threshold=0.2999038100242615
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0023429985158145428, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%331] -> add[%333]
  bit=32, threshold=0.2335038185119629
  SimulatedQuantizeParams(in_scale=6.693293e-05, out_scale=1.4335400644327478e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%332] -> add[%333]
  bit=32, threshold=0.2335038185119629
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4335400644327478e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%333] -> nn.relu[%334]
  bit=32, threshold=3.1558542251586914
  SimulatedQuantizeParams(in_scale=1.4335401e-09, out_scale=1.4335400644327478e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%334] -> nn.conv2d[%336]
  bit=8, threshold=3.1558542251586914
  SimulatedQuantizeParams(in_scale=1.4335401e-09, out_scale=0.024655111134052277, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%335] -> nn.conv2d[%336]
  bit=8, threshold=21.305335998535156
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.1664479374885559, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%336] -> add[%338]
  bit=32, threshold=6.985711934248684e-06
  SimulatedQuantizeParams(in_scale=0.0041037924, out_scale=1.4537532067038228e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%337] -> add[%338]
  bit=32, threshold=6.985711934248684e-06
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4537532067038228e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%338] -> add[%340]
  bit=32, threshold=0.7500615119934082
  SimulatedQuantizeParams(in_scale=1.4537532e-08, out_scale=1.4537533843395067e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%339] -> add[%340]
  bit=32, threshold=0.7500615119934082
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4537533843395067e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%340] -> add[%341]
  bit=32, threshold=2.6619513034820557
  SimulatedQuantizeParams(in_scale=1.4537534e-08, out_scale=1.4612056453700006e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%322] -> add[%341]
  bit=32, threshold=2.6619513034820557
  SimulatedQuantizeParams(in_scale=9.1011354e-10, out_scale=1.4612056453700006e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%341] -> nn.relu[%342]
  bit=32, threshold=31.64546775817871
  SimulatedQuantizeParams(in_scale=1.46120565e-08, out_scale=1.4612056453700006e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%342] -> nn.global_avg_pool2d[%343]
  not quantized
  SimulatedQuantizeParams(in_scale=1.46120565e-08, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.global_avg_pool2d[%343] -> nn.batch_flatten[%344]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.batch_flatten[%344] -> nn.dense[%346]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%345] -> nn.dense[%346]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.dense[%346] -> add[%348]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%347] -> add[%348]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
add[%348] -> OUT
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
measures
Measure(version=0.1, strategy=Strategy(model_hash=8203843766811889671, bits=[8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32], thresholds=[2.632871389389038, 0.4152296781539917, 6.714378833770752, 1.1478554010391235, 6.954009056091309, 6.954009056091309, 6.954009056091309, 0.7806138396263123, 6.352216720581055, 3.609348277677782e-05, 6.352197647094727, 1.1276936531066895, 6.1032490730285645, 2.3370871543884277, 0.5345894694328308, 3.9813029766082764, 1.262006402015686, 2.719296932220459, 2.719296932220459, 1.9482002258300781, 3.951660633087158, 0.0005308822146616876, 3.9520955085754395, 1.3787344694137573, 2.940986394882202, 1.010841965675354, 4.712383270263672, 0.9353684782981873, 4.421801567077637, 4.7217559814453125, 3.250359535217285, 0.6211404204368591, 2.8185155391693115, 0.0001491166913183406, 2.8184585571289062, 0.866001307964325, 2.6623101234436035, 2.152655839920044, 0.44724932312965393, 2.904548168182373, 0.6904637813568115, 2.822270393371582, 2.822270393371582, 1.9035805463790894, 3.8936703205108643, 0.00045458402018994093, 3.893477201461792, 0.7913899421691895, 3.684328079223633, 3.684328079223633, 3.5399110317230225, 0.3974285423755646, 1.983106017112732, 8.732146670809016e-05, 1.9830396175384521, 0.4925910234451294, 2.0954365730285645, 2.0954365730285645, 0.4486173391342163, 1.9672000408172607, 0.5062980651855469, 2.0664215087890625, 2.0664215087890625, 1.8476616144180298, 3.312628746032715, 0.0003840157587546855, 3.3123068809509277, 0.7631657719612122, 2.5921828746795654, 3.573615789413452, 3.573615789413452, 0.36732399463653564, 2.245978832244873, 2.0109149772906676e-05, 2.245997190475464, 0.9314664602279663, 1.8265528678894043, 1.8265528678894043, 0.42026185989379883, 2.1064140796661377, 1.214323878288269, 2.2599823474884033, 1.8708946704864502, 1.3626824617385864, 2.778872489929199, 0.0001041114010149613, 2.778829336166382, 0.7123274803161621, 2.6818673610687256, 0.9827033281326294, 3.1742241382598877, 0.7738578915596008, 2.983016014099121, 3.1103503704071045, 3.1103503704071045, 0.5742223262786865, 2.518481969833374, 1.86040488188155e-05, 2.518493890762329, 0.7704314589500427, 1.7646877765655518, 1.7646877765655518, 0.47330763936042786, 1.9731547832489014, 0.723396897315979, 1.9288861751556396, 1.5150694847106934, 1.141211748123169, 2.086682081222534, 0.00010310194920748472, 2.0867760181427, 0.6999180912971497, 1.7131221294403076, 3.1264991760253906, 3.1264991760253906, 0.5724043250083923, 2.6635236740112305, 2.223523915745318e-05, 2.6635382175445557, 0.38112956285476685, 2.611788034439087, 2.611788034439087, 0.49697554111480713, 2.311939001083374, 0.6469704508781433, 2.3451850414276123, 2.3451850414276123, 1.0766618251800537, 2.13672137260437, 9.10297385416925e-05, 2.1367878913879395, 0.5087187886238098, 2.220126152038574, 3.3671715259552, 3.3671715259552, 0.5054697394371033, 4.340580463409424, 1.91199123946717e-05, 4.340596675872803, 0.5363987684249878, 4.052689552307129, 4.052689552307129, 0.38693496584892273, 2.4639086723327637, 0.30612683296203613, 2.488009214401245, 2.488009214401245, 1.297215223312378, 2.581594944000244, 9.05219349078834e-05, 2.58166766166687, 0.9778615236282349, 2.680483818054199, 4.0864033699035645, 4.0864033699035645, 0.4596196413040161, 1.5258698463439941, 9.595743904355913e-06, 1.5258769989013672, 0.4184238314628601, 1.4605731964111328, 1.4605731964111328, 0.30364659428596497, 2.1970677375793457, 0.7777569890022278, 2.1624836921691895, 1.4219791889190674, 1.4316024780273438, 2.0762205123901367, 8.254509884864092e-05, 2.0761892795562744, 0.6372677087783813, 1.6626203060150146, 0.5935622453689575, 2.6671197414398193, 0.6732668280601501, 2.593458652496338, 3.0308609008789062, 3.0308609008789062, 0.37425264716148376, 1.9474414587020874, 9.153559403785039e-06, 1.947449803352356, 0.5056662559509277, 1.7014548778533936, 1.7014548778533936, 0.33455488085746765, 1.8411896228790283, 0.46148568391799927, 1.7053759098052979, 1.647982120513916, 1.0857713222503662, 1.5720548629760742, 9.36389114940539e-05, 1.5719871520996094, 0.2686067819595337, 1.5411996841430664, 2.7829463481903076, 2.7829463481903076, 0.4839337170124054, 1.9945893287658691, 1.1808400813606568e-05, 1.9945985078811646, 0.26229533553123474, 1.8306634426116943, 1.8306634426116943, 0.36937764286994934, 1.847825527191162, 0.43430301547050476, 1.8622921705245972, 1.8622921705245972, 1.594677448272705, 2.3158316612243652, 0.00011440380330896005, 2.3159091472625732, 0.3116801381111145, 2.2071754932403564, 2.8605494499206543, 2.8605494499206543, 0.5596972107887268, 2.4184799194335938, 1.3071515240881126e-05, 2.4184908866882324, 0.40307891368865967, 2.409250020980835, 2.409250020980835, 0.5429575443267822, 2.2186012268066406, 0.4328063428401947, 2.164431571960449, 2.0097506046295166, 1.2114336490631104, 2.379636526107788, 0.00010832246334757656, 2.379695415496826, 0.479257732629776, 2.4099862575531006, 3.1041388511657715, 3.1041388511657715, 0.6724961400032043, 3.011554002761841, 1.9492714272928424e-05, 3.011565685272217, 0.3605198860168457, 2.9795713424682617, 2.9795713424682617, 0.831631600856781, 5.395462512969971, 0.36735647916793823, 5.028106212615967, 5.028106212615967, 1.1028225421905518, 2.370915651321411, 7.481401553377509e-05, 2.3709654808044434, 0.66150963306427, 2.523857831954956, 4.179228782653809, 4.179228782653809, 0.6628884077072144, 2.9265055656433105, 1.2748128938255832e-05, 2.9265148639678955, 0.4389674663543701, 2.8095805644989014, 2.8095805644989014, 0.4743006229400635, 2.1837751865386963, 0.48615795373916626, 2.192058563232422, 2.192058563232422, 1.115149974822998, 2.6620707511901855, 8.815713226795197e-05, 2.662008285522461, 0.6554509997367859, 2.7143783569335938, 4.210724830627441, 4.210724830627441, 0.49684634804725647, 1.8963062763214111, 1.528146924556495e-07, 1.896306037902832, 0.35779690742492676, 1.92616868019104, 1.92616868019104, 0.3185475468635559, 1.6194868087768555, 0.46971455216407776, 1.5406055450439453, 1.2875313758850098, 1.17811119556427, 1.3878285884857178, 8.042706554078904e-07, 1.3878283500671387, 0.41873371601104736, 1.2506301403045654, 0.5161229372024536, 1.9967124462127686, 0.4455389380455017, 1.871436357498169, 2.130772113800049, 1.954453945159912, 0.5491331219673157, 2.2441487312316895, 3.667103385396331e-07, 2.2441484928131104, 0.5107687711715698, 2.2612874507904053, 2.2612874507904053, 0.34910857677459717, 2.251326084136963, 0.676749587059021, 2.2335867881774902, 2.2335867881774902, 0.9454169273376465, 1.6747239828109741, 1.1228644325456116e-06, 1.6747242212295532, 0.6884205341339111, 1.6762053966522217, 2.6619513034820557, 2.6619513034820557, 0.8817574381828308, 3.901273250579834, 6.118337978477939e-07, 3.901273250579834, 0.8181827068328857, 3.6566030979156494, 3.6566030979156494, 0.2999038100242615, 3.0785038471221924, 0.2335038185119629, 3.1558542251586914, 3.1558542251586914, 21.305335998535156, 31.219112396240234, 6.985711934248684e-06, 31.2191162109375, 0.7500615119934082, 31.379152297973633, 31.64546775817871, 31.64546775817871, 10.443441390991211, 10.443441390991211, 0.6911724805831909, 27.76286506652832, 0.04205476865172386, 27.765003204345703]), result=MeasureResult(accuracy=0.796875, kl_distance=None))
best_measure
Measure(version=0.1, strategy=Strategy(model_hash=8203843766811889671, bits=[8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32], thresholds=[2.632871389389038, 0.4152296781539917, 6.714378833770752, 1.1478554010391235, 6.954009056091309, 6.954009056091309, 6.954009056091309, 0.7806138396263123, 6.352216720581055, 3.609348277677782e-05, 6.352197647094727, 1.1276936531066895, 6.1032490730285645, 2.3370871543884277, 0.5345894694328308, 3.9813029766082764, 1.262006402015686, 2.719296932220459, 2.719296932220459, 1.9482002258300781, 3.951660633087158, 0.0005308822146616876, 3.9520955085754395, 1.3787344694137573, 2.940986394882202, 1.010841965675354, 4.712383270263672, 0.9353684782981873, 4.421801567077637, 4.7217559814453125, 3.250359535217285, 0.6211404204368591, 2.8185155391693115, 0.0001491166913183406, 2.8184585571289062, 0.866001307964325, 2.6623101234436035, 2.152655839920044, 0.44724932312965393, 2.904548168182373, 0.6904637813568115, 2.822270393371582, 2.822270393371582, 1.9035805463790894, 3.8936703205108643, 0.00045458402018994093, 3.893477201461792, 0.7913899421691895, 3.684328079223633, 3.684328079223633, 3.5399110317230225, 0.3974285423755646, 1.983106017112732, 8.732146670809016e-05, 1.9830396175384521, 0.4925910234451294, 2.0954365730285645, 2.0954365730285645, 0.4486173391342163, 1.9672000408172607, 0.5062980651855469, 2.0664215087890625, 2.0664215087890625, 1.8476616144180298, 3.312628746032715, 0.0003840157587546855, 3.3123068809509277, 0.7631657719612122, 2.5921828746795654, 3.573615789413452, 3.573615789413452, 0.36732399463653564, 2.245978832244873, 2.0109149772906676e-05, 2.245997190475464, 0.9314664602279663, 1.8265528678894043, 1.8265528678894043, 0.42026185989379883, 2.1064140796661377, 1.214323878288269, 2.2599823474884033, 1.8708946704864502, 1.3626824617385864, 2.778872489929199, 0.0001041114010149613, 2.778829336166382, 0.7123274803161621, 2.6818673610687256, 0.9827033281326294, 3.1742241382598877, 0.7738578915596008, 2.983016014099121, 3.1103503704071045, 3.1103503704071045, 0.5742223262786865, 2.518481969833374, 1.86040488188155e-05, 2.518493890762329, 0.7704314589500427, 1.7646877765655518, 1.7646877765655518, 0.47330763936042786, 1.9731547832489014, 0.723396897315979, 1.9288861751556396, 1.5150694847106934, 1.141211748123169, 2.086682081222534, 0.00010310194920748472, 2.0867760181427, 0.6999180912971497, 1.7131221294403076, 3.1264991760253906, 3.1264991760253906, 0.5724043250083923, 2.6635236740112305, 2.223523915745318e-05, 2.6635382175445557, 0.38112956285476685, 2.611788034439087, 2.611788034439087, 0.49697554111480713, 2.311939001083374, 0.6469704508781433, 2.3451850414276123, 2.3451850414276123, 1.0766618251800537, 2.13672137260437, 9.10297385416925e-05, 2.1367878913879395, 0.5087187886238098, 2.220126152038574, 3.3671715259552, 3.3671715259552, 0.5054697394371033, 4.340580463409424, 1.91199123946717e-05, 4.340596675872803, 0.5363987684249878, 4.052689552307129, 4.052689552307129, 0.38693496584892273, 2.4639086723327637, 0.30612683296203613, 2.488009214401245, 2.488009214401245, 1.297215223312378, 2.581594944000244, 9.05219349078834e-05, 2.58166766166687, 0.9778615236282349, 2.680483818054199, 4.0864033699035645, 4.0864033699035645, 0.4596196413040161, 1.5258698463439941, 9.595743904355913e-06, 1.5258769989013672, 0.4184238314628601, 1.4605731964111328, 1.4605731964111328, 0.30364659428596497, 2.1970677375793457, 0.7777569890022278, 2.1624836921691895, 1.4219791889190674, 1.4316024780273438, 2.0762205123901367, 8.254509884864092e-05, 2.0761892795562744, 0.6372677087783813, 1.6626203060150146, 0.5935622453689575, 2.6671197414398193, 0.6732668280601501, 2.593458652496338, 3.0308609008789062, 3.0308609008789062, 0.37425264716148376, 1.9474414587020874, 9.153559403785039e-06, 1.947449803352356, 0.5056662559509277, 1.7014548778533936, 1.7014548778533936, 0.33455488085746765, 1.8411896228790283, 0.46148568391799927, 1.7053759098052979, 1.647982120513916, 1.0857713222503662, 1.5720548629760742, 9.36389114940539e-05, 1.5719871520996094, 0.2686067819595337, 1.5411996841430664, 2.7829463481903076, 2.7829463481903076, 0.4839337170124054, 1.9945893287658691, 1.1808400813606568e-05, 1.9945985078811646, 0.26229533553123474, 1.8306634426116943, 1.8306634426116943, 0.36937764286994934, 1.847825527191162, 0.43430301547050476, 1.8622921705245972, 1.8622921705245972, 1.594677448272705, 2.3158316612243652, 0.00011440380330896005, 2.3159091472625732, 0.3116801381111145, 2.2071754932403564, 2.8605494499206543, 2.8605494499206543, 0.5596972107887268, 2.4184799194335938, 1.3071515240881126e-05, 2.4184908866882324, 0.40307891368865967, 2.409250020980835, 2.409250020980835, 0.5429575443267822, 2.2186012268066406, 0.4328063428401947, 2.164431571960449, 2.0097506046295166, 1.2114336490631104, 2.379636526107788, 0.00010832246334757656, 2.379695415496826, 0.479257732629776, 2.4099862575531006, 3.1041388511657715, 3.1041388511657715, 0.6724961400032043, 3.011554002761841, 1.9492714272928424e-05, 3.011565685272217, 0.3605198860168457, 2.9795713424682617, 2.9795713424682617, 0.831631600856781, 5.395462512969971, 0.36735647916793823, 5.028106212615967, 5.028106212615967, 1.1028225421905518, 2.370915651321411, 7.481401553377509e-05, 2.3709654808044434, 0.66150963306427, 2.523857831954956, 4.179228782653809, 4.179228782653809, 0.6628884077072144, 2.9265055656433105, 1.2748128938255832e-05, 2.9265148639678955, 0.4389674663543701, 2.8095805644989014, 2.8095805644989014, 0.4743006229400635, 2.1837751865386963, 0.48615795373916626, 2.192058563232422, 2.192058563232422, 1.115149974822998, 2.6620707511901855, 8.815713226795197e-05, 2.662008285522461, 0.6554509997367859, 2.7143783569335938, 4.210724830627441, 4.210724830627441, 0.49684634804725647, 1.8963062763214111, 1.528146924556495e-07, 1.896306037902832, 0.35779690742492676, 1.92616868019104, 1.92616868019104, 0.3185475468635559, 1.6194868087768555, 0.46971455216407776, 1.5406055450439453, 1.2875313758850098, 1.17811119556427, 1.3878285884857178, 8.042706554078904e-07, 1.3878283500671387, 0.41873371601104736, 1.2506301403045654, 0.5161229372024536, 1.9967124462127686, 0.4455389380455017, 1.871436357498169, 2.130772113800049, 1.954453945159912, 0.5491331219673157, 2.2441487312316895, 3.667103385396331e-07, 2.2441484928131104, 0.5107687711715698, 2.2612874507904053, 2.2612874507904053, 0.34910857677459717, 2.251326084136963, 0.676749587059021, 2.2335867881774902, 2.2335867881774902, 0.9454169273376465, 1.6747239828109741, 1.1228644325456116e-06, 1.6747242212295532, 0.6884205341339111, 1.6762053966522217, 2.6619513034820557, 2.6619513034820557, 0.8817574381828308, 3.901273250579834, 6.118337978477939e-07, 3.901273250579834, 0.8181827068328857, 3.6566030979156494, 3.6566030979156494, 0.2999038100242615, 3.0785038471221924, 0.2335038185119629, 3.1558542251586914, 3.1558542251586914, 21.305335998535156, 31.219112396240234, 6.985711934248684e-06, 31.2191162109375, 0.7500615119934082, 31.379152297973633, 31.64546775817871, 31.64546775817871, 10.443441390991211, 10.443441390991211, 0.6911724805831909, 27.76286506652832, 0.04205476865172386, 27.765003204345703]), result=MeasureResult(accuracy=0.796875, kl_distance=None))
data
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
constant
nn.conv2d
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
constant
add
add
nn.relu
nn.global_avg_pool2d
nn.batch_flatten
constant
nn.dense
constant
add
data -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
add -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> add
constant -> add
add -> add
nn.relu -> add
add -> nn.relu
nn.relu -> nn.global_avg_pool2d
nn.global_avg_pool2d -> nn.batch_flatten
nn.batch_flatten -> nn.dense
constant -> nn.dense
nn.dense -> add
constant -> add
analyzed condition
node_conds: [False, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, False, True, True, True, False, False, False, False, False, False]
edge_conds: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]

select descriptor
---------
nn.conv2d[%2]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%4]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%5]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%6]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%8]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%10]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%12]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%13]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%15]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%17]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%18]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%20]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%22]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%24]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%26]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%28]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%29]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%30]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%32]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%34]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%36]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%37]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%39]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%41]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%42]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%44]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%46]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%48]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%49]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%50]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%52]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%54]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%56]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%57]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%59]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%61]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%62]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%64]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%66]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%68]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%69]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%70]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%72]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%74]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%76]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%77]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%79]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%81]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%82]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%84]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%86]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%88]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%90]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%92]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%93]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%94]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%96]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%98]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%100]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%101]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%103]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%105]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%106]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%108]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%110]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%112]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%113]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%114]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%116]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%118]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%120]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%121]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%123]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%125]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%126]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%128]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%130]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%132]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%133]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%134]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%136]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%138]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%140]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%141]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%143]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%145]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%146]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%148]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%150]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%152]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%153]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%154]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%156]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%158]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%160]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%161]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%163]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%165]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%166]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%168]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%170]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%172]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%174]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%176]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%177]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%178]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%180]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%182]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%184]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%185]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%187]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%189]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%190]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%192]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%194]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%196]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%197]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%198]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%200]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%202]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%204]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%205]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%207]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%209]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%210]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%212]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%214]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%216]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%217]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%218]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%220]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%222]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%224]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%225]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%227]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%229]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%230]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%232]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%234]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%236]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%237]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%238]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%240]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%242]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%244]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%245]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%247]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%249]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%250]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%252]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%254]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%256]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%257]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%258]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%260]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%262]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%264]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%265]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%267]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%269]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%270]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%272]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%274]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%276]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%277]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%278]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%280]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%282]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%284]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%285]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%287]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%289]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%290]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%292]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%294]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%296]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%298]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%300]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%301]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%302]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%304]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%306]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%308]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%309]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%311]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%313]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%314]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%316]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%318]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%320]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%321]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%322]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%324]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%326]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%328]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%329]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%331]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%333]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%334]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%336]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%338]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%340]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
add[%341]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%342]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %4 = nn.conv2d(%3, meta[relay.Constant][2] /* ty=Tensor[(64, 64, 1, 1), float32] */ /* ty=Tensor[(64, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %5 = add(%4, meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %6 = add(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%7, meta[relay.Constant][5] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %9 = add(%8, meta[relay.Constant][6] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.relu(%9) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.conv2d(%10, meta[relay.Constant][7] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %12 = add(%11, meta[relay.Constant][8] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %13 = add(%12, meta[relay.Constant][9] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %14 = nn.conv2d(%3, meta[relay.Constant][10] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %15 = add(%14, meta[relay.Constant][11] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %16 = add(%13, %15) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %17 = nn.relu(%16) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %18 = nn.conv2d(%17, meta[relay.Constant][12] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %19 = add(%18, meta[relay.Constant][13] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %20 = add(%19, meta[relay.Constant][14] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %21 = nn.relu(%20) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, meta[relay.Constant][15] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %23 = add(%22, meta[relay.Constant][16] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %24 = nn.relu(%23) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %25 = nn.conv2d(%24, meta[relay.Constant][17] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %26 = add(%25, meta[relay.Constant][18] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %27 = add(%26, meta[relay.Constant][19] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %28 = add(%27, %17) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %29 = nn.relu(%28) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %30 = nn.conv2d(%29, meta[relay.Constant][20] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %31 = add(%30, meta[relay.Constant][21] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %32 = add(%31, meta[relay.Constant][22] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %34 = nn.conv2d(%33, meta[relay.Constant][23] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %35 = add(%34, meta[relay.Constant][24] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %36 = nn.relu(%35) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %37 = nn.conv2d(%36, meta[relay.Constant][25] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %38 = add(%37, meta[relay.Constant][26] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %39 = add(%38, meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %40 = add(%39, %29) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %42 = nn.conv2d(%41, meta[relay.Constant][28] /* ty=Tensor[(128, 256, 1, 1), float32] */ /* ty=Tensor[(128, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %43 = add(%42, meta[relay.Constant][29] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %44 = add(%43, meta[relay.Constant][30] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %45 = nn.relu(%44) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %46 = nn.conv2d(%45, meta[relay.Constant][31] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %47 = add(%46, meta[relay.Constant][32] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %48 = nn.relu(%47) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %49 = nn.conv2d(%48, meta[relay.Constant][33] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %50 = add(%49, meta[relay.Constant][34] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %51 = add(%50, meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %52 = nn.conv2d(%41, meta[relay.Constant][36] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %53 = add(%52, meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %54 = add(%51, %53) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %55 = nn.relu(%54) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %56 = nn.conv2d(%55, meta[relay.Constant][38] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %57 = add(%56, meta[relay.Constant][39] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %58 = add(%57, meta[relay.Constant][40] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %59 = nn.relu(%58) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %60 = nn.conv2d(%59, meta[relay.Constant][41] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %61 = add(%60, meta[relay.Constant][42] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %63 = nn.conv2d(%62, meta[relay.Constant][43] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %64 = add(%63, meta[relay.Constant][44] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %65 = add(%64, meta[relay.Constant][45] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %66 = add(%65, %55) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %68 = nn.conv2d(%67, meta[relay.Constant][46] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %69 = add(%68, meta[relay.Constant][47] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %70 = add(%69, meta[relay.Constant][48] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %71 = nn.relu(%70) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %72 = nn.conv2d(%71, meta[relay.Constant][49] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %73 = add(%72, meta[relay.Constant][50] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %74 = nn.relu(%73) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %75 = nn.conv2d(%74, meta[relay.Constant][51] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %76 = add(%75, meta[relay.Constant][52] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %77 = add(%76, meta[relay.Constant][53] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %78 = add(%77, %67) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %79 = nn.relu(%78) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %80 = nn.conv2d(%79, meta[relay.Constant][54] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %81 = add(%80, meta[relay.Constant][55] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %82 = add(%81, meta[relay.Constant][56] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %83 = nn.relu(%82) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %84 = nn.conv2d(%83, meta[relay.Constant][57] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %85 = add(%84, meta[relay.Constant][58] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %86 = nn.relu(%85) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %87 = nn.conv2d(%86, meta[relay.Constant][59] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %88 = add(%87, meta[relay.Constant][60] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %89 = add(%88, meta[relay.Constant][61] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %90 = add(%89, %79) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %92 = nn.conv2d(%91, meta[relay.Constant][62] /* ty=Tensor[(256, 512, 1, 1), float32] */ /* ty=Tensor[(256, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %93 = add(%92, meta[relay.Constant][63] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %94 = add(%93, meta[relay.Constant][64] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %95 = nn.relu(%94) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %96 = nn.conv2d(%95, meta[relay.Constant][65] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %97 = add(%96, meta[relay.Constant][66] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %98 = nn.relu(%97) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %99 = nn.conv2d(%98, meta[relay.Constant][67] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %100 = add(%99, meta[relay.Constant][68] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %101 = add(%100, meta[relay.Constant][69] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %102 = nn.conv2d(%91, meta[relay.Constant][70] /* ty=Tensor[(1024, 512, 1, 1), float32] */ /* ty=Tensor[(1024, 512, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %103 = add(%102, meta[relay.Constant][71] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %104 = add(%101, %103) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %105 = nn.relu(%104) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %106 = nn.conv2d(%105, meta[relay.Constant][72] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %107 = add(%106, meta[relay.Constant][73] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %108 = add(%107, meta[relay.Constant][74] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %109 = nn.relu(%108) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %110 = nn.conv2d(%109, meta[relay.Constant][75] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %111 = add(%110, meta[relay.Constant][76] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %112 = nn.relu(%111) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %113 = nn.conv2d(%112, meta[relay.Constant][77] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %114 = add(%113, meta[relay.Constant][78] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %115 = add(%114, meta[relay.Constant][79] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %116 = add(%115, %105) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %117 = nn.relu(%116) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %118 = nn.conv2d(%117, meta[relay.Constant][80] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %119 = add(%118, meta[relay.Constant][81] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %120 = add(%119, meta[relay.Constant][82] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %121 = nn.relu(%120) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %122 = nn.conv2d(%121, meta[relay.Constant][83] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %123 = add(%122, meta[relay.Constant][84] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %125 = nn.conv2d(%124, meta[relay.Constant][85] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %126 = add(%125, meta[relay.Constant][86] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %127 = add(%126, meta[relay.Constant][87] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %128 = add(%127, %117) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %129 = nn.relu(%128) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %130 = nn.conv2d(%129, meta[relay.Constant][88] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %131 = add(%130, meta[relay.Constant][89] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %132 = add(%131, meta[relay.Constant][90] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %133 = nn.relu(%132) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %134 = nn.conv2d(%133, meta[relay.Constant][91] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %135 = add(%134, meta[relay.Constant][92] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %136 = nn.relu(%135) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %137 = nn.conv2d(%136, meta[relay.Constant][93] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %138 = add(%137, meta[relay.Constant][94] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %139 = add(%138, meta[relay.Constant][95] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %140 = add(%139, %129) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %141 = nn.relu(%140) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %142 = nn.conv2d(%141, meta[relay.Constant][96] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %143 = add(%142, meta[relay.Constant][97] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %144 = add(%143, meta[relay.Constant][98] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %145 = nn.relu(%144) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %146 = nn.conv2d(%145, meta[relay.Constant][99] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %147 = add(%146, meta[relay.Constant][100] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %148 = nn.relu(%147) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %149 = nn.conv2d(%148, meta[relay.Constant][101] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %150 = add(%149, meta[relay.Constant][102] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %151 = add(%150, meta[relay.Constant][103] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %152 = add(%151, %141) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %153 = nn.relu(%152) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %154 = nn.conv2d(%153, meta[relay.Constant][104] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %155 = add(%154, meta[relay.Constant][105] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %156 = add(%155, meta[relay.Constant][106] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %158 = nn.conv2d(%157, meta[relay.Constant][107] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %159 = add(%158, meta[relay.Constant][108] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %161 = nn.conv2d(%160, meta[relay.Constant][109] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %162 = add(%161, meta[relay.Constant][110] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %163 = add(%162, meta[relay.Constant][111] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %164 = add(%163, %153) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %165 = nn.relu(%164) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %166 = nn.conv2d(%165, meta[relay.Constant][112] /* ty=Tensor[(512, 1024, 1, 1), float32] */ /* ty=Tensor[(512, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %167 = add(%166, meta[relay.Constant][113] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %168 = add(%167, meta[relay.Constant][114] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %169 = nn.relu(%168) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %170 = nn.conv2d(%169, meta[relay.Constant][115] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %171 = add(%170, meta[relay.Constant][116] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %173 = nn.conv2d(%172, meta[relay.Constant][117] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %174 = add(%173, meta[relay.Constant][118] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %175 = add(%174, meta[relay.Constant][119] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %176 = nn.conv2d(%165, meta[relay.Constant][120] /* ty=Tensor[(2048, 1024, 1, 1), float32] */ /* ty=Tensor[(2048, 1024, 1, 1), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %177 = add(%176, meta[relay.Constant][121] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %178 = add(%175, %177) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %180 = nn.conv2d(%179, meta[relay.Constant][122] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %181 = add(%180, meta[relay.Constant][123] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %182 = add(%181, meta[relay.Constant][124] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %183 = nn.relu(%182) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %184 = nn.conv2d(%183, meta[relay.Constant][125] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %185 = add(%184, meta[relay.Constant][126] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %186 = nn.relu(%185) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %187 = nn.conv2d(%186, meta[relay.Constant][127] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %188 = add(%187, meta[relay.Constant][128] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %189 = add(%188, meta[relay.Constant][129] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %190 = add(%189, %179) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %191 = nn.relu(%190) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %192 = nn.conv2d(%191, meta[relay.Constant][130] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %193 = add(%192, meta[relay.Constant][131] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %194 = add(%193, meta[relay.Constant][132] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %195 = nn.relu(%194) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %196 = nn.conv2d(%195, meta[relay.Constant][133] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %197 = add(%196, meta[relay.Constant][134] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %198 = nn.relu(%197) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %199 = nn.conv2d(%198, meta[relay.Constant][135] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %200 = add(%199, meta[relay.Constant][136] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %201 = add(%200, meta[relay.Constant][137] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %202 = add(%201, %191) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %203 = nn.relu(%202) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %204 = nn.global_avg_pool2d(%203) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %205 = nn.batch_flatten(%204) /* ty=Tensor[(32, 2048), float32] */;
  %206 = nn.dense(%205, meta[relay.Constant][138] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%206, meta[relay.Constant][139] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data

calculate parameters
---------
data[%0] -> nn.conv2d[%2]
  bit=8, threshold=2.632871389389038
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.02056930772960186, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%1] -> nn.conv2d[%2]
  bit=8, threshold=0.4152296781539917
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00324398186057806, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%2] -> add[%4]
  bit=32, threshold=1.1478554010391235
  SimulatedQuantizeParams(in_scale=6.672646e-05, out_scale=3.1266262912055254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%3] -> add[%4]
  bit=32, threshold=1.1478554010391235
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.1266262912055254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%4] -> nn.relu[%5]
  bit=32, threshold=6.954009056091309
  SimulatedQuantizeParams(in_scale=3.1266263e-09, out_scale=3.1266262912055254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%5] -> nn.max_pool2d[%6]
  bit=32, threshold=6.954009056091309
  SimulatedQuantizeParams(in_scale=3.1266263e-09, out_scale=3.2382128090091555e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%6] -> nn.conv2d[%8]
  bit=8, threshold=6.954009056091309
  SimulatedQuantizeParams(in_scale=3.2382128e-09, out_scale=0.05432819575071335, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%7] -> nn.conv2d[%8]
  bit=8, threshold=0.7806138396263123
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0060985456220805645, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%8] -> add[%10]
  bit=32, threshold=3.609348277677782e-05
  SimulatedQuantizeParams(in_scale=0.00033132298, out_scale=2.9579814153635198e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%9] -> add[%10]
  bit=32, threshold=3.609348277677782e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.9579814153635198e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%10] -> add[%12]
  bit=32, threshold=1.1276936531066895
  SimulatedQuantizeParams(in_scale=2.9579814e-09, out_scale=2.9579725335793228e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%11] -> add[%12]
  bit=32, threshold=1.1276936531066895
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.9579725335793228e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%12] -> nn.relu[%13]
  bit=32, threshold=6.1032490730285645
  SimulatedQuantizeParams(in_scale=2.9579725e-09, out_scale=2.9579725335793228e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%13] -> nn.conv2d[%15]
  bit=8, threshold=2.3370871543884277
  SimulatedQuantizeParams(in_scale=2.9579725e-09, out_scale=0.01825849339365959, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%14] -> nn.conv2d[%15]
  bit=8, threshold=0.5345894694328308
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004176480229943991, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%15] -> add[%17]
  bit=32, threshold=1.262006402015686
  SimulatedQuantizeParams(in_scale=7.6256234e-05, out_scale=1.853938669249544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%16] -> add[%17]
  bit=32, threshold=1.262006402015686
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.853938669249544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%17] -> nn.relu[%18]
  bit=32, threshold=2.719296932220459
  SimulatedQuantizeParams(in_scale=1.8539387e-09, out_scale=1.853938669249544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%18] -> nn.conv2d[%20]
  bit=8, threshold=2.719296932220459
  SimulatedQuantizeParams(in_scale=1.8539387e-09, out_scale=0.021244507282972336, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%19] -> nn.conv2d[%20]
  bit=8, threshold=1.9482002258300781
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.015220314264297485, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%20] -> add[%22]
  bit=32, threshold=0.0005308822146616876
  SimulatedQuantizeParams(in_scale=0.00032334807, out_scale=1.840135377406682e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%21] -> add[%22]
  bit=32, threshold=0.0005308822146616876
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.840135377406682e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%22] -> add[%24]
  bit=32, threshold=1.3787344694137573
  SimulatedQuantizeParams(in_scale=1.8401354e-09, out_scale=1.8403378820863736e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%23] -> add[%24]
  bit=32, threshold=1.3787344694137573
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.8403378820863736e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.max_pool2d[%6] -> nn.conv2d[%26]
  bit=8, threshold=6.954009056091309
  SimulatedQuantizeParams(in_scale=3.2382128e-09, out_scale=0.05432819575071335, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%25] -> nn.conv2d[%26]
  bit=8, threshold=1.010841965675354
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007897202856838703, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%26] -> add[%28]
  bit=32, threshold=0.9353684782981873
  SimulatedQuantizeParams(in_scale=0.00042904078, out_scale=2.1943744599184356e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%27] -> add[%28]
  bit=32, threshold=0.9353684782981873
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.1943744599184356e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%24] -> add[%29]
  bit=32, threshold=4.421801567077637
  SimulatedQuantizeParams(in_scale=1.8403379e-09, out_scale=2.059061809944751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%28] -> add[%29]
  bit=32, threshold=4.421801567077637
  SimulatedQuantizeParams(in_scale=2.1943745e-09, out_scale=2.059061809944751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%29] -> nn.relu[%30]
  bit=32, threshold=4.7217559814453125
  SimulatedQuantizeParams(in_scale=2.0590618e-09, out_scale=2.059061809944751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%30] -> nn.conv2d[%32]
  bit=8, threshold=3.250359535217285
  SimulatedQuantizeParams(in_scale=2.0590618e-09, out_scale=0.02539343386888504, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%31] -> nn.conv2d[%32]
  bit=8, threshold=0.6211404204368591
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004852659534662962, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%32] -> add[%34]
  bit=32, threshold=0.0001491166913183406
  SimulatedQuantizeParams(in_scale=0.00012322569, out_scale=1.3124735742664484e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%33] -> add[%34]
  bit=32, threshold=0.0001491166913183406
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3124735742664484e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%34] -> add[%36]
  bit=32, threshold=0.866001307964325
  SimulatedQuantizeParams(in_scale=1.3124736e-09, out_scale=1.3124470399361599e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%35] -> add[%36]
  bit=32, threshold=0.866001307964325
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3124470399361599e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%36] -> nn.relu[%37]
  bit=32, threshold=2.6623101234436035
  SimulatedQuantizeParams(in_scale=1.312447e-09, out_scale=1.3124470399361599e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%37] -> nn.conv2d[%39]
  bit=8, threshold=2.152655839920044
  SimulatedQuantizeParams(in_scale=1.312447e-09, out_scale=0.016817623749375343, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%38] -> nn.conv2d[%39]
  bit=8, threshold=0.44724932312965393
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0034941353369504213, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%39] -> add[%41]
  bit=32, threshold=0.6904637813568115
  SimulatedQuantizeParams(in_scale=5.8763053e-05, out_scale=1.3525356390431398e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%40] -> add[%41]
  bit=32, threshold=0.6904637813568115
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3525356390431398e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%41] -> nn.relu[%42]
  bit=32, threshold=2.822270393371582
  SimulatedQuantizeParams(in_scale=1.3525356e-09, out_scale=1.3525356390431398e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%42] -> nn.conv2d[%44]
  bit=8, threshold=2.822270393371582
  SimulatedQuantizeParams(in_scale=1.3525356e-09, out_scale=0.022048987448215485, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%43] -> nn.conv2d[%44]
  bit=8, threshold=1.9035805463790894
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.014871723018586636, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%44] -> add[%46]
  bit=32, threshold=0.00045458402018994093
  SimulatedQuantizeParams(in_scale=0.00032790643, out_scale=1.8131315338010268e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%45] -> add[%46]
  bit=32, threshold=0.00045458402018994093
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.8131315338010268e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%46] -> add[%48]
  bit=32, threshold=0.7913899421691895
  SimulatedQuantizeParams(in_scale=1.8131315e-09, out_scale=1.8130416057360321e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%47] -> add[%48]
  bit=32, threshold=0.7913899421691895
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.8130416057360321e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%48] -> add[%49]
  bit=32, threshold=3.250359535217285
  SimulatedQuantizeParams(in_scale=1.8130416e-09, out_scale=1.7156489562353272e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%30] -> add[%49]
  bit=32, threshold=3.250359535217285
  SimulatedQuantizeParams(in_scale=2.0590618e-09, out_scale=1.7156489562353272e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%49] -> nn.relu[%50]
  bit=32, threshold=3.684328079223633
  SimulatedQuantizeParams(in_scale=1.715649e-09, out_scale=1.7156489562353272e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%50] -> nn.conv2d[%52]
  bit=8, threshold=3.5399110317230225
  SimulatedQuantizeParams(in_scale=1.715649e-09, out_scale=0.027655554935336113, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%51] -> nn.conv2d[%52]
  bit=8, threshold=0.3974285423755646
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0031049104873090982, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%52] -> add[%54]
  bit=32, threshold=8.732146670809016e-05
  SimulatedQuantizeParams(in_scale=8.586802e-05, out_scale=9.234557007964384e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%53] -> add[%54]
  bit=32, threshold=8.732146670809016e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.234557007964384e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%54] -> add[%56]
  bit=32, threshold=0.4925910234451294
  SimulatedQuantizeParams(in_scale=9.234557e-10, out_scale=9.234247810852025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%55] -> add[%56]
  bit=32, threshold=0.4925910234451294
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.234247810852025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%56] -> nn.relu[%57]
  bit=32, threshold=2.0954365730285645
  SimulatedQuantizeParams(in_scale=9.234248e-10, out_scale=9.234247810852025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%57] -> nn.conv2d[%59]
  bit=8, threshold=2.0954365730285645
  SimulatedQuantizeParams(in_scale=9.234248e-10, out_scale=0.01637059822678566, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%58] -> nn.conv2d[%59]
  bit=8, threshold=0.4486173391342163
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003504822961986065, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%59] -> add[%61]
  bit=32, threshold=0.5062980651855469
  SimulatedQuantizeParams(in_scale=5.7376048e-05, out_scale=9.160489033988028e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%60] -> add[%61]
  bit=32, threshold=0.5062980651855469
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.160489033988028e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%61] -> nn.relu[%62]
  bit=32, threshold=2.0664215087890625
  SimulatedQuantizeParams(in_scale=9.160489e-10, out_scale=9.160489033988028e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%62] -> nn.conv2d[%64]
  bit=8, threshold=2.0664215087890625
  SimulatedQuantizeParams(in_scale=9.160489e-10, out_scale=0.01614391803741455, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%63] -> nn.conv2d[%64]
  bit=8, threshold=1.8476616144180298
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.014434856362640858, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%64] -> add[%66]
  bit=32, threshold=0.0003840157587546855
  SimulatedQuantizeParams(in_scale=0.00023303514, out_scale=1.542562966250216e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%65] -> add[%66]
  bit=32, threshold=0.0003840157587546855
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.542562966250216e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%66] -> add[%68]
  bit=32, threshold=0.7631657719612122
  SimulatedQuantizeParams(in_scale=1.542563e-09, out_scale=1.5424130861418917e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%67] -> add[%68]
  bit=32, threshold=0.7631657719612122
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.5424130861418917e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%68] -> add[%69]
  bit=32, threshold=3.5399110317230225
  SimulatedQuantizeParams(in_scale=1.5424131e-09, out_scale=1.64839952798701e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%50] -> add[%69]
  bit=32, threshold=3.5399110317230225
  SimulatedQuantizeParams(in_scale=1.715649e-09, out_scale=1.64839952798701e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%69] -> nn.relu[%70]
  bit=32, threshold=3.573615789413452
  SimulatedQuantizeParams(in_scale=1.6483995e-09, out_scale=1.64839952798701e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%70] -> nn.conv2d[%72]
  bit=8, threshold=3.573615789413452
  SimulatedQuantizeParams(in_scale=1.6483995e-09, out_scale=0.027918873354792595, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%71] -> nn.conv2d[%72]
  bit=8, threshold=0.36732399463653564
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0028697187080979347, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%72] -> add[%74]
  bit=32, threshold=2.0109149772906676e-05
  SimulatedQuantizeParams(in_scale=8.011932e-05, out_scale=1.0458653942890805e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%73] -> add[%74]
  bit=32, threshold=2.0109149772906676e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0458653942890805e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%74] -> add[%76]
  bit=32, threshold=0.9314664602279663
  SimulatedQuantizeParams(in_scale=1.0458654e-09, out_scale=1.0458739430063702e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%75] -> add[%76]
  bit=32, threshold=0.9314664602279663
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0458739430063702e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%76] -> nn.relu[%77]
  bit=32, threshold=1.8265528678894043
  SimulatedQuantizeParams(in_scale=1.0458739e-09, out_scale=1.0458739430063702e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%77] -> nn.conv2d[%79]
  bit=8, threshold=1.8265528678894043
  SimulatedQuantizeParams(in_scale=1.0458739e-09, out_scale=0.014269944280385971, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%78] -> nn.conv2d[%79]
  bit=8, threshold=0.42026185989379883
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0032832957804203033, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%79] -> add[%81]
  bit=32, threshold=1.214323878288269
  SimulatedQuantizeParams(in_scale=4.685245e-05, out_scale=9.808754919404805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%80] -> add[%81]
  bit=32, threshold=1.214323878288269
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.808754919404805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%81] -> nn.relu[%82]
  bit=32, threshold=2.2599823474884033
  SimulatedQuantizeParams(in_scale=9.808755e-10, out_scale=9.808754919404805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%82] -> nn.conv2d[%84]
  bit=8, threshold=1.8708946704864502
  SimulatedQuantizeParams(in_scale=9.808755e-10, out_scale=0.014616364613175392, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%83] -> nn.conv2d[%84]
  bit=8, threshold=1.3626824617385864
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.010645956732332706, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%84] -> add[%86]
  bit=32, threshold=0.0001041114010149613
  SimulatedQuantizeParams(in_scale=0.00015560519, out_scale=1.2940133409244936e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%85] -> add[%86]
  bit=32, threshold=0.0001041114010149613
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2940133409244936e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%86] -> add[%88]
  bit=32, threshold=0.7123274803161621
  SimulatedQuantizeParams(in_scale=1.2940133e-09, out_scale=1.2939932458877479e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%87] -> add[%88]
  bit=32, threshold=0.7123274803161621
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2939932458877479e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%70] -> nn.conv2d[%90]
  bit=8, threshold=3.573615789413452
  SimulatedQuantizeParams(in_scale=1.6483995e-09, out_scale=0.027918873354792595, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%89] -> nn.conv2d[%90]
  bit=8, threshold=0.9827033281326294
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007677369751036167, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%90] -> add[%92]
  bit=32, threshold=0.7738578915596008
  SimulatedQuantizeParams(in_scale=0.00021434351, out_scale=1.4781132984253986e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%91] -> add[%92]
  bit=32, threshold=0.7738578915596008
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4781132984253986e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%88] -> add[%93]
  bit=32, threshold=2.983016014099121
  SimulatedQuantizeParams(in_scale=1.2939932e-09, out_scale=1.389075077184998e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%92] -> add[%93]
  bit=32, threshold=2.983016014099121
  SimulatedQuantizeParams(in_scale=1.4781133e-09, out_scale=1.389075077184998e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%93] -> nn.relu[%94]
  bit=32, threshold=3.1103503704071045
  SimulatedQuantizeParams(in_scale=1.3890751e-09, out_scale=1.389075077184998e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%94] -> nn.conv2d[%96]
  bit=8, threshold=3.1103503704071045
  SimulatedQuantizeParams(in_scale=1.3890751e-09, out_scale=0.024299612268805504, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%95] -> nn.conv2d[%96]
  bit=8, threshold=0.5742223262786865
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0044861119240522385, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%96] -> add[%98]
  bit=32, threshold=1.86040488188155e-05
  SimulatedQuantizeParams(in_scale=0.00010901078, out_scale=1.1727595561339399e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%97] -> add[%98]
  bit=32, threshold=1.86040488188155e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1727595561339399e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%98] -> add[%100]
  bit=32, threshold=0.7704314589500427
  SimulatedQuantizeParams(in_scale=1.1727596e-09, out_scale=1.172765107249063e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%99] -> add[%100]
  bit=32, threshold=0.7704314589500427
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.172765107249063e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%100] -> nn.relu[%101]
  bit=32, threshold=1.7646877765655518
  SimulatedQuantizeParams(in_scale=1.1727651e-09, out_scale=1.172765107249063e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%101] -> nn.conv2d[%103]
  bit=8, threshold=1.7646877765655518
  SimulatedQuantizeParams(in_scale=1.1727651e-09, out_scale=0.013786623254418373, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%102] -> nn.conv2d[%103]
  bit=8, threshold=0.47330763936042786
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0036977159325033426, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%103] -> add[%105]
  bit=32, threshold=0.723396897315979
  SimulatedQuantizeParams(in_scale=5.0979015e-05, out_scale=9.188217964251066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%104] -> add[%105]
  bit=32, threshold=0.723396897315979
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.188217964251066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%105] -> nn.relu[%106]
  bit=32, threshold=1.9288861751556396
  SimulatedQuantizeParams(in_scale=9.188218e-10, out_scale=9.188217964251066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%106] -> nn.conv2d[%108]
  bit=8, threshold=1.5150694847106934
  SimulatedQuantizeParams(in_scale=9.188218e-10, out_scale=0.011836480349302292, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%107] -> nn.conv2d[%108]
  bit=8, threshold=1.141211748123169
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008915716782212257, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%108] -> add[%110]
  bit=32, threshold=0.00010310194920748472
  SimulatedQuantizeParams(in_scale=0.00010553071, out_scale=9.716870641440778e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%109] -> add[%110]
  bit=32, threshold=0.00010310194920748472
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.716870641440778e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%110] -> add[%112]
  bit=32, threshold=0.6999180912971497
  SimulatedQuantizeParams(in_scale=9.716871e-10, out_scale=9.71730806931248e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%111] -> add[%112]
  bit=32, threshold=0.6999180912971497
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.71730806931248e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%112] -> add[%113]
  bit=32, threshold=3.1103503704071045
  SimulatedQuantizeParams(in_scale=9.717308e-10, out_scale=1.448369757461876e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%94] -> add[%113]
  bit=32, threshold=3.1103503704071045
  SimulatedQuantizeParams(in_scale=1.3890751e-09, out_scale=1.448369757461876e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%113] -> nn.relu[%114]
  bit=32, threshold=3.1264991760253906
  SimulatedQuantizeParams(in_scale=1.4483698e-09, out_scale=1.448369757461876e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%114] -> nn.conv2d[%116]
  bit=8, threshold=3.1264991760253906
  SimulatedQuantizeParams(in_scale=1.4483698e-09, out_scale=0.024425774812698364, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%115] -> nn.conv2d[%116]
  bit=8, threshold=0.5724043250083923
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004471908789128065, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%116] -> add[%118]
  bit=32, threshold=2.223523915745318e-05
  SimulatedQuantizeParams(in_scale=0.00010922983, out_scale=1.2402998628147088e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%117] -> add[%118]
  bit=32, threshold=2.223523915745318e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2402998628147088e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%118] -> add[%120]
  bit=32, threshold=0.38112956285476685
  SimulatedQuantizeParams(in_scale=1.2402999e-09, out_scale=1.240306635175159e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%119] -> add[%120]
  bit=32, threshold=0.38112956285476685
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.240306635175159e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%120] -> nn.relu[%121]
  bit=32, threshold=2.611788034439087
  SimulatedQuantizeParams(in_scale=1.2403066e-09, out_scale=1.240306635175159e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%121] -> nn.conv2d[%123]
  bit=8, threshold=2.611788034439087
  SimulatedQuantizeParams(in_scale=1.2403066e-09, out_scale=0.020404594019055367, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%122] -> nn.conv2d[%123]
  bit=8, threshold=0.49697554111480713
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0038826214149594307, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%123] -> add[%125]
  bit=32, threshold=0.6469704508781433
  SimulatedQuantizeParams(in_scale=7.922331e-05, out_scale=1.0765804914214527e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%124] -> add[%125]
  bit=32, threshold=0.6469704508781433
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0765804914214527e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%125] -> nn.relu[%126]
  bit=32, threshold=2.3451850414276123
  SimulatedQuantizeParams(in_scale=1.0765805e-09, out_scale=1.0765804914214527e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%126] -> nn.conv2d[%128]
  bit=8, threshold=2.3451850414276123
  SimulatedQuantizeParams(in_scale=1.0765805e-09, out_scale=0.01832175813615322, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%127] -> nn.conv2d[%128]
  bit=8, threshold=1.0766618251800537
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00841142050921917, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%128] -> add[%130]
  bit=32, threshold=9.10297385416925e-05
  SimulatedQuantizeParams(in_scale=0.00015411201, out_scale=9.949884249849106e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%129] -> add[%130]
  bit=32, threshold=9.10297385416925e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.949884249849106e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%130] -> add[%132]
  bit=32, threshold=0.5087187886238098
  SimulatedQuantizeParams(in_scale=9.949884e-10, out_scale=9.950194002072976e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%131] -> add[%132]
  bit=32, threshold=0.5087187886238098
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.950194002072976e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%132] -> add[%133]
  bit=32, threshold=3.1264991760253906
  SimulatedQuantizeParams(in_scale=9.950194e-10, out_scale=1.4558896310745695e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%114] -> add[%133]
  bit=32, threshold=3.1264991760253906
  SimulatedQuantizeParams(in_scale=1.4483698e-09, out_scale=1.4558896310745695e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%133] -> nn.relu[%134]
  bit=32, threshold=3.3671715259552
  SimulatedQuantizeParams(in_scale=1.4558896e-09, out_scale=1.4558896310745695e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%134] -> nn.conv2d[%136]
  bit=8, threshold=3.3671715259552
  SimulatedQuantizeParams(in_scale=1.4558896e-09, out_scale=0.026306027546525, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%135] -> nn.conv2d[%136]
  bit=8, threshold=0.5054697394371033
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003948982339352369, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%136] -> add[%138]
  bit=32, threshold=1.91199123946717e-05
  SimulatedQuantizeParams(in_scale=0.00010388204, out_scale=2.0212402862540557e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%137] -> add[%138]
  bit=32, threshold=1.91199123946717e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.0212402862540557e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%138] -> add[%140]
  bit=32, threshold=0.5363987684249878
  SimulatedQuantizeParams(in_scale=2.0212403e-09, out_scale=2.021247835770623e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%139] -> add[%140]
  bit=32, threshold=0.5363987684249878
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.021247835770623e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%140] -> nn.relu[%141]
  bit=32, threshold=4.052689552307129
  SimulatedQuantizeParams(in_scale=2.0212478e-09, out_scale=2.021247835770623e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%141] -> nn.conv2d[%143]
  bit=8, threshold=4.052689552307129
  SimulatedQuantizeParams(in_scale=2.0212478e-09, out_scale=0.031661637127399445, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%142] -> nn.conv2d[%143]
  bit=8, threshold=0.38693496584892273
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003022929420694709, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%143] -> add[%145]
  bit=32, threshold=0.30612683296203613
  SimulatedQuantizeParams(in_scale=9.571089e-05, out_scale=1.1473468841671774e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%144] -> add[%145]
  bit=32, threshold=0.30612683296203613
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1473468841671774e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%145] -> nn.relu[%146]
  bit=32, threshold=2.488009214401245
  SimulatedQuantizeParams(in_scale=1.1473469e-09, out_scale=1.1473468841671774e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%146] -> nn.conv2d[%148]
  bit=8, threshold=2.488009214401245
  SimulatedQuantizeParams(in_scale=1.1473469e-09, out_scale=0.019437571987509727, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%147] -> nn.conv2d[%148]
  bit=8, threshold=1.297215223312378
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.010134493932127953, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%148] -> add[%150]
  bit=32, threshold=9.05219349078834e-05
  SimulatedQuantizeParams(in_scale=0.00019698996, out_scale=1.2021488249303047e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%149] -> add[%150]
  bit=32, threshold=9.05219349078834e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2021488249303047e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%150] -> add[%152]
  bit=32, threshold=0.9778615236282349
  SimulatedQuantizeParams(in_scale=1.2021488e-09, out_scale=1.2021826867325558e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%151] -> add[%152]
  bit=32, threshold=0.9778615236282349
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2021826867325558e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%152] -> add[%153]
  bit=32, threshold=3.3671715259552
  SimulatedQuantizeParams(in_scale=1.2021827e-09, out_scale=1.5679614273622633e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%134] -> add[%153]
  bit=32, threshold=3.3671715259552
  SimulatedQuantizeParams(in_scale=1.4558896e-09, out_scale=1.5679614273622633e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%153] -> nn.relu[%154]
  bit=32, threshold=4.0864033699035645
  SimulatedQuantizeParams(in_scale=1.5679614e-09, out_scale=1.5679614273622633e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%154] -> nn.conv2d[%156]
  bit=8, threshold=4.0864033699035645
  SimulatedQuantizeParams(in_scale=1.5679614e-09, out_scale=0.0319250263273716, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%155] -> nn.conv2d[%156]
  bit=8, threshold=0.4596196413040161
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003590778447687626, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%156] -> add[%158]
  bit=32, threshold=9.595743904355913e-06
  SimulatedQuantizeParams(in_scale=0.000114635695, out_scale=7.105385169126066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%157] -> add[%158]
  bit=32, threshold=9.595743904355913e-06
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.105385169126066e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%158] -> add[%160]
  bit=32, threshold=0.4184238314628601
  SimulatedQuantizeParams(in_scale=7.105385e-10, out_scale=7.105418475816805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%159] -> add[%160]
  bit=32, threshold=0.4184238314628601
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.105418475816805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%160] -> nn.relu[%161]
  bit=32, threshold=1.4605731964111328
  SimulatedQuantizeParams(in_scale=7.1054185e-10, out_scale=7.105418475816805e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%161] -> nn.conv2d[%163]
  bit=8, threshold=1.4605731964111328
  SimulatedQuantizeParams(in_scale=7.1054185e-10, out_scale=0.011410728096961975, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%162] -> nn.conv2d[%163]
  bit=8, threshold=0.30364659428596497
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0023722390178591013, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%163] -> add[%165]
  bit=32, threshold=0.7777569890022278
  SimulatedQuantizeParams(in_scale=2.7068974e-05, out_scale=1.0230893909835004e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%164] -> add[%165]
  bit=32, threshold=0.7777569890022278
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0230893909835004e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%165] -> nn.relu[%166]
  bit=32, threshold=2.1624836921691895
  SimulatedQuantizeParams(in_scale=1.0230894e-09, out_scale=1.0230893909835004e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%166] -> nn.conv2d[%168]
  bit=8, threshold=1.4219791889190674
  SimulatedQuantizeParams(in_scale=1.0230894e-09, out_scale=0.011109212413430214, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%167] -> nn.conv2d[%168]
  bit=8, threshold=1.4316024780273438
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.011184394359588623, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%168] -> add[%170]
  bit=32, threshold=8.254509884864092e-05
  SimulatedQuantizeParams(in_scale=0.00012424981, out_scale=9.66815516534325e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%169] -> add[%170]
  bit=32, threshold=8.254509884864092e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.66815516534325e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%170] -> add[%172]
  bit=32, threshold=0.6372677087783813
  SimulatedQuantizeParams(in_scale=9.668155e-10, out_scale=9.668009726127025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%171] -> add[%172]
  bit=32, threshold=0.6372677087783813
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.668009726127025e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%154] -> nn.conv2d[%174]
  bit=8, threshold=4.0864033699035645
  SimulatedQuantizeParams(in_scale=1.5679614e-09, out_scale=0.0319250263273716, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%173] -> nn.conv2d[%174]
  bit=8, threshold=0.5935622453689575
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004637205041944981, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%174] -> add[%176]
  bit=32, threshold=0.6732668280601501
  SimulatedQuantizeParams(in_scale=0.00014804289, out_scale=1.241974412202751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%175] -> add[%176]
  bit=32, threshold=0.6732668280601501
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.241974412202751e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%172] -> add[%177]
  bit=32, threshold=2.593458652496338
  SimulatedQuantizeParams(in_scale=9.66801e-10, out_scale=1.2076732947008395e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%176] -> add[%177]
  bit=32, threshold=2.593458652496338
  SimulatedQuantizeParams(in_scale=1.2419744e-09, out_scale=1.2076732947008395e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%177] -> nn.relu[%178]
  bit=32, threshold=3.0308609008789062
  SimulatedQuantizeParams(in_scale=1.2076733e-09, out_scale=1.2076732947008395e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%178] -> nn.conv2d[%180]
  bit=8, threshold=3.0308609008789062
  SimulatedQuantizeParams(in_scale=1.2076733e-09, out_scale=0.023678600788116455, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%179] -> nn.conv2d[%180]
  bit=8, threshold=0.37425264716148376
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.002923848805949092, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%180] -> add[%182]
  bit=32, threshold=9.153559403785039e-06
  SimulatedQuantizeParams(in_scale=6.923265e-05, out_scale=9.068480966156756e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%181] -> add[%182]
  bit=32, threshold=9.153559403785039e-06
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.068480966156756e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%182] -> add[%184]
  bit=32, threshold=0.5056662559509277
  SimulatedQuantizeParams(in_scale=9.068481e-10, out_scale=9.068519823962617e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%183] -> add[%184]
  bit=32, threshold=0.5056662559509277
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.068519823962617e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%184] -> nn.relu[%185]
  bit=32, threshold=1.7014548778533936
  SimulatedQuantizeParams(in_scale=9.06852e-10, out_scale=9.068519823962617e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%185] -> nn.conv2d[%187]
  bit=8, threshold=1.7014548778533936
  SimulatedQuantizeParams(in_scale=9.06852e-10, out_scale=0.013292616233229637, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%186] -> nn.conv2d[%187]
  bit=8, threshold=0.33455488085746765
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.002613710006698966, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%187] -> add[%189]
  bit=32, threshold=0.46148568391799927
  SimulatedQuantizeParams(in_scale=3.4743043e-05, out_scale=8.573707299674993e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%188] -> add[%189]
  bit=32, threshold=0.46148568391799927
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.573707299674993e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%189] -> nn.relu[%190]
  bit=32, threshold=1.7053759098052979
  SimulatedQuantizeParams(in_scale=8.5737073e-10, out_scale=8.573707299674993e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%190] -> nn.conv2d[%192]
  bit=8, threshold=1.647982120513916
  SimulatedQuantizeParams(in_scale=8.5737073e-10, out_scale=0.012874860316514969, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%191] -> nn.conv2d[%192]
  bit=8, threshold=1.0857713222503662
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008482588455080986, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%192] -> add[%194]
  bit=32, threshold=9.36389114940539e-05
  SimulatedQuantizeParams(in_scale=0.00010921214, out_scale=7.320450912118304e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%193] -> add[%194]
  bit=32, threshold=9.36389114940539e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.320450912118304e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%194] -> add[%196]
  bit=32, threshold=0.2686067819595337
  SimulatedQuantizeParams(in_scale=7.320451e-10, out_scale=7.32013560877931e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%195] -> add[%196]
  bit=32, threshold=0.2686067819595337
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.32013560877931e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%196] -> add[%197]
  bit=32, threshold=3.0308609008789062
  SimulatedQuantizeParams(in_scale=7.3201356e-10, out_scale=1.4113545887539658e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%178] -> add[%197]
  bit=32, threshold=3.0308609008789062
  SimulatedQuantizeParams(in_scale=1.2076733e-09, out_scale=1.4113545887539658e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%197] -> nn.relu[%198]
  bit=32, threshold=2.7829463481903076
  SimulatedQuantizeParams(in_scale=1.4113546e-09, out_scale=1.4113545887539658e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%198] -> nn.conv2d[%200]
  bit=8, threshold=2.7829463481903076
  SimulatedQuantizeParams(in_scale=1.4113546e-09, out_scale=0.021741768345236778, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%199] -> nn.conv2d[%200]
  bit=8, threshold=0.4839337170124054
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003780732164159417, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%200] -> add[%202]
  bit=32, threshold=1.1808400813606568e-05
  SimulatedQuantizeParams(in_scale=8.2199804e-05, out_scale=9.288030344833942e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%201] -> add[%202]
  bit=32, threshold=1.1808400813606568e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.288030344833942e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%202] -> add[%204]
  bit=32, threshold=0.26229533553123474
  SimulatedQuantizeParams(in_scale=9.2880303e-10, out_scale=9.28807308842039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%203] -> add[%204]
  bit=32, threshold=0.26229533553123474
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.28807308842039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%204] -> nn.relu[%205]
  bit=32, threshold=1.8306634426116943
  SimulatedQuantizeParams(in_scale=9.288073e-10, out_scale=9.28807308842039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%205] -> nn.conv2d[%207]
  bit=8, threshold=1.8306634426116943
  SimulatedQuantizeParams(in_scale=9.288073e-10, out_scale=0.014302058145403862, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%206] -> nn.conv2d[%207]
  bit=8, threshold=0.36937764286994934
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0028857628349214792, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%207] -> add[%209]
  bit=32, threshold=0.43430301547050476
  SimulatedQuantizeParams(in_scale=4.127235e-05, out_scale=8.604608137119385e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%208] -> add[%209]
  bit=32, threshold=0.43430301547050476
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.604608137119385e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%209] -> nn.relu[%210]
  bit=32, threshold=1.8622921705245972
  SimulatedQuantizeParams(in_scale=8.604608e-10, out_scale=8.604608137119385e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%210] -> nn.conv2d[%212]
  bit=8, threshold=1.8622921705245972
  SimulatedQuantizeParams(in_scale=8.604608e-10, out_scale=0.014549157582223415, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%211] -> nn.conv2d[%212]
  bit=8, threshold=1.594677448272705
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.012458417564630508, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%212] -> add[%214]
  bit=32, threshold=0.00011440380330896005
  SimulatedQuantizeParams(in_scale=0.00018125949, out_scale=1.0783931525537582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%213] -> add[%214]
  bit=32, threshold=0.00011440380330896005
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0783931525537582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%214] -> add[%216]
  bit=32, threshold=0.3116801381111145
  SimulatedQuantizeParams(in_scale=1.0783932e-09, out_scale=1.0784292348020585e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%215] -> add[%216]
  bit=32, threshold=0.3116801381111145
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0784292348020585e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%216] -> add[%217]
  bit=32, threshold=2.7829463481903076
  SimulatedQuantizeParams(in_scale=1.0784292e-09, out_scale=1.2959103790066706e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%198] -> add[%217]
  bit=32, threshold=2.7829463481903076
  SimulatedQuantizeParams(in_scale=1.4113546e-09, out_scale=1.2959103790066706e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%217] -> nn.relu[%218]
  bit=32, threshold=2.8605494499206543
  SimulatedQuantizeParams(in_scale=1.2959104e-09, out_scale=1.2959103790066706e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%218] -> nn.conv2d[%220]
  bit=8, threshold=2.8605494499206543
  SimulatedQuantizeParams(in_scale=1.2959104e-09, out_scale=0.02234804257750511, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%219] -> nn.conv2d[%220]
  bit=8, threshold=0.5596972107887268
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004372634459286928, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%220] -> add[%222]
  bit=32, threshold=1.3071515240881126e-05
  SimulatedQuantizeParams(in_scale=9.771982e-05, out_scale=1.1261924726113648e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%221] -> add[%222]
  bit=32, threshold=1.3071515240881126e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1261924726113648e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%222] -> add[%224]
  bit=32, threshold=0.40307891368865967
  SimulatedQuantizeParams(in_scale=1.1261925e-09, out_scale=1.126197579637278e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%223] -> add[%224]
  bit=32, threshold=0.40307891368865967
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.126197579637278e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%224] -> nn.relu[%225]
  bit=32, threshold=2.409250020980835
  SimulatedQuantizeParams(in_scale=1.1261976e-09, out_scale=1.126197579637278e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%225] -> nn.conv2d[%227]
  bit=8, threshold=2.409250020980835
  SimulatedQuantizeParams(in_scale=1.1261976e-09, out_scale=0.018822265788912773, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%226] -> nn.conv2d[%227]
  bit=8, threshold=0.5429575443267822
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004241855815052986, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%227] -> add[%229]
  bit=32, threshold=0.4328063428401947
  SimulatedQuantizeParams(in_scale=7.984134e-05, out_scale=1.0331167032973099e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%228] -> add[%229]
  bit=32, threshold=0.4328063428401947
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0331167032973099e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%229] -> nn.relu[%230]
  bit=32, threshold=2.164431571960449
  SimulatedQuantizeParams(in_scale=1.0331167e-09, out_scale=1.0331167032973099e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%230] -> nn.conv2d[%232]
  bit=8, threshold=2.0097506046295166
  SimulatedQuantizeParams(in_scale=1.0331167e-09, out_scale=0.0157011765986681, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%231] -> nn.conv2d[%232]
  bit=8, threshold=1.2114336490631104
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00946432538330555, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%232] -> add[%234]
  bit=32, threshold=0.00010832246334757656
  SimulatedQuantizeParams(in_scale=0.00014860104, out_scale=1.1081046080718693e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%233] -> add[%234]
  bit=32, threshold=0.00010832246334757656
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1081046080718693e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%234] -> add[%236]
  bit=32, threshold=0.479257732629776
  SimulatedQuantizeParams(in_scale=1.1081046e-09, out_scale=1.1081320305805775e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%235] -> add[%236]
  bit=32, threshold=0.479257732629776
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1081320305805775e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%236] -> add[%237]
  bit=32, threshold=2.8605494499206543
  SimulatedQuantizeParams(in_scale=1.108132e-09, out_scale=1.3320471392574973e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%218] -> add[%237]
  bit=32, threshold=2.8605494499206543
  SimulatedQuantizeParams(in_scale=1.2959104e-09, out_scale=1.3320471392574973e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%237] -> nn.relu[%238]
  bit=32, threshold=3.1041388511657715
  SimulatedQuantizeParams(in_scale=1.3320471e-09, out_scale=1.3320471392574973e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%238] -> nn.conv2d[%240]
  bit=8, threshold=3.1041388511657715
  SimulatedQuantizeParams(in_scale=1.3320471e-09, out_scale=0.02425108477473259, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%239] -> nn.conv2d[%240]
  bit=8, threshold=0.6724961400032043
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005253876093775034, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%240] -> add[%242]
  bit=32, threshold=1.9492714272928424e-05
  SimulatedQuantizeParams(in_scale=0.00012741219, out_scale=1.4023641137228537e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%241] -> add[%242]
  bit=32, threshold=1.9492714272928424e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4023641137228537e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%242] -> add[%244]
  bit=32, threshold=0.3605198860168457
  SimulatedQuantizeParams(in_scale=1.4023641e-09, out_scale=1.4023695538156744e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%243] -> add[%244]
  bit=32, threshold=0.3605198860168457
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4023695538156744e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%244] -> nn.relu[%245]
  bit=32, threshold=2.9795713424682617
  SimulatedQuantizeParams(in_scale=1.4023696e-09, out_scale=1.4023695538156744e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%245] -> nn.conv2d[%247]
  bit=8, threshold=2.9795713424682617
  SimulatedQuantizeParams(in_scale=1.4023696e-09, out_scale=0.023277901113033295, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%246] -> nn.conv2d[%247]
  bit=8, threshold=0.831631600856781
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006497121881693602, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%247] -> add[%249]
  bit=32, threshold=0.36735647916793823
  SimulatedQuantizeParams(in_scale=0.00015123936, out_scale=2.5124580194102464e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%248] -> add[%249]
  bit=32, threshold=0.36735647916793823
  SimulatedQuantizeParams(in_scale=1.0, out_scale=2.5124580194102464e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%249] -> nn.relu[%250]
  bit=32, threshold=5.028106212615967
  SimulatedQuantizeParams(in_scale=2.512458e-09, out_scale=2.5124580194102464e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%250] -> nn.conv2d[%252]
  bit=8, threshold=5.028106212615967
  SimulatedQuantizeParams(in_scale=2.512458e-09, out_scale=0.03928207978606224, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%251] -> nn.conv2d[%252]
  bit=8, threshold=1.1028225421905518
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008615801110863686, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%252] -> add[%254]
  bit=32, threshold=7.481401553377509e-05
  SimulatedQuantizeParams(in_scale=0.00033844658, out_scale=1.1040436342923954e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%253] -> add[%254]
  bit=32, threshold=7.481401553377509e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.1040436342923954e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%254] -> add[%256]
  bit=32, threshold=0.66150963306427
  SimulatedQuantizeParams(in_scale=1.1040436e-09, out_scale=1.10406683795361e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%255] -> add[%256]
  bit=32, threshold=0.66150963306427
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.10406683795361e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%256] -> add[%257]
  bit=32, threshold=3.1041388511657715
  SimulatedQuantizeParams(in_scale=1.1040668e-09, out_scale=1.44547729341582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%238] -> add[%257]
  bit=32, threshold=3.1041388511657715
  SimulatedQuantizeParams(in_scale=1.3320471e-09, out_scale=1.44547729341582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%257] -> nn.relu[%258]
  bit=32, threshold=4.179228782653809
  SimulatedQuantizeParams(in_scale=1.4454773e-09, out_scale=1.44547729341582e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%258] -> nn.conv2d[%260]
  bit=8, threshold=4.179228782653809
  SimulatedQuantizeParams(in_scale=1.4454773e-09, out_scale=0.03265022486448288, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%259] -> nn.conv2d[%260]
  bit=8, threshold=0.6628884077072144
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005178815685212612, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%260] -> add[%262]
  bit=32, threshold=1.2748128938255832e-05
  SimulatedQuantizeParams(in_scale=0.0001690895, out_scale=1.3627603490107276e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%261] -> add[%262]
  bit=32, threshold=1.2748128938255832e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3627603490107276e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%262] -> add[%264]
  bit=32, threshold=0.4389674663543701
  SimulatedQuantizeParams(in_scale=1.3627603e-09, out_scale=1.3627646788805237e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%263] -> add[%264]
  bit=32, threshold=0.4389674663543701
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.3627646788805237e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%264] -> nn.relu[%265]
  bit=32, threshold=2.8095805644989014
  SimulatedQuantizeParams(in_scale=1.3627647e-09, out_scale=1.3627646788805237e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%265] -> nn.conv2d[%267]
  bit=8, threshold=2.8095805644989014
  SimulatedQuantizeParams(in_scale=1.3627647e-09, out_scale=0.021949848160147667, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%266] -> nn.conv2d[%267]
  bit=8, threshold=0.4743006229400635
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003705473616719246, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%267] -> add[%269]
  bit=32, threshold=0.48615795373916626
  SimulatedQuantizeParams(in_scale=8.133458e-05, out_scale=1.0168995645543077e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%268] -> add[%269]
  bit=32, threshold=0.48615795373916626
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0168995645543077e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%269] -> nn.relu[%270]
  bit=32, threshold=2.192058563232422
  SimulatedQuantizeParams(in_scale=1.0168996e-09, out_scale=1.0168995645543077e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%270] -> nn.conv2d[%272]
  bit=8, threshold=2.192058563232422
  SimulatedQuantizeParams(in_scale=1.0168996e-09, out_scale=0.017125457525253296, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%271] -> nn.conv2d[%272]
  bit=8, threshold=1.115149974822998
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008712109178304672, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%272] -> add[%274]
  bit=32, threshold=8.815713226795197e-05
  SimulatedQuantizeParams(in_scale=0.00014919885, out_scale=1.2396232929035023e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%273] -> add[%274]
  bit=32, threshold=8.815713226795197e-05
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.2396232929035023e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%274] -> add[%276]
  bit=32, threshold=0.6554509997367859
  SimulatedQuantizeParams(in_scale=1.2396233e-09, out_scale=1.239594205060257e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%275] -> add[%276]
  bit=32, threshold=0.6554509997367859
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.239594205060257e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%276] -> add[%277]
  bit=32, threshold=4.179228782653809
  SimulatedQuantizeParams(in_scale=1.2395942e-09, out_scale=1.9461050548841285e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%258] -> add[%277]
  bit=32, threshold=4.179228782653809
  SimulatedQuantizeParams(in_scale=1.4454773e-09, out_scale=1.9461050548841285e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%277] -> nn.relu[%278]
  bit=32, threshold=4.210724830627441
  SimulatedQuantizeParams(in_scale=1.946105e-09, out_scale=1.9461050548841285e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%278] -> nn.conv2d[%280]
  bit=8, threshold=4.210724830627441
  SimulatedQuantizeParams(in_scale=1.946105e-09, out_scale=0.032896287739276886, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%279] -> nn.conv2d[%280]
  bit=8, threshold=0.49684634804725647
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003881612094119191, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%280] -> add[%282]
  bit=32, threshold=1.528146924556495e-07
  SimulatedQuantizeParams(in_scale=0.00012769063, out_scale=8.830364217615738e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%281] -> add[%282]
  bit=32, threshold=1.528146924556495e-07
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.830364217615738e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%282] -> add[%284]
  bit=32, threshold=0.35779690742492676
  SimulatedQuantizeParams(in_scale=8.830364e-10, out_scale=8.830363107392714e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%283] -> add[%284]
  bit=32, threshold=0.35779690742492676
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.830363107392714e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%284] -> nn.relu[%285]
  bit=32, threshold=1.92616868019104
  SimulatedQuantizeParams(in_scale=8.830363e-10, out_scale=8.830363107392714e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%285] -> nn.conv2d[%287]
  bit=8, threshold=1.92616868019104
  SimulatedQuantizeParams(in_scale=8.830363e-10, out_scale=0.0150481928139925, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%286] -> nn.conv2d[%287]
  bit=8, threshold=0.3185475468635559
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0024886527098715305, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%287] -> add[%289]
  bit=32, threshold=0.46971455216407776
  SimulatedQuantizeParams(in_scale=3.7449725e-05, out_scale=7.541323121529331e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%288] -> add[%289]
  bit=32, threshold=0.46971455216407776
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.541323121529331e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%289] -> nn.relu[%290]
  bit=32, threshold=1.5406055450439453
  SimulatedQuantizeParams(in_scale=7.541323e-10, out_scale=7.541323121529331e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%290] -> nn.conv2d[%292]
  bit=8, threshold=1.2875313758850098
  SimulatedQuantizeParams(in_scale=7.541323e-10, out_scale=0.010058838874101639, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%291] -> nn.conv2d[%292]
  bit=8, threshold=1.17811119556427
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00920399371534586, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%292] -> add[%294]
  bit=32, threshold=8.042706554078904e-07
  SimulatedQuantizeParams(in_scale=9.258149e-05, out_scale=6.462580470767421e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%293] -> add[%294]
  bit=32, threshold=8.042706554078904e-07
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.462580470767421e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%294] -> add[%296]
  bit=32, threshold=0.41873371601104736
  SimulatedQuantizeParams(in_scale=6.4625805e-10, out_scale=6.462579360544396e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%295] -> add[%296]
  bit=32, threshold=0.41873371601104736
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.462579360544396e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
nn.relu[%278] -> nn.conv2d[%298]
  bit=8, threshold=4.210724830627441
  SimulatedQuantizeParams(in_scale=1.946105e-09, out_scale=0.032896287739276886, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%297] -> nn.conv2d[%298]
  bit=8, threshold=0.5161229372024536
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004032210446894169, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%298] -> add[%300]
  bit=32, threshold=0.4455389380455017
  SimulatedQuantizeParams(in_scale=0.00013264475, out_scale=9.297916880868229e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%299] -> add[%300]
  bit=32, threshold=0.4455389380455017
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.297916880868229e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%296] -> add[%301]
  bit=32, threshold=1.871436357498169
  SimulatedQuantizeParams(in_scale=6.4625794e-10, out_scale=8.714554633471039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%300] -> add[%301]
  bit=32, threshold=1.871436357498169
  SimulatedQuantizeParams(in_scale=9.297917e-10, out_scale=8.714554633471039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%301] -> nn.relu[%302]
  bit=32, threshold=2.130772113800049
  SimulatedQuantizeParams(in_scale=8.7145546e-10, out_scale=8.714554633471039e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%302] -> nn.conv2d[%304]
  bit=8, threshold=1.954453945159912
  SimulatedQuantizeParams(in_scale=8.7145546e-10, out_scale=0.015269171446561813, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%303] -> nn.conv2d[%304]
  bit=8, threshold=0.5491331219673157
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004290102515369654, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%304] -> add[%306]
  bit=32, threshold=3.667103385396331e-07
  SimulatedQuantizeParams(in_scale=6.550631e-05, out_scale=1.0450131870953783e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%305] -> add[%306]
  bit=32, threshold=3.667103385396331e-07
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0450131870953783e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%306] -> add[%308]
  bit=32, threshold=0.5107687711715698
  SimulatedQuantizeParams(in_scale=1.0450132e-09, out_scale=1.0450130760730758e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%307] -> add[%308]
  bit=32, threshold=0.5107687711715698
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0450130760730758e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%308] -> nn.relu[%309]
  bit=32, threshold=2.2612874507904053
  SimulatedQuantizeParams(in_scale=1.0450131e-09, out_scale=1.0450130760730758e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%309] -> nn.conv2d[%311]
  bit=8, threshold=2.2612874507904053
  SimulatedQuantizeParams(in_scale=1.0450131e-09, out_scale=0.01766630820930004, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%310] -> nn.conv2d[%311]
  bit=8, threshold=0.34910857677459717
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0027274107560515404, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%311] -> add[%313]
  bit=32, threshold=0.676749587059021
  SimulatedQuantizeParams(in_scale=4.818328e-05, out_scale=1.0483554024887098e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%312] -> add[%313]
  bit=32, threshold=0.676749587059021
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0483554024887098e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%313] -> nn.relu[%314]
  bit=32, threshold=2.2335867881774902
  SimulatedQuantizeParams(in_scale=1.0483554e-09, out_scale=1.0483554024887098e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%314] -> nn.conv2d[%316]
  bit=8, threshold=2.2335867881774902
  SimulatedQuantizeParams(in_scale=1.0483554e-09, out_scale=0.017449896782636642, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%315] -> nn.conv2d[%316]
  bit=8, threshold=0.9454169273376465
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007386069744825363, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%316] -> add[%318]
  bit=32, threshold=1.1228644325456116e-06
  SimulatedQuantizeParams(in_scale=0.00012888615, out_scale=7.798541257209024e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%317] -> add[%318]
  bit=32, threshold=1.1228644325456116e-06
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.798541257209024e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%318] -> add[%320]
  bit=32, threshold=0.6884205341339111
  SimulatedQuantizeParams(in_scale=7.798541e-10, out_scale=7.798542367432049e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%319] -> add[%320]
  bit=32, threshold=0.6884205341339111
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.798542367432049e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%320] -> add[%321]
  bit=32, threshold=1.954453945159912
  SimulatedQuantizeParams(in_scale=7.7985424e-10, out_scale=9.101135400868543e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%302] -> add[%321]
  bit=32, threshold=1.954453945159912
  SimulatedQuantizeParams(in_scale=8.7145546e-10, out_scale=9.101135400868543e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%321] -> nn.relu[%322]
  bit=32, threshold=2.6619513034820557
  SimulatedQuantizeParams(in_scale=9.1011354e-10, out_scale=9.101135400868543e-10, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%322] -> nn.conv2d[%324]
  bit=8, threshold=2.6619513034820557
  SimulatedQuantizeParams(in_scale=9.1011354e-10, out_scale=0.02079649455845356, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%323] -> nn.conv2d[%324]
  bit=8, threshold=0.8817574381828308
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006888729985803366, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%324] -> add[%326]
  bit=32, threshold=6.118337978477939e-07
  SimulatedQuantizeParams(in_scale=0.00014326144, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%325] -> add[%326]
  bit=32, threshold=6.118337978477939e-07
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%326] -> add[%328]
  bit=32, threshold=0.8181827068328857
  SimulatedQuantizeParams(in_scale=1.8166719e-09, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%327] -> add[%328]
  bit=32, threshold=0.8181827068328857
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%328] -> nn.relu[%329]
  bit=32, threshold=3.6566030979156494
  SimulatedQuantizeParams(in_scale=1.8166719e-09, out_scale=1.816671924004254e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%329] -> nn.conv2d[%331]
  bit=8, threshold=3.6566030979156494
  SimulatedQuantizeParams(in_scale=1.8166719e-09, out_scale=0.02856721170246601, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%330] -> nn.conv2d[%331]
  bit=8, threshold=0.2999038100242615
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0023429985158145428, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%331] -> add[%333]
  bit=32, threshold=0.2335038185119629
  SimulatedQuantizeParams(in_scale=6.693293e-05, out_scale=1.4335400644327478e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%332] -> add[%333]
  bit=32, threshold=0.2335038185119629
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4335400644327478e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%333] -> nn.relu[%334]
  bit=32, threshold=3.1558542251586914
  SimulatedQuantizeParams(in_scale=1.4335401e-09, out_scale=1.4335400644327478e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%334] -> nn.conv2d[%336]
  bit=8, threshold=3.1558542251586914
  SimulatedQuantizeParams(in_scale=1.4335401e-09, out_scale=0.024655111134052277, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%335] -> nn.conv2d[%336]
  bit=8, threshold=21.305335998535156
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.1664479374885559, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%336] -> add[%338]
  bit=32, threshold=6.985711934248684e-06
  SimulatedQuantizeParams(in_scale=0.0041037924, out_scale=1.4537532067038228e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%337] -> add[%338]
  bit=32, threshold=6.985711934248684e-06
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4537532067038228e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%338] -> add[%340]
  bit=32, threshold=0.7500615119934082
  SimulatedQuantizeParams(in_scale=1.4537532e-08, out_scale=1.4537533843395067e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%339] -> add[%340]
  bit=32, threshold=0.7500615119934082
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.4537533843395067e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%340] -> add[%341]
  bit=32, threshold=2.6619513034820557
  SimulatedQuantizeParams(in_scale=1.4537534e-08, out_scale=1.4612056453700006e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%322] -> add[%341]
  bit=32, threshold=2.6619513034820557
  SimulatedQuantizeParams(in_scale=9.1011354e-10, out_scale=1.4612056453700006e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
add[%341] -> nn.relu[%342]
  bit=32, threshold=31.64546775817871
  SimulatedQuantizeParams(in_scale=1.46120565e-08, out_scale=1.4612056453700006e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%342] -> nn.global_avg_pool2d[%343]
  not quantized
  SimulatedQuantizeParams(in_scale=1.46120565e-08, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.global_avg_pool2d[%343] -> nn.batch_flatten[%344]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.batch_flatten[%344] -> nn.dense[%346]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%345] -> nn.dense[%346]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.dense[%346] -> add[%348]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%347] -> add[%348]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
add[%348] -> OUT
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
num of snodes:
349
num of nodes:
349
---------
simulated_quantize(data[%0])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0205693f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%6])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00324398f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%12])
  in_scale: 6.67265e-05f /* ty=float32 */
  out_scale: 3.12663e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%18])
  in_scale: 1f /* ty=float32 */
  out_scale: 3.12663e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%24])
  in_scale: 3.12663e-09f /* ty=float32 */
  out_scale: 3.12663e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%30])
  in_scale: 3.12663e-09f /* ty=float32 */
  out_scale: 3.23821e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%36])
  in_scale: 3.23821e-09f /* ty=float32 */
  out_scale: 0.0543282f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%42])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00609855f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%48])
  in_scale: 0.000331323f /* ty=float32 */
  out_scale: 2.95798e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%54])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.95798e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%60])
  in_scale: 2.95798e-09f /* ty=float32 */
  out_scale: 2.95797e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%66])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.95797e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%72])
  in_scale: 2.95797e-09f /* ty=float32 */
  out_scale: 2.95797e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%78])
  in_scale: 2.95797e-09f /* ty=float32 */
  out_scale: 0.0182585f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%84])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00417648f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%90])
  in_scale: 7.62562e-05f /* ty=float32 */
  out_scale: 1.85394e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%96])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.85394e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%102])
  in_scale: 1.85394e-09f /* ty=float32 */
  out_scale: 1.85394e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%108])
  in_scale: 1.85394e-09f /* ty=float32 */
  out_scale: 0.0212445f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%114])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0152203f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%120])
  in_scale: 0.000323348f /* ty=float32 */
  out_scale: 1.84014e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%126])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.84014e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%132])
  in_scale: 1.84014e-09f /* ty=float32 */
  out_scale: 1.84034e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%138])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.84034e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%144])
  in_scale: 1.84034e-09f /* ty=float32 */
  out_scale: 2.05906e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%36])
  in_scale: 3.23821e-09f /* ty=float32 */
  out_scale: 0.0543282f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%155])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0078972f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%161])
  in_scale: 0.000429041f /* ty=float32 */
  out_scale: 2.19437e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%167])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.19437e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%173])
  in_scale: 2.19437e-09f /* ty=float32 */
  out_scale: 2.05906e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%179])
  in_scale: 2.05906e-09f /* ty=float32 */
  out_scale: 2.05906e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%185])
  in_scale: 2.05906e-09f /* ty=float32 */
  out_scale: 0.0253934f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%191])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00485266f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%197])
  in_scale: 0.000123226f /* ty=float32 */
  out_scale: 1.31247e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%203])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.31247e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%209])
  in_scale: 1.31247e-09f /* ty=float32 */
  out_scale: 1.31245e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%215])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.31245e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%221])
  in_scale: 1.31245e-09f /* ty=float32 */
  out_scale: 1.31245e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%227])
  in_scale: 1.31245e-09f /* ty=float32 */
  out_scale: 0.0168176f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%233])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00349414f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%239])
  in_scale: 5.87631e-05f /* ty=float32 */
  out_scale: 1.35254e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%245])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.35254e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%251])
  in_scale: 1.35254e-09f /* ty=float32 */
  out_scale: 1.35254e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%257])
  in_scale: 1.35254e-09f /* ty=float32 */
  out_scale: 0.022049f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%263])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0148717f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%269])
  in_scale: 0.000327906f /* ty=float32 */
  out_scale: 1.81313e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%275])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.81313e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%281])
  in_scale: 1.81313e-09f /* ty=float32 */
  out_scale: 1.81304e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%287])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.81304e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%293])
  in_scale: 1.81304e-09f /* ty=float32 */
  out_scale: 1.71565e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%185])
  in_scale: 2.05906e-09f /* ty=float32 */
  out_scale: 1.71565e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%304])
  in_scale: 1.71565e-09f /* ty=float32 */
  out_scale: 1.71565e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%310])
  in_scale: 1.71565e-09f /* ty=float32 */
  out_scale: 0.0276556f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%316])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00310491f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%322])
  in_scale: 8.5868e-05f /* ty=float32 */
  out_scale: 9.23456e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%328])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.23456e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%334])
  in_scale: 9.23456e-10f /* ty=float32 */
  out_scale: 9.23425e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%340])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.23425e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%346])
  in_scale: 9.23425e-10f /* ty=float32 */
  out_scale: 9.23425e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%352])
  in_scale: 9.23425e-10f /* ty=float32 */
  out_scale: 0.0163706f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%358])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00350482f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%364])
  in_scale: 5.7376e-05f /* ty=float32 */
  out_scale: 9.16049e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%370])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.16049e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%376])
  in_scale: 9.16049e-10f /* ty=float32 */
  out_scale: 9.16049e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%382])
  in_scale: 9.16049e-10f /* ty=float32 */
  out_scale: 0.0161439f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%388])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0144349f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%394])
  in_scale: 0.000233035f /* ty=float32 */
  out_scale: 1.54256e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%400])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.54256e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%406])
  in_scale: 1.54256e-09f /* ty=float32 */
  out_scale: 1.54241e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%412])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.54241e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%418])
  in_scale: 1.54241e-09f /* ty=float32 */
  out_scale: 1.6484e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%310])
  in_scale: 1.71565e-09f /* ty=float32 */
  out_scale: 1.6484e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%429])
  in_scale: 1.6484e-09f /* ty=float32 */
  out_scale: 1.6484e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%435])
  in_scale: 1.6484e-09f /* ty=float32 */
  out_scale: 0.0279189f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%441])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00286972f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%447])
  in_scale: 8.01193e-05f /* ty=float32 */
  out_scale: 1.04587e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%453])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.04587e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%459])
  in_scale: 1.04587e-09f /* ty=float32 */
  out_scale: 1.04587e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%465])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.04587e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%471])
  in_scale: 1.04587e-09f /* ty=float32 */
  out_scale: 1.04587e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%477])
  in_scale: 1.04587e-09f /* ty=float32 */
  out_scale: 0.0142699f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%483])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0032833f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%489])
  in_scale: 4.68524e-05f /* ty=float32 */
  out_scale: 9.80875e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%495])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.80875e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%501])
  in_scale: 9.80875e-10f /* ty=float32 */
  out_scale: 9.80875e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%507])
  in_scale: 9.80875e-10f /* ty=float32 */
  out_scale: 0.0146164f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%513])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.010646f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%519])
  in_scale: 0.000155605f /* ty=float32 */
  out_scale: 1.29401e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%525])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.29401e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%531])
  in_scale: 1.29401e-09f /* ty=float32 */
  out_scale: 1.29399e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%537])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.29399e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%543])
  in_scale: 1.29399e-09f /* ty=float32 */
  out_scale: 1.38908e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%435])
  in_scale: 1.6484e-09f /* ty=float32 */
  out_scale: 0.0279189f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%554])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00767737f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%560])
  in_scale: 0.000214344f /* ty=float32 */
  out_scale: 1.47811e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%566])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.47811e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%572])
  in_scale: 1.47811e-09f /* ty=float32 */
  out_scale: 1.38908e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%578])
  in_scale: 1.38908e-09f /* ty=float32 */
  out_scale: 1.38908e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%584])
  in_scale: 1.38908e-09f /* ty=float32 */
  out_scale: 0.0242996f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%590])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00448611f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%596])
  in_scale: 0.000109011f /* ty=float32 */
  out_scale: 1.17276e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%602])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.17276e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%608])
  in_scale: 1.17276e-09f /* ty=float32 */
  out_scale: 1.17277e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%614])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.17277e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%620])
  in_scale: 1.17277e-09f /* ty=float32 */
  out_scale: 1.17277e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%626])
  in_scale: 1.17277e-09f /* ty=float32 */
  out_scale: 0.0137866f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%632])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00369772f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%638])
  in_scale: 5.0979e-05f /* ty=float32 */
  out_scale: 9.18822e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%644])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.18822e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%650])
  in_scale: 9.18822e-10f /* ty=float32 */
  out_scale: 9.18822e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%656])
  in_scale: 9.18822e-10f /* ty=float32 */
  out_scale: 0.0118365f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%662])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00891572f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%668])
  in_scale: 0.000105531f /* ty=float32 */
  out_scale: 9.71687e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%674])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.71687e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%680])
  in_scale: 9.71687e-10f /* ty=float32 */
  out_scale: 9.71731e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%686])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.71731e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%692])
  in_scale: 9.71731e-10f /* ty=float32 */
  out_scale: 1.44837e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%584])
  in_scale: 1.38908e-09f /* ty=float32 */
  out_scale: 1.44837e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%703])
  in_scale: 1.44837e-09f /* ty=float32 */
  out_scale: 1.44837e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%709])
  in_scale: 1.44837e-09f /* ty=float32 */
  out_scale: 0.0244258f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%715])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00447191f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%721])
  in_scale: 0.00010923f /* ty=float32 */
  out_scale: 1.2403e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%727])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.2403e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%733])
  in_scale: 1.2403e-09f /* ty=float32 */
  out_scale: 1.24031e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%739])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.24031e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%745])
  in_scale: 1.24031e-09f /* ty=float32 */
  out_scale: 1.24031e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%751])
  in_scale: 1.24031e-09f /* ty=float32 */
  out_scale: 0.0204046f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%757])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00388262f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%763])
  in_scale: 7.92233e-05f /* ty=float32 */
  out_scale: 1.07658e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%769])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.07658e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%775])
  in_scale: 1.07658e-09f /* ty=float32 */
  out_scale: 1.07658e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%781])
  in_scale: 1.07658e-09f /* ty=float32 */
  out_scale: 0.0183218f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%787])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00841142f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%793])
  in_scale: 0.000154112f /* ty=float32 */
  out_scale: 9.94988e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%799])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.94988e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%805])
  in_scale: 9.94988e-10f /* ty=float32 */
  out_scale: 9.95019e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%811])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.95019e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%817])
  in_scale: 9.95019e-10f /* ty=float32 */
  out_scale: 1.45589e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%709])
  in_scale: 1.44837e-09f /* ty=float32 */
  out_scale: 1.45589e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%828])
  in_scale: 1.45589e-09f /* ty=float32 */
  out_scale: 1.45589e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%834])
  in_scale: 1.45589e-09f /* ty=float32 */
  out_scale: 0.026306f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%840])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00394898f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%846])
  in_scale: 0.000103882f /* ty=float32 */
  out_scale: 2.02124e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%852])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.02124e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%858])
  in_scale: 2.02124e-09f /* ty=float32 */
  out_scale: 2.02125e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%864])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.02125e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%870])
  in_scale: 2.02125e-09f /* ty=float32 */
  out_scale: 2.02125e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%876])
  in_scale: 2.02125e-09f /* ty=float32 */
  out_scale: 0.0316616f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%882])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00302293f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%888])
  in_scale: 9.57109e-05f /* ty=float32 */
  out_scale: 1.14735e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%894])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.14735e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%900])
  in_scale: 1.14735e-09f /* ty=float32 */
  out_scale: 1.14735e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%906])
  in_scale: 1.14735e-09f /* ty=float32 */
  out_scale: 0.0194376f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%912])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0101345f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%918])
  in_scale: 0.00019699f /* ty=float32 */
  out_scale: 1.20215e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%924])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.20215e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%930])
  in_scale: 1.20215e-09f /* ty=float32 */
  out_scale: 1.20218e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%936])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.20218e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%942])
  in_scale: 1.20218e-09f /* ty=float32 */
  out_scale: 1.56796e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%834])
  in_scale: 1.45589e-09f /* ty=float32 */
  out_scale: 1.56796e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%953])
  in_scale: 1.56796e-09f /* ty=float32 */
  out_scale: 1.56796e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%959])
  in_scale: 1.56796e-09f /* ty=float32 */
  out_scale: 0.031925f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%965])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00359078f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%971])
  in_scale: 0.000114636f /* ty=float32 */
  out_scale: 7.10539e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%977])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.10539e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%983])
  in_scale: 7.10539e-10f /* ty=float32 */
  out_scale: 7.10542e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%989])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.10542e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%995])
  in_scale: 7.10542e-10f /* ty=float32 */
  out_scale: 7.10542e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1001])
  in_scale: 7.10542e-10f /* ty=float32 */
  out_scale: 0.0114107f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1007])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00237224f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1013])
  in_scale: 2.7069e-05f /* ty=float32 */
  out_scale: 1.02309e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1019])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.02309e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1025])
  in_scale: 1.02309e-09f /* ty=float32 */
  out_scale: 1.02309e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1031])
  in_scale: 1.02309e-09f /* ty=float32 */
  out_scale: 0.0111092f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1037])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0111844f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1043])
  in_scale: 0.00012425f /* ty=float32 */
  out_scale: 9.66816e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1049])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.66816e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1055])
  in_scale: 9.66816e-10f /* ty=float32 */
  out_scale: 9.66801e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1061])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.66801e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1067])
  in_scale: 9.66801e-10f /* ty=float32 */
  out_scale: 1.20767e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%959])
  in_scale: 1.56796e-09f /* ty=float32 */
  out_scale: 0.031925f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1078])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00463721f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1084])
  in_scale: 0.000148043f /* ty=float32 */
  out_scale: 1.24197e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1090])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.24197e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1096])
  in_scale: 1.24197e-09f /* ty=float32 */
  out_scale: 1.20767e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1102])
  in_scale: 1.20767e-09f /* ty=float32 */
  out_scale: 1.20767e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1108])
  in_scale: 1.20767e-09f /* ty=float32 */
  out_scale: 0.0236786f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1114])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00292385f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1120])
  in_scale: 6.92327e-05f /* ty=float32 */
  out_scale: 9.06848e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1126])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.06848e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1132])
  in_scale: 9.06848e-10f /* ty=float32 */
  out_scale: 9.06852e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1138])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.06852e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1144])
  in_scale: 9.06852e-10f /* ty=float32 */
  out_scale: 9.06852e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1150])
  in_scale: 9.06852e-10f /* ty=float32 */
  out_scale: 0.0132926f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1156])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00261371f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1162])
  in_scale: 3.4743e-05f /* ty=float32 */
  out_scale: 8.57371e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1168])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.57371e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1174])
  in_scale: 8.57371e-10f /* ty=float32 */
  out_scale: 8.57371e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1180])
  in_scale: 8.57371e-10f /* ty=float32 */
  out_scale: 0.0128749f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1186])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00848259f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1192])
  in_scale: 0.000109212f /* ty=float32 */
  out_scale: 7.32045e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1198])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.32045e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1204])
  in_scale: 7.32045e-10f /* ty=float32 */
  out_scale: 7.32014e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1210])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.32014e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1216])
  in_scale: 7.32014e-10f /* ty=float32 */
  out_scale: 1.41135e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1108])
  in_scale: 1.20767e-09f /* ty=float32 */
  out_scale: 1.41135e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1227])
  in_scale: 1.41135e-09f /* ty=float32 */
  out_scale: 1.41135e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1233])
  in_scale: 1.41135e-09f /* ty=float32 */
  out_scale: 0.0217418f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1239])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00378073f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1245])
  in_scale: 8.21998e-05f /* ty=float32 */
  out_scale: 9.28803e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1251])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.28803e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1257])
  in_scale: 9.28803e-10f /* ty=float32 */
  out_scale: 9.28807e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1263])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.28807e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1269])
  in_scale: 9.28807e-10f /* ty=float32 */
  out_scale: 9.28807e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1275])
  in_scale: 9.28807e-10f /* ty=float32 */
  out_scale: 0.0143021f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1281])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00288576f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1287])
  in_scale: 4.12723e-05f /* ty=float32 */
  out_scale: 8.60461e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1293])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.60461e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1299])
  in_scale: 8.60461e-10f /* ty=float32 */
  out_scale: 8.60461e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1305])
  in_scale: 8.60461e-10f /* ty=float32 */
  out_scale: 0.0145492f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1311])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0124584f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1317])
  in_scale: 0.000181259f /* ty=float32 */
  out_scale: 1.07839e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1323])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.07839e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1329])
  in_scale: 1.07839e-09f /* ty=float32 */
  out_scale: 1.07843e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1335])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.07843e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1341])
  in_scale: 1.07843e-09f /* ty=float32 */
  out_scale: 1.29591e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1233])
  in_scale: 1.41135e-09f /* ty=float32 */
  out_scale: 1.29591e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1352])
  in_scale: 1.29591e-09f /* ty=float32 */
  out_scale: 1.29591e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1358])
  in_scale: 1.29591e-09f /* ty=float32 */
  out_scale: 0.022348f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1364])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00437263f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1370])
  in_scale: 9.77198e-05f /* ty=float32 */
  out_scale: 1.12619e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1376])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.12619e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1382])
  in_scale: 1.12619e-09f /* ty=float32 */
  out_scale: 1.1262e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1388])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.1262e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1394])
  in_scale: 1.1262e-09f /* ty=float32 */
  out_scale: 1.1262e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1400])
  in_scale: 1.1262e-09f /* ty=float32 */
  out_scale: 0.0188223f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1406])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00424186f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1412])
  in_scale: 7.98413e-05f /* ty=float32 */
  out_scale: 1.03312e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1418])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.03312e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1424])
  in_scale: 1.03312e-09f /* ty=float32 */
  out_scale: 1.03312e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1430])
  in_scale: 1.03312e-09f /* ty=float32 */
  out_scale: 0.0157012f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1436])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00946433f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1442])
  in_scale: 0.000148601f /* ty=float32 */
  out_scale: 1.1081e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1448])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.1081e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1454])
  in_scale: 1.1081e-09f /* ty=float32 */
  out_scale: 1.10813e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1460])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.10813e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1466])
  in_scale: 1.10813e-09f /* ty=float32 */
  out_scale: 1.33205e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1358])
  in_scale: 1.29591e-09f /* ty=float32 */
  out_scale: 1.33205e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1477])
  in_scale: 1.33205e-09f /* ty=float32 */
  out_scale: 1.33205e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1483])
  in_scale: 1.33205e-09f /* ty=float32 */
  out_scale: 0.0242511f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1489])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00525388f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1495])
  in_scale: 0.000127412f /* ty=float32 */
  out_scale: 1.40236e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1501])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.40236e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1507])
  in_scale: 1.40236e-09f /* ty=float32 */
  out_scale: 1.40237e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1513])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.40237e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1519])
  in_scale: 1.40237e-09f /* ty=float32 */
  out_scale: 1.40237e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1525])
  in_scale: 1.40237e-09f /* ty=float32 */
  out_scale: 0.0232779f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1531])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00649712f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1537])
  in_scale: 0.000151239f /* ty=float32 */
  out_scale: 2.51246e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1543])
  in_scale: 1f /* ty=float32 */
  out_scale: 2.51246e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1549])
  in_scale: 2.51246e-09f /* ty=float32 */
  out_scale: 2.51246e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1555])
  in_scale: 2.51246e-09f /* ty=float32 */
  out_scale: 0.0392821f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1561])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0086158f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1567])
  in_scale: 0.000338447f /* ty=float32 */
  out_scale: 1.10404e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1573])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.10404e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1579])
  in_scale: 1.10404e-09f /* ty=float32 */
  out_scale: 1.10407e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1585])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.10407e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1591])
  in_scale: 1.10407e-09f /* ty=float32 */
  out_scale: 1.44548e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1483])
  in_scale: 1.33205e-09f /* ty=float32 */
  out_scale: 1.44548e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1602])
  in_scale: 1.44548e-09f /* ty=float32 */
  out_scale: 1.44548e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1608])
  in_scale: 1.44548e-09f /* ty=float32 */
  out_scale: 0.0326502f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1614])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00517882f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1620])
  in_scale: 0.00016909f /* ty=float32 */
  out_scale: 1.36276e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1626])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.36276e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1632])
  in_scale: 1.36276e-09f /* ty=float32 */
  out_scale: 1.36276e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1638])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.36276e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1644])
  in_scale: 1.36276e-09f /* ty=float32 */
  out_scale: 1.36276e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1650])
  in_scale: 1.36276e-09f /* ty=float32 */
  out_scale: 0.0219498f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1656])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00370547f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1662])
  in_scale: 8.13346e-05f /* ty=float32 */
  out_scale: 1.0169e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1668])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.0169e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1674])
  in_scale: 1.0169e-09f /* ty=float32 */
  out_scale: 1.0169e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1680])
  in_scale: 1.0169e-09f /* ty=float32 */
  out_scale: 0.0171255f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1686])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00871211f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1692])
  in_scale: 0.000149199f /* ty=float32 */
  out_scale: 1.23962e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1698])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.23962e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1704])
  in_scale: 1.23962e-09f /* ty=float32 */
  out_scale: 1.23959e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1710])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.23959e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1716])
  in_scale: 1.23959e-09f /* ty=float32 */
  out_scale: 1.94611e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1608])
  in_scale: 1.44548e-09f /* ty=float32 */
  out_scale: 1.94611e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1727])
  in_scale: 1.94611e-09f /* ty=float32 */
  out_scale: 1.94611e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1733])
  in_scale: 1.94611e-09f /* ty=float32 */
  out_scale: 0.0328963f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1739])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00388161f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1745])
  in_scale: 0.000127691f /* ty=float32 */
  out_scale: 8.83036e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1751])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.83036e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1757])
  in_scale: 8.83036e-10f /* ty=float32 */
  out_scale: 8.83036e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1763])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.83036e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1769])
  in_scale: 8.83036e-10f /* ty=float32 */
  out_scale: 8.83036e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1775])
  in_scale: 8.83036e-10f /* ty=float32 */
  out_scale: 0.0150482f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1781])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00248865f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1787])
  in_scale: 3.74497e-05f /* ty=float32 */
  out_scale: 7.54132e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1793])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.54132e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1799])
  in_scale: 7.54132e-10f /* ty=float32 */
  out_scale: 7.54132e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1805])
  in_scale: 7.54132e-10f /* ty=float32 */
  out_scale: 0.0100588f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1811])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00920399f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1817])
  in_scale: 9.25815e-05f /* ty=float32 */
  out_scale: 6.46258e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1823])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.46258e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1829])
  in_scale: 6.46258e-10f /* ty=float32 */
  out_scale: 6.46258e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1835])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.46258e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1841])
  in_scale: 6.46258e-10f /* ty=float32 */
  out_scale: 8.71455e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1733])
  in_scale: 1.94611e-09f /* ty=float32 */
  out_scale: 0.0328963f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1852])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00403221f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1858])
  in_scale: 0.000132645f /* ty=float32 */
  out_scale: 9.29792e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1864])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.29792e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1870])
  in_scale: 9.29792e-10f /* ty=float32 */
  out_scale: 8.71455e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1876])
  in_scale: 8.71455e-10f /* ty=float32 */
  out_scale: 8.71455e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1882])
  in_scale: 8.71455e-10f /* ty=float32 */
  out_scale: 0.0152692f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1888])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0042901f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1894])
  in_scale: 6.55063e-05f /* ty=float32 */
  out_scale: 1.04501e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1900])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.04501e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1906])
  in_scale: 1.04501e-09f /* ty=float32 */
  out_scale: 1.04501e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1912])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.04501e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1918])
  in_scale: 1.04501e-09f /* ty=float32 */
  out_scale: 1.04501e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1924])
  in_scale: 1.04501e-09f /* ty=float32 */
  out_scale: 0.0176663f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1930])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00272741f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1936])
  in_scale: 4.81833e-05f /* ty=float32 */
  out_scale: 1.04836e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1942])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.04836e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1948])
  in_scale: 1.04836e-09f /* ty=float32 */
  out_scale: 1.04836e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1954])
  in_scale: 1.04836e-09f /* ty=float32 */
  out_scale: 0.0174499f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1960])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00738607f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1966])
  in_scale: 0.000128886f /* ty=float32 */
  out_scale: 7.79854e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1972])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.79854e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1978])
  in_scale: 7.79854e-10f /* ty=float32 */
  out_scale: 7.79854e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1984])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.79854e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1990])
  in_scale: 7.79854e-10f /* ty=float32 */
  out_scale: 9.10114e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1882])
  in_scale: 8.71455e-10f /* ty=float32 */
  out_scale: 9.10114e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2001])
  in_scale: 9.10114e-10f /* ty=float32 */
  out_scale: 9.10114e-10f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2007])
  in_scale: 9.10114e-10f /* ty=float32 */
  out_scale: 0.0207965f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2013])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00688873f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2019])
  in_scale: 0.000143261f /* ty=float32 */
  out_scale: 1.81667e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2025])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.81667e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2031])
  in_scale: 1.81667e-09f /* ty=float32 */
  out_scale: 1.81667e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2037])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.81667e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2043])
  in_scale: 1.81667e-09f /* ty=float32 */
  out_scale: 1.81667e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2049])
  in_scale: 1.81667e-09f /* ty=float32 */
  out_scale: 0.0285672f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2055])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.002343f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2061])
  in_scale: 6.69329e-05f /* ty=float32 */
  out_scale: 1.43354e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2067])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.43354e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2073])
  in_scale: 1.43354e-09f /* ty=float32 */
  out_scale: 1.43354e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2079])
  in_scale: 1.43354e-09f /* ty=float32 */
  out_scale: 0.0246551f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2085])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.166448f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2091])
  in_scale: 0.00410379f /* ty=float32 */
  out_scale: 1.45375e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2097])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.45375e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2103])
  in_scale: 1.45375e-08f /* ty=float32 */
  out_scale: 1.45375e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2109])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.45375e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2115])
  in_scale: 1.45375e-08f /* ty=float32 */
  out_scale: 1.46121e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2007])
  in_scale: 9.10114e-10f /* ty=float32 */
  out_scale: 1.46121e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2126])
  in_scale: 1.46121e-08f /* ty=float32 */
  out_scale: 1.46121e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2132])
  in_scale: 1.46121e-08f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.global_avg_pool2d[%2138])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.batch_flatten[%2144])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2150])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.dense[%2156])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2162])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2168])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
DEBUG:root:simulated graph
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.simulated_quantize(%data, 1f /* ty=float32 */, 0.0205693f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 3, 224, 224), float32] */;
  %1 = nn.simulated_quantize(meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, 1f /* ty=float32 */, 0.00324398f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %2 = nn.conv2d(%0, %1, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %3 = nn.simulated_quantize(%2, 6.67265e-05f /* ty=float32 */, 3.12663e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %4 = nn.simulated_quantize(meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 3.12663e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %5 = add(%3, %4) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %6 = nn.simulated_quantize(%5, 3.12663e-09f /* ty=float32 */, 3.12663e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %8 = nn.simulated_quantize(%7, 3.12663e-09f /* ty=float32 */, 3.23821e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 112, 112), float32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %10 = nn.simulated_quantize(%9, 3.23821e-09f /* ty=float32 */, 0.0543282f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %11 = nn.simulated_quantize(meta[relay.Constant][2] /* ty=Tensor[(64, 64, 1, 1), float32] */ /* ty=Tensor[(64, 64, 1, 1), float32] */, 1f /* ty=float32 */, 0.00609855f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %12 = nn.conv2d(%10, %11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %13 = nn.simulated_quantize(%12, 0.000331323f /* ty=float32 */, 2.95798e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %14 = nn.simulated_quantize(meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 2.95798e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %15 = add(%13, %14) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %16 = nn.simulated_quantize(%15, 2.95798e-09f /* ty=float32 */, 2.95797e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %17 = nn.simulated_quantize(meta[relay.Constant][4] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 2.95797e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %18 = add(%16, %17) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %19 = nn.simulated_quantize(%18, 2.95797e-09f /* ty=float32 */, 2.95797e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %20 = nn.relu(%19) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %21 = nn.simulated_quantize(%20, 2.95797e-09f /* ty=float32 */, 0.0182585f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %22 = nn.simulated_quantize(meta[relay.Constant][5] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00417648f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %23 = nn.conv2d(%21, %22, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %24 = nn.simulated_quantize(%23, 7.62562e-05f /* ty=float32 */, 1.85394e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %25 = nn.simulated_quantize(meta[relay.Constant][6] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 1.85394e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %26 = add(%24, %25) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %27 = nn.simulated_quantize(%26, 1.85394e-09f /* ty=float32 */, 1.85394e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %29 = nn.simulated_quantize(%28, 1.85394e-09f /* ty=float32 */, 0.0212445f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %30 = nn.simulated_quantize(meta[relay.Constant][7] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, 1f /* ty=float32 */, 0.0152203f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 64, 1, 1), float32] */;
  %31 = nn.conv2d(%29, %30, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %32 = nn.simulated_quantize(%31, 0.000323348f /* ty=float32 */, 1.84014e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %33 = nn.simulated_quantize(meta[relay.Constant][8] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.84014e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %34 = add(%32, %33) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %35 = nn.simulated_quantize(%34, 1.84014e-09f /* ty=float32 */, 1.84034e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %36 = nn.simulated_quantize(meta[relay.Constant][9] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.84034e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %37 = add(%35, %36) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %38 = nn.simulated_quantize(%37, 1.84034e-09f /* ty=float32 */, 2.05906e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %39 = nn.simulated_quantize(%9, 3.23821e-09f /* ty=float32 */, 0.0543282f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %40 = nn.simulated_quantize(meta[relay.Constant][10] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, 1f /* ty=float32 */, 0.0078972f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 64, 1, 1), float32] */;
  %41 = nn.conv2d(%39, %40, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %42 = nn.simulated_quantize(%41, 0.000429041f /* ty=float32 */, 2.19437e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %43 = nn.simulated_quantize(meta[relay.Constant][11] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 2.19437e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %44 = add(%42, %43) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %45 = nn.simulated_quantize(%44, 2.19437e-09f /* ty=float32 */, 2.05906e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %46 = add(%38, %45) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %47 = nn.simulated_quantize(%46, 2.05906e-09f /* ty=float32 */, 2.05906e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %48 = nn.relu(%47) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %49 = nn.simulated_quantize(%48, 2.05906e-09f /* ty=float32 */, 0.0253934f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %50 = nn.simulated_quantize(meta[relay.Constant][12] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00485266f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %51 = nn.conv2d(%49, %50, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %52 = nn.simulated_quantize(%51, 0.000123226f /* ty=float32 */, 1.31247e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %53 = nn.simulated_quantize(meta[relay.Constant][13] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 1.31247e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %55 = nn.simulated_quantize(%54, 1.31247e-09f /* ty=float32 */, 1.31245e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %56 = nn.simulated_quantize(meta[relay.Constant][14] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 1.31245e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %57 = add(%55, %56) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %58 = nn.simulated_quantize(%57, 1.31245e-09f /* ty=float32 */, 1.31245e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %59 = nn.relu(%58) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %60 = nn.simulated_quantize(%59, 1.31245e-09f /* ty=float32 */, 0.0168176f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %61 = nn.simulated_quantize(meta[relay.Constant][15] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00349414f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %62 = nn.conv2d(%60, %61, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %63 = nn.simulated_quantize(%62, 5.87631e-05f /* ty=float32 */, 1.35254e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %64 = nn.simulated_quantize(meta[relay.Constant][16] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 1.35254e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %65 = add(%63, %64) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %66 = nn.simulated_quantize(%65, 1.35254e-09f /* ty=float32 */, 1.35254e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %68 = nn.simulated_quantize(%67, 1.35254e-09f /* ty=float32 */, 0.022049f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %69 = nn.simulated_quantize(meta[relay.Constant][17] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, 1f /* ty=float32 */, 0.0148717f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 64, 1, 1), float32] */;
  %70 = nn.conv2d(%68, %69, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %71 = nn.simulated_quantize(%70, 0.000327906f /* ty=float32 */, 1.81313e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %72 = nn.simulated_quantize(meta[relay.Constant][18] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.81313e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %74 = nn.simulated_quantize(%73, 1.81313e-09f /* ty=float32 */, 1.81304e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %75 = nn.simulated_quantize(meta[relay.Constant][19] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.81304e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %76 = add(%74, %75) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %77 = nn.simulated_quantize(%76, 1.81304e-09f /* ty=float32 */, 1.71565e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %78 = nn.simulated_quantize(%48, 2.05906e-09f /* ty=float32 */, 1.71565e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %79 = add(%77, %78) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %80 = nn.simulated_quantize(%79, 1.71565e-09f /* ty=float32 */, 1.71565e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %81 = nn.relu(%80) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %82 = nn.simulated_quantize(%81, 1.71565e-09f /* ty=float32 */, 0.0276556f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %83 = nn.simulated_quantize(meta[relay.Constant][20] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00310491f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %84 = nn.conv2d(%82, %83, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %85 = nn.simulated_quantize(%84, 8.5868e-05f /* ty=float32 */, 9.23456e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %86 = nn.simulated_quantize(meta[relay.Constant][21] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 9.23456e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %87 = add(%85, %86) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %88 = nn.simulated_quantize(%87, 9.23456e-10f /* ty=float32 */, 9.23425e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %89 = nn.simulated_quantize(meta[relay.Constant][22] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 9.23425e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %90 = add(%88, %89) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %91 = nn.simulated_quantize(%90, 9.23425e-10f /* ty=float32 */, 9.23425e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %92 = nn.relu(%91) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %93 = nn.simulated_quantize(%92, 9.23425e-10f /* ty=float32 */, 0.0163706f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %94 = nn.simulated_quantize(meta[relay.Constant][23] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00350482f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %95 = nn.conv2d(%93, %94, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %96 = nn.simulated_quantize(%95, 5.7376e-05f /* ty=float32 */, 9.16049e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %97 = nn.simulated_quantize(meta[relay.Constant][24] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 9.16049e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %98 = add(%96, %97) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %99 = nn.simulated_quantize(%98, 9.16049e-10f /* ty=float32 */, 9.16049e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %100 = nn.relu(%99) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %101 = nn.simulated_quantize(%100, 9.16049e-10f /* ty=float32 */, 0.0161439f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 56, 56), float32] */;
  %102 = nn.simulated_quantize(meta[relay.Constant][25] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, 1f /* ty=float32 */, 0.0144349f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 64, 1, 1), float32] */;
  %103 = nn.conv2d(%101, %102, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %104 = nn.simulated_quantize(%103, 0.000233035f /* ty=float32 */, 1.54256e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %105 = nn.simulated_quantize(meta[relay.Constant][26] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.54256e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %106 = add(%104, %105) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %107 = nn.simulated_quantize(%106, 1.54256e-09f /* ty=float32 */, 1.54241e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %108 = nn.simulated_quantize(meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.54241e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %109 = add(%107, %108) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %110 = nn.simulated_quantize(%109, 1.54241e-09f /* ty=float32 */, 1.6484e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %111 = nn.simulated_quantize(%81, 1.71565e-09f /* ty=float32 */, 1.6484e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %112 = add(%110, %111) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %113 = nn.simulated_quantize(%112, 1.6484e-09f /* ty=float32 */, 1.6484e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %114 = nn.relu(%113) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %115 = nn.simulated_quantize(%114, 1.6484e-09f /* ty=float32 */, 0.0279189f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %116 = nn.simulated_quantize(meta[relay.Constant][28] /* ty=Tensor[(128, 256, 1, 1), float32] */ /* ty=Tensor[(128, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00286972f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 256, 1, 1), float32] */;
  %117 = nn.conv2d(%115, %116, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %118 = nn.simulated_quantize(%117, 8.01193e-05f /* ty=float32 */, 1.04587e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %119 = nn.simulated_quantize(meta[relay.Constant][29] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.04587e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %120 = add(%118, %119) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %121 = nn.simulated_quantize(%120, 1.04587e-09f /* ty=float32 */, 1.04587e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %122 = nn.simulated_quantize(meta[relay.Constant][30] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.04587e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %123 = add(%121, %122) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %124 = nn.simulated_quantize(%123, 1.04587e-09f /* ty=float32 */, 1.04587e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %125 = nn.relu(%124) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %126 = nn.simulated_quantize(%125, 1.04587e-09f /* ty=float32 */, 0.0142699f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %127 = nn.simulated_quantize(meta[relay.Constant][31] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 1f /* ty=float32 */, 0.0032833f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %129 = nn.simulated_quantize(%128, 4.68524e-05f /* ty=float32 */, 9.80875e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %130 = nn.simulated_quantize(meta[relay.Constant][32] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 9.80875e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %131 = add(%129, %130) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %132 = nn.simulated_quantize(%131, 9.80875e-10f /* ty=float32 */, 9.80875e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %133 = nn.relu(%132) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %134 = nn.simulated_quantize(%133, 9.80875e-10f /* ty=float32 */, 0.0146164f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %135 = nn.simulated_quantize(meta[relay.Constant][33] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, 1f /* ty=float32 */, 0.010646f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 128, 1, 1), float32] */;
  %136 = nn.conv2d(%134, %135, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %137 = nn.simulated_quantize(%136, 0.000155605f /* ty=float32 */, 1.29401e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %138 = nn.simulated_quantize(meta[relay.Constant][34] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.29401e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %139 = add(%137, %138) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %140 = nn.simulated_quantize(%139, 1.29401e-09f /* ty=float32 */, 1.29399e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %141 = nn.simulated_quantize(meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.29399e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %142 = add(%140, %141) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %143 = nn.simulated_quantize(%142, 1.29399e-09f /* ty=float32 */, 1.38908e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %144 = nn.simulated_quantize(%114, 1.6484e-09f /* ty=float32 */, 0.0279189f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 56, 56), float32] */;
  %145 = nn.simulated_quantize(meta[relay.Constant][36] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00767737f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %146 = nn.conv2d(%144, %145, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %147 = nn.simulated_quantize(%146, 0.000214344f /* ty=float32 */, 1.47811e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %148 = nn.simulated_quantize(meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.47811e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %149 = add(%147, %148) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %150 = nn.simulated_quantize(%149, 1.47811e-09f /* ty=float32 */, 1.38908e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %151 = add(%143, %150) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %152 = nn.simulated_quantize(%151, 1.38908e-09f /* ty=float32 */, 1.38908e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %153 = nn.relu(%152) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %154 = nn.simulated_quantize(%153, 1.38908e-09f /* ty=float32 */, 0.0242996f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %155 = nn.simulated_quantize(meta[relay.Constant][38] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, 1f /* ty=float32 */, 0.00448611f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 512, 1, 1), float32] */;
  %156 = nn.conv2d(%154, %155, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %157 = nn.simulated_quantize(%156, 0.000109011f /* ty=float32 */, 1.17276e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %158 = nn.simulated_quantize(meta[relay.Constant][39] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.17276e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %159 = add(%157, %158) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %160 = nn.simulated_quantize(%159, 1.17276e-09f /* ty=float32 */, 1.17277e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %161 = nn.simulated_quantize(meta[relay.Constant][40] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.17277e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %162 = add(%160, %161) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %163 = nn.simulated_quantize(%162, 1.17277e-09f /* ty=float32 */, 1.17277e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %164 = nn.relu(%163) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %165 = nn.simulated_quantize(%164, 1.17277e-09f /* ty=float32 */, 0.0137866f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %166 = nn.simulated_quantize(meta[relay.Constant][41] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 1f /* ty=float32 */, 0.00369772f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %167 = nn.conv2d(%165, %166, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %168 = nn.simulated_quantize(%167, 5.0979e-05f /* ty=float32 */, 9.18822e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %169 = nn.simulated_quantize(meta[relay.Constant][42] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 9.18822e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %170 = add(%168, %169) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %171 = nn.simulated_quantize(%170, 9.18822e-10f /* ty=float32 */, 9.18822e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %173 = nn.simulated_quantize(%172, 9.18822e-10f /* ty=float32 */, 0.0118365f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %174 = nn.simulated_quantize(meta[relay.Constant][43] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, 1f /* ty=float32 */, 0.00891572f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 128, 1, 1), float32] */;
  %175 = nn.conv2d(%173, %174, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %176 = nn.simulated_quantize(%175, 0.000105531f /* ty=float32 */, 9.71687e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %177 = nn.simulated_quantize(meta[relay.Constant][44] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 9.71687e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %178 = add(%176, %177) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %179 = nn.simulated_quantize(%178, 9.71687e-10f /* ty=float32 */, 9.71731e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %180 = nn.simulated_quantize(meta[relay.Constant][45] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 9.71731e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %181 = add(%179, %180) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %182 = nn.simulated_quantize(%181, 9.71731e-10f /* ty=float32 */, 1.44837e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %183 = nn.simulated_quantize(%153, 1.38908e-09f /* ty=float32 */, 1.44837e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %184 = add(%182, %183) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %185 = nn.simulated_quantize(%184, 1.44837e-09f /* ty=float32 */, 1.44837e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %186 = nn.relu(%185) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %187 = nn.simulated_quantize(%186, 1.44837e-09f /* ty=float32 */, 0.0244258f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %188 = nn.simulated_quantize(meta[relay.Constant][46] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, 1f /* ty=float32 */, 0.00447191f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 512, 1, 1), float32] */;
  %189 = nn.conv2d(%187, %188, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %190 = nn.simulated_quantize(%189, 0.00010923f /* ty=float32 */, 1.2403e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %191 = nn.simulated_quantize(meta[relay.Constant][47] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.2403e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %192 = add(%190, %191) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %193 = nn.simulated_quantize(%192, 1.2403e-09f /* ty=float32 */, 1.24031e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %194 = nn.simulated_quantize(meta[relay.Constant][48] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.24031e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %195 = add(%193, %194) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %196 = nn.simulated_quantize(%195, 1.24031e-09f /* ty=float32 */, 1.24031e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %197 = nn.relu(%196) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %198 = nn.simulated_quantize(%197, 1.24031e-09f /* ty=float32 */, 0.0204046f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %199 = nn.simulated_quantize(meta[relay.Constant][49] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 1f /* ty=float32 */, 0.00388262f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %200 = nn.conv2d(%198, %199, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %201 = nn.simulated_quantize(%200, 7.92233e-05f /* ty=float32 */, 1.07658e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %202 = nn.simulated_quantize(meta[relay.Constant][50] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.07658e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %203 = add(%201, %202) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %204 = nn.simulated_quantize(%203, 1.07658e-09f /* ty=float32 */, 1.07658e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %206 = nn.simulated_quantize(%205, 1.07658e-09f /* ty=float32 */, 0.0183218f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %207 = nn.simulated_quantize(meta[relay.Constant][51] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, 1f /* ty=float32 */, 0.00841142f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 128, 1, 1), float32] */;
  %208 = nn.conv2d(%206, %207, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %209 = nn.simulated_quantize(%208, 0.000154112f /* ty=float32 */, 9.94988e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %210 = nn.simulated_quantize(meta[relay.Constant][52] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 9.94988e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %211 = add(%209, %210) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %212 = nn.simulated_quantize(%211, 9.94988e-10f /* ty=float32 */, 9.95019e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %213 = nn.simulated_quantize(meta[relay.Constant][53] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 9.95019e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %214 = add(%212, %213) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %215 = nn.simulated_quantize(%214, 9.95019e-10f /* ty=float32 */, 1.45589e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %216 = nn.simulated_quantize(%186, 1.44837e-09f /* ty=float32 */, 1.45589e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %217 = add(%215, %216) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %218 = nn.simulated_quantize(%217, 1.45589e-09f /* ty=float32 */, 1.45589e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %219 = nn.relu(%218) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %220 = nn.simulated_quantize(%219, 1.45589e-09f /* ty=float32 */, 0.026306f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %221 = nn.simulated_quantize(meta[relay.Constant][54] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, 1f /* ty=float32 */, 0.00394898f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 512, 1, 1), float32] */;
  %222 = nn.conv2d(%220, %221, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %223 = nn.simulated_quantize(%222, 0.000103882f /* ty=float32 */, 2.02124e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %224 = nn.simulated_quantize(meta[relay.Constant][55] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 2.02124e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %225 = add(%223, %224) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %226 = nn.simulated_quantize(%225, 2.02124e-09f /* ty=float32 */, 2.02125e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %227 = nn.simulated_quantize(meta[relay.Constant][56] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 2.02125e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %228 = add(%226, %227) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %229 = nn.simulated_quantize(%228, 2.02125e-09f /* ty=float32 */, 2.02125e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %230 = nn.relu(%229) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %231 = nn.simulated_quantize(%230, 2.02125e-09f /* ty=float32 */, 0.0316616f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %232 = nn.simulated_quantize(meta[relay.Constant][57] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 1f /* ty=float32 */, 0.00302293f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %233 = nn.conv2d(%231, %232, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %234 = nn.simulated_quantize(%233, 9.57109e-05f /* ty=float32 */, 1.14735e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %235 = nn.simulated_quantize(meta[relay.Constant][58] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 1.14735e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %236 = add(%234, %235) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %237 = nn.simulated_quantize(%236, 1.14735e-09f /* ty=float32 */, 1.14735e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %238 = nn.relu(%237) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %239 = nn.simulated_quantize(%238, 1.14735e-09f /* ty=float32 */, 0.0194376f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 28, 28), float32] */;
  %240 = nn.simulated_quantize(meta[relay.Constant][59] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, 1f /* ty=float32 */, 0.0101345f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 128, 1, 1), float32] */;
  %241 = nn.conv2d(%239, %240, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %242 = nn.simulated_quantize(%241, 0.00019699f /* ty=float32 */, 1.20215e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %243 = nn.simulated_quantize(meta[relay.Constant][60] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.20215e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %244 = add(%242, %243) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %245 = nn.simulated_quantize(%244, 1.20215e-09f /* ty=float32 */, 1.20218e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %246 = nn.simulated_quantize(meta[relay.Constant][61] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.20218e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %247 = add(%245, %246) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %248 = nn.simulated_quantize(%247, 1.20218e-09f /* ty=float32 */, 1.56796e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %249 = nn.simulated_quantize(%219, 1.45589e-09f /* ty=float32 */, 1.56796e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %250 = add(%248, %249) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %251 = nn.simulated_quantize(%250, 1.56796e-09f /* ty=float32 */, 1.56796e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %252 = nn.relu(%251) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %253 = nn.simulated_quantize(%252, 1.56796e-09f /* ty=float32 */, 0.031925f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %254 = nn.simulated_quantize(meta[relay.Constant][62] /* ty=Tensor[(256, 512, 1, 1), float32] */ /* ty=Tensor[(256, 512, 1, 1), float32] */, 1f /* ty=float32 */, 0.00359078f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 512, 1, 1), float32] */;
  %255 = nn.conv2d(%253, %254, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %256 = nn.simulated_quantize(%255, 0.000114636f /* ty=float32 */, 7.10539e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %257 = nn.simulated_quantize(meta[relay.Constant][63] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 7.10539e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %258 = add(%256, %257) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %259 = nn.simulated_quantize(%258, 7.10539e-10f /* ty=float32 */, 7.10542e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %260 = nn.simulated_quantize(meta[relay.Constant][64] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 7.10542e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %261 = add(%259, %260) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %262 = nn.simulated_quantize(%261, 7.10542e-10f /* ty=float32 */, 7.10542e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %263 = nn.relu(%262) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %264 = nn.simulated_quantize(%263, 7.10542e-10f /* ty=float32 */, 0.0114107f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %265 = nn.simulated_quantize(meta[relay.Constant][65] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00237224f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %266 = nn.conv2d(%264, %265, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %267 = nn.simulated_quantize(%266, 2.7069e-05f /* ty=float32 */, 1.02309e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %268 = nn.simulated_quantize(meta[relay.Constant][66] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.02309e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %269 = add(%267, %268) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %270 = nn.simulated_quantize(%269, 1.02309e-09f /* ty=float32 */, 1.02309e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %271 = nn.relu(%270) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %272 = nn.simulated_quantize(%271, 1.02309e-09f /* ty=float32 */, 0.0111092f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %273 = nn.simulated_quantize(meta[relay.Constant][67] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.0111844f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %274 = nn.conv2d(%272, %273, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %275 = nn.simulated_quantize(%274, 0.00012425f /* ty=float32 */, 9.66816e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %276 = nn.simulated_quantize(meta[relay.Constant][68] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 9.66816e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %277 = add(%275, %276) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %278 = nn.simulated_quantize(%277, 9.66816e-10f /* ty=float32 */, 9.66801e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %279 = nn.simulated_quantize(meta[relay.Constant][69] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 9.66801e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %280 = add(%278, %279) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %281 = nn.simulated_quantize(%280, 9.66801e-10f /* ty=float32 */, 1.20767e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %282 = nn.simulated_quantize(%252, 1.56796e-09f /* ty=float32 */, 0.031925f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 28, 28), float32] */;
  %283 = nn.simulated_quantize(meta[relay.Constant][70] /* ty=Tensor[(1024, 512, 1, 1), float32] */ /* ty=Tensor[(1024, 512, 1, 1), float32] */, 1f /* ty=float32 */, 0.00463721f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 512, 1, 1), float32] */;
  %284 = nn.conv2d(%282, %283, strides=[2, 2], padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %285 = nn.simulated_quantize(%284, 0.000148043f /* ty=float32 */, 1.24197e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %286 = nn.simulated_quantize(meta[relay.Constant][71] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 1.24197e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %287 = add(%285, %286) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %288 = nn.simulated_quantize(%287, 1.24197e-09f /* ty=float32 */, 1.20767e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %289 = add(%281, %288) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %290 = nn.simulated_quantize(%289, 1.20767e-09f /* ty=float32 */, 1.20767e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %291 = nn.relu(%290) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %292 = nn.simulated_quantize(%291, 1.20767e-09f /* ty=float32 */, 0.0236786f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %293 = nn.simulated_quantize(meta[relay.Constant][72] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 1f /* ty=float32 */, 0.00292385f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %294 = nn.conv2d(%292, %293, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %295 = nn.simulated_quantize(%294, 6.92327e-05f /* ty=float32 */, 9.06848e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %296 = nn.simulated_quantize(meta[relay.Constant][73] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 9.06848e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %297 = add(%295, %296) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %298 = nn.simulated_quantize(%297, 9.06848e-10f /* ty=float32 */, 9.06852e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %299 = nn.simulated_quantize(meta[relay.Constant][74] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 9.06852e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %300 = add(%298, %299) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %301 = nn.simulated_quantize(%300, 9.06852e-10f /* ty=float32 */, 9.06852e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %302 = nn.relu(%301) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %303 = nn.simulated_quantize(%302, 9.06852e-10f /* ty=float32 */, 0.0132926f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %304 = nn.simulated_quantize(meta[relay.Constant][75] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00261371f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %305 = nn.conv2d(%303, %304, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %306 = nn.simulated_quantize(%305, 3.4743e-05f /* ty=float32 */, 8.57371e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %307 = nn.simulated_quantize(meta[relay.Constant][76] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 8.57371e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %308 = add(%306, %307) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %309 = nn.simulated_quantize(%308, 8.57371e-10f /* ty=float32 */, 8.57371e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %310 = nn.relu(%309) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %311 = nn.simulated_quantize(%310, 8.57371e-10f /* ty=float32 */, 0.0128749f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %312 = nn.simulated_quantize(meta[relay.Constant][77] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00848259f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %313 = nn.conv2d(%311, %312, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %314 = nn.simulated_quantize(%313, 0.000109212f /* ty=float32 */, 7.32045e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %315 = nn.simulated_quantize(meta[relay.Constant][78] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 7.32045e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %316 = add(%314, %315) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %317 = nn.simulated_quantize(%316, 7.32045e-10f /* ty=float32 */, 7.32014e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %318 = nn.simulated_quantize(meta[relay.Constant][79] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 7.32014e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %319 = add(%317, %318) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %320 = nn.simulated_quantize(%319, 7.32014e-10f /* ty=float32 */, 1.41135e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %321 = nn.simulated_quantize(%291, 1.20767e-09f /* ty=float32 */, 1.41135e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %322 = add(%320, %321) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %323 = nn.simulated_quantize(%322, 1.41135e-09f /* ty=float32 */, 1.41135e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %324 = nn.relu(%323) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %325 = nn.simulated_quantize(%324, 1.41135e-09f /* ty=float32 */, 0.0217418f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %326 = nn.simulated_quantize(meta[relay.Constant][80] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 1f /* ty=float32 */, 0.00378073f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %327 = nn.conv2d(%325, %326, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %328 = nn.simulated_quantize(%327, 8.21998e-05f /* ty=float32 */, 9.28803e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %329 = nn.simulated_quantize(meta[relay.Constant][81] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 9.28803e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %330 = add(%328, %329) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %331 = nn.simulated_quantize(%330, 9.28803e-10f /* ty=float32 */, 9.28807e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %332 = nn.simulated_quantize(meta[relay.Constant][82] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 9.28807e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %333 = add(%331, %332) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %334 = nn.simulated_quantize(%333, 9.28807e-10f /* ty=float32 */, 9.28807e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %335 = nn.relu(%334) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %336 = nn.simulated_quantize(%335, 9.28807e-10f /* ty=float32 */, 0.0143021f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %337 = nn.simulated_quantize(meta[relay.Constant][83] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00288576f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %338 = nn.conv2d(%336, %337, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %339 = nn.simulated_quantize(%338, 4.12723e-05f /* ty=float32 */, 8.60461e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %340 = nn.simulated_quantize(meta[relay.Constant][84] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 8.60461e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %341 = add(%339, %340) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %342 = nn.simulated_quantize(%341, 8.60461e-10f /* ty=float32 */, 8.60461e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %343 = nn.relu(%342) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %344 = nn.simulated_quantize(%343, 8.60461e-10f /* ty=float32 */, 0.0145492f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %345 = nn.simulated_quantize(meta[relay.Constant][85] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.0124584f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %346 = nn.conv2d(%344, %345, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %347 = nn.simulated_quantize(%346, 0.000181259f /* ty=float32 */, 1.07839e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %348 = nn.simulated_quantize(meta[relay.Constant][86] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 1.07839e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %349 = add(%347, %348) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %350 = nn.simulated_quantize(%349, 1.07839e-09f /* ty=float32 */, 1.07843e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %351 = nn.simulated_quantize(meta[relay.Constant][87] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 1.07843e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %352 = add(%350, %351) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %353 = nn.simulated_quantize(%352, 1.07843e-09f /* ty=float32 */, 1.29591e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %354 = nn.simulated_quantize(%324, 1.41135e-09f /* ty=float32 */, 1.29591e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %355 = add(%353, %354) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %356 = nn.simulated_quantize(%355, 1.29591e-09f /* ty=float32 */, 1.29591e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %357 = nn.relu(%356) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %358 = nn.simulated_quantize(%357, 1.29591e-09f /* ty=float32 */, 0.022348f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %359 = nn.simulated_quantize(meta[relay.Constant][88] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 1f /* ty=float32 */, 0.00437263f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %360 = nn.conv2d(%358, %359, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %361 = nn.simulated_quantize(%360, 9.77198e-05f /* ty=float32 */, 1.12619e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %362 = nn.simulated_quantize(meta[relay.Constant][89] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.12619e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %363 = add(%361, %362) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %364 = nn.simulated_quantize(%363, 1.12619e-09f /* ty=float32 */, 1.1262e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %365 = nn.simulated_quantize(meta[relay.Constant][90] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.1262e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %366 = add(%364, %365) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %367 = nn.simulated_quantize(%366, 1.1262e-09f /* ty=float32 */, 1.1262e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %368 = nn.relu(%367) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %369 = nn.simulated_quantize(%368, 1.1262e-09f /* ty=float32 */, 0.0188223f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %370 = nn.simulated_quantize(meta[relay.Constant][91] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00424186f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %371 = nn.conv2d(%369, %370, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %372 = nn.simulated_quantize(%371, 7.98413e-05f /* ty=float32 */, 1.03312e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %373 = nn.simulated_quantize(meta[relay.Constant][92] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.03312e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %374 = add(%372, %373) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %375 = nn.simulated_quantize(%374, 1.03312e-09f /* ty=float32 */, 1.03312e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %376 = nn.relu(%375) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %377 = nn.simulated_quantize(%376, 1.03312e-09f /* ty=float32 */, 0.0157012f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %378 = nn.simulated_quantize(meta[relay.Constant][93] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00946433f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %379 = nn.conv2d(%377, %378, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %380 = nn.simulated_quantize(%379, 0.000148601f /* ty=float32 */, 1.1081e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %381 = nn.simulated_quantize(meta[relay.Constant][94] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 1.1081e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %382 = add(%380, %381) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %383 = nn.simulated_quantize(%382, 1.1081e-09f /* ty=float32 */, 1.10813e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %384 = nn.simulated_quantize(meta[relay.Constant][95] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 1.10813e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %385 = add(%383, %384) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %386 = nn.simulated_quantize(%385, 1.10813e-09f /* ty=float32 */, 1.33205e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %387 = nn.simulated_quantize(%357, 1.29591e-09f /* ty=float32 */, 1.33205e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %388 = add(%386, %387) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %389 = nn.simulated_quantize(%388, 1.33205e-09f /* ty=float32 */, 1.33205e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %390 = nn.relu(%389) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %391 = nn.simulated_quantize(%390, 1.33205e-09f /* ty=float32 */, 0.0242511f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %392 = nn.simulated_quantize(meta[relay.Constant][96] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 1f /* ty=float32 */, 0.00525388f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %393 = nn.conv2d(%391, %392, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %394 = nn.simulated_quantize(%393, 0.000127412f /* ty=float32 */, 1.40236e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %395 = nn.simulated_quantize(meta[relay.Constant][97] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.40236e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %396 = add(%394, %395) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %397 = nn.simulated_quantize(%396, 1.40236e-09f /* ty=float32 */, 1.40237e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %398 = nn.simulated_quantize(meta[relay.Constant][98] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.40237e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %399 = add(%397, %398) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %400 = nn.simulated_quantize(%399, 1.40237e-09f /* ty=float32 */, 1.40237e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %401 = nn.relu(%400) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %402 = nn.simulated_quantize(%401, 1.40237e-09f /* ty=float32 */, 0.0232779f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %403 = nn.simulated_quantize(meta[relay.Constant][99] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00649712f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %404 = nn.conv2d(%402, %403, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %405 = nn.simulated_quantize(%404, 0.000151239f /* ty=float32 */, 2.51246e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %406 = nn.simulated_quantize(meta[relay.Constant][100] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 2.51246e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %407 = add(%405, %406) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %408 = nn.simulated_quantize(%407, 2.51246e-09f /* ty=float32 */, 2.51246e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %409 = nn.relu(%408) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %410 = nn.simulated_quantize(%409, 2.51246e-09f /* ty=float32 */, 0.0392821f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %411 = nn.simulated_quantize(meta[relay.Constant][101] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.0086158f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %412 = nn.conv2d(%410, %411, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %413 = nn.simulated_quantize(%412, 0.000338447f /* ty=float32 */, 1.10404e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %414 = nn.simulated_quantize(meta[relay.Constant][102] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 1.10404e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %415 = add(%413, %414) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %416 = nn.simulated_quantize(%415, 1.10404e-09f /* ty=float32 */, 1.10407e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %417 = nn.simulated_quantize(meta[relay.Constant][103] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 1.10407e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %418 = add(%416, %417) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %419 = nn.simulated_quantize(%418, 1.10407e-09f /* ty=float32 */, 1.44548e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %420 = nn.simulated_quantize(%390, 1.33205e-09f /* ty=float32 */, 1.44548e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %421 = add(%419, %420) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %422 = nn.simulated_quantize(%421, 1.44548e-09f /* ty=float32 */, 1.44548e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %423 = nn.relu(%422) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %424 = nn.simulated_quantize(%423, 1.44548e-09f /* ty=float32 */, 0.0326502f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %425 = nn.simulated_quantize(meta[relay.Constant][104] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 1f /* ty=float32 */, 0.00517882f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 1024, 1, 1), float32] */;
  %426 = nn.conv2d(%424, %425, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %427 = nn.simulated_quantize(%426, 0.00016909f /* ty=float32 */, 1.36276e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %428 = nn.simulated_quantize(meta[relay.Constant][105] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.36276e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %429 = add(%427, %428) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %430 = nn.simulated_quantize(%429, 1.36276e-09f /* ty=float32 */, 1.36276e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %431 = nn.simulated_quantize(meta[relay.Constant][106] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.36276e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %432 = add(%430, %431) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %433 = nn.simulated_quantize(%432, 1.36276e-09f /* ty=float32 */, 1.36276e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %434 = nn.relu(%433) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %435 = nn.simulated_quantize(%434, 1.36276e-09f /* ty=float32 */, 0.0219498f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %436 = nn.simulated_quantize(meta[relay.Constant][107] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 1f /* ty=float32 */, 0.00370547f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %437 = nn.conv2d(%435, %436, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %438 = nn.simulated_quantize(%437, 8.13346e-05f /* ty=float32 */, 1.0169e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %439 = nn.simulated_quantize(meta[relay.Constant][108] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1f /* ty=float32 */, 1.0169e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(256, 1, 1), float32] */;
  %440 = add(%438, %439) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %441 = nn.simulated_quantize(%440, 1.0169e-09f /* ty=float32 */, 1.0169e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %442 = nn.relu(%441) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %443 = nn.simulated_quantize(%442, 1.0169e-09f /* ty=float32 */, 0.0171255f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 14, 14), float32] */;
  %444 = nn.simulated_quantize(meta[relay.Constant][109] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00871211f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(1024, 256, 1, 1), float32] */;
  %445 = nn.conv2d(%443, %444, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1]) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %446 = nn.simulated_quantize(%445, 0.000149199f /* ty=float32 */, 1.23962e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %447 = nn.simulated_quantize(meta[relay.Constant][110] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 1.23962e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %448 = add(%446, %447) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %449 = nn.simulated_quantize(%448, 1.23962e-09f /* ty=float32 */, 1.23959e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %450 = nn.simulated_quantize(meta[relay.Constant][111] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1f /* ty=float32 */, 1.23959e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(1024, 1, 1), float32] */;
  %451 = add(%449, %450) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %452 = nn.simulated_quantize(%451, 1.23959e-09f /* ty=float32 */, 1.94611e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %453 = nn.simulated_quantize(%423, 1.44548e-09f /* ty=float32 */, 1.94611e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %454 = add(%452, %453) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %455 = nn.simulated_quantize(%454, 1.94611e-09f /* ty=float32 */, 1.94611e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %456 = nn.relu(%455) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %457 = nn.simulated_quantize(%456, 1.94611e-09f /* ty=float32 */, 0.0328963f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %458 = nn.simulated_quantize(meta[relay.Constant][112] /* ty=Tensor[(512, 1024, 1, 1), float32] */ /* ty=Tensor[(512, 1024, 1, 1), float32] */, 1f /* ty=float32 */, 0.00388161f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 1024, 1, 1), float32] */;
  %459 = nn.conv2d(%457, %458, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %460 = nn.simulated_quantize(%459, 0.000127691f /* ty=float32 */, 8.83036e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %461 = nn.simulated_quantize(meta[relay.Constant][113] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 8.83036e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %462 = add(%460, %461) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %463 = nn.simulated_quantize(%462, 8.83036e-10f /* ty=float32 */, 8.83036e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %464 = nn.simulated_quantize(meta[relay.Constant][114] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 8.83036e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %465 = add(%463, %464) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %466 = nn.simulated_quantize(%465, 8.83036e-10f /* ty=float32 */, 8.83036e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %467 = nn.relu(%466) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %468 = nn.simulated_quantize(%467, 8.83036e-10f /* ty=float32 */, 0.0150482f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %469 = nn.simulated_quantize(meta[relay.Constant][115] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 1f /* ty=float32 */, 0.00248865f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %470 = nn.conv2d(%468, %469, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %471 = nn.simulated_quantize(%470, 3.74497e-05f /* ty=float32 */, 7.54132e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %472 = nn.simulated_quantize(meta[relay.Constant][116] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 7.54132e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %473 = add(%471, %472) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %474 = nn.simulated_quantize(%473, 7.54132e-10f /* ty=float32 */, 7.54132e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %475 = nn.relu(%474) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %476 = nn.simulated_quantize(%475, 7.54132e-10f /* ty=float32 */, 0.0100588f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %477 = nn.simulated_quantize(meta[relay.Constant][117] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, 1f /* ty=float32 */, 0.00920399f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(2048, 512, 1, 1), float32] */;
  %478 = nn.conv2d(%476, %477, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %479 = nn.simulated_quantize(%478, 9.25815e-05f /* ty=float32 */, 6.46258e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %480 = nn.simulated_quantize(meta[relay.Constant][118] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 1f /* ty=float32 */, 6.46258e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %481 = add(%479, %480) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %482 = nn.simulated_quantize(%481, 6.46258e-10f /* ty=float32 */, 6.46258e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %483 = nn.simulated_quantize(meta[relay.Constant][119] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 1f /* ty=float32 */, 6.46258e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %484 = add(%482, %483) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %485 = nn.simulated_quantize(%484, 6.46258e-10f /* ty=float32 */, 8.71455e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %486 = nn.simulated_quantize(%456, 1.94611e-09f /* ty=float32 */, 0.0328963f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1024, 14, 14), float32] */;
  %487 = nn.simulated_quantize(meta[relay.Constant][120] /* ty=Tensor[(2048, 1024, 1, 1), float32] */ /* ty=Tensor[(2048, 1024, 1, 1), float32] */, 1f /* ty=float32 */, 0.00403221f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(2048, 1024, 1, 1), float32] */;
  %488 = nn.conv2d(%486, %487, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %489 = nn.simulated_quantize(%488, 0.000132645f /* ty=float32 */, 9.29792e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %490 = nn.simulated_quantize(meta[relay.Constant][121] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 1f /* ty=float32 */, 9.29792e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %491 = add(%489, %490) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %492 = nn.simulated_quantize(%491, 9.29792e-10f /* ty=float32 */, 8.71455e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %493 = add(%485, %492) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %494 = nn.simulated_quantize(%493, 8.71455e-10f /* ty=float32 */, 8.71455e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %495 = nn.relu(%494) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %496 = nn.simulated_quantize(%495, 8.71455e-10f /* ty=float32 */, 0.0152692f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %497 = nn.simulated_quantize(meta[relay.Constant][122] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, 1f /* ty=float32 */, 0.0042901f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 2048, 1, 1), float32] */;
  %498 = nn.conv2d(%496, %497, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %499 = nn.simulated_quantize(%498, 6.55063e-05f /* ty=float32 */, 1.04501e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %500 = nn.simulated_quantize(meta[relay.Constant][123] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.04501e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %501 = add(%499, %500) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %502 = nn.simulated_quantize(%501, 1.04501e-09f /* ty=float32 */, 1.04501e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %503 = nn.simulated_quantize(meta[relay.Constant][124] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.04501e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %504 = add(%502, %503) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %505 = nn.simulated_quantize(%504, 1.04501e-09f /* ty=float32 */, 1.04501e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %506 = nn.relu(%505) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %507 = nn.simulated_quantize(%506, 1.04501e-09f /* ty=float32 */, 0.0176663f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %508 = nn.simulated_quantize(meta[relay.Constant][125] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 1f /* ty=float32 */, 0.00272741f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %509 = nn.conv2d(%507, %508, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %510 = nn.simulated_quantize(%509, 4.81833e-05f /* ty=float32 */, 1.04836e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %511 = nn.simulated_quantize(meta[relay.Constant][126] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.04836e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %512 = add(%510, %511) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %513 = nn.simulated_quantize(%512, 1.04836e-09f /* ty=float32 */, 1.04836e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %514 = nn.relu(%513) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %515 = nn.simulated_quantize(%514, 1.04836e-09f /* ty=float32 */, 0.0174499f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %516 = nn.simulated_quantize(meta[relay.Constant][127] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, 1f /* ty=float32 */, 0.00738607f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(2048, 512, 1, 1), float32] */;
  %517 = nn.conv2d(%515, %516, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %518 = nn.simulated_quantize(%517, 0.000128886f /* ty=float32 */, 7.79854e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %519 = nn.simulated_quantize(meta[relay.Constant][128] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 1f /* ty=float32 */, 7.79854e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %520 = add(%518, %519) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %521 = nn.simulated_quantize(%520, 7.79854e-10f /* ty=float32 */, 7.79854e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %522 = nn.simulated_quantize(meta[relay.Constant][129] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 1f /* ty=float32 */, 7.79854e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %523 = add(%521, %522) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %524 = nn.simulated_quantize(%523, 7.79854e-10f /* ty=float32 */, 9.10114e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %525 = nn.simulated_quantize(%495, 8.71455e-10f /* ty=float32 */, 9.10114e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %526 = add(%524, %525) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %527 = nn.simulated_quantize(%526, 9.10114e-10f /* ty=float32 */, 9.10114e-10f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %528 = nn.relu(%527) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %529 = nn.simulated_quantize(%528, 9.10114e-10f /* ty=float32 */, 0.0207965f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %530 = nn.simulated_quantize(meta[relay.Constant][130] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, 1f /* ty=float32 */, 0.00688873f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 2048, 1, 1), float32] */;
  %531 = nn.conv2d(%529, %530, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %532 = nn.simulated_quantize(%531, 0.000143261f /* ty=float32 */, 1.81667e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %533 = nn.simulated_quantize(meta[relay.Constant][131] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.81667e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %534 = add(%532, %533) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %535 = nn.simulated_quantize(%534, 1.81667e-09f /* ty=float32 */, 1.81667e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %536 = nn.simulated_quantize(meta[relay.Constant][132] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.81667e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %537 = add(%535, %536) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %538 = nn.simulated_quantize(%537, 1.81667e-09f /* ty=float32 */, 1.81667e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %539 = nn.relu(%538) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %540 = nn.simulated_quantize(%539, 1.81667e-09f /* ty=float32 */, 0.0285672f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %541 = nn.simulated_quantize(meta[relay.Constant][133] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 1f /* ty=float32 */, 0.002343f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %542 = nn.conv2d(%540, %541, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %543 = nn.simulated_quantize(%542, 6.69329e-05f /* ty=float32 */, 1.43354e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %544 = nn.simulated_quantize(meta[relay.Constant][134] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1f /* ty=float32 */, 1.43354e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(512, 1, 1), float32] */;
  %545 = add(%543, %544) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %546 = nn.simulated_quantize(%545, 1.43354e-09f /* ty=float32 */, 1.43354e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %547 = nn.relu(%546) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %548 = nn.simulated_quantize(%547, 1.43354e-09f /* ty=float32 */, 0.0246551f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 512, 7, 7), float32] */;
  %549 = nn.simulated_quantize(meta[relay.Constant][135] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, 1f /* ty=float32 */, 0.166448f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(2048, 512, 1, 1), float32] */;
  %550 = nn.conv2d(%548, %549, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1]) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %551 = nn.simulated_quantize(%550, 0.00410379f /* ty=float32 */, 1.45375e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %552 = nn.simulated_quantize(meta[relay.Constant][136] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 1f /* ty=float32 */, 1.45375e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %553 = add(%551, %552) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %554 = nn.simulated_quantize(%553, 1.45375e-08f /* ty=float32 */, 1.45375e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %555 = nn.simulated_quantize(meta[relay.Constant][137] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 1f /* ty=float32 */, 1.45375e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(2048, 1, 1), float32] */;
  %556 = add(%554, %555) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %557 = nn.simulated_quantize(%556, 1.45375e-08f /* ty=float32 */, 1.46121e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %558 = nn.simulated_quantize(%528, 9.10114e-10f /* ty=float32 */, 1.46121e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %559 = add(%557, %558) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %560 = nn.simulated_quantize(%559, 1.46121e-08f /* ty=float32 */, 1.46121e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %561 = nn.relu(%560) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %562 = nn.simulated_quantize(%561, 1.46121e-08f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %563 = nn.global_avg_pool2d(%562) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %564 = nn.simulated_quantize(%563, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %565 = nn.batch_flatten(%564) /* ty=Tensor[(32, 2048), float32] */;
  %566 = nn.simulated_quantize(%565, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048), float32] */;
  %567 = nn.simulated_quantize(meta[relay.Constant][138] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000, 2048), float32] */;
  %568 = nn.dense(%566, %567, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  %569 = nn.simulated_quantize(%568, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */;
  %570 = nn.simulated_quantize(meta[relay.Constant][139] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000), float32] */;
  %571 = add(%569, %570) /* ty=Tensor[(32, 1000), float32] */;
  nn.simulated_quantize(%571, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:root:quantize graph
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 224, 224), float32]) -> Tensor[(32, 1000), float32] {
  %0 = qnn.quantize(%data, 0.0205693f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 3, 224, 224), int8] */;
  %1 = qnn.quantize(meta[relay.Constant][0] /* ty=Tensor[(64, 3, 7, 7), float32] */ /* ty=Tensor[(64, 3, 7, 7), float32] */, 0.00324398f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 3, 7, 7), int8] */;
  %2 = nn.conv2d(%0, %1, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7], out_dtype="int32") /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %3 = qnn.requantize(%2, 6.67265e-05f /* ty=float32 */, 0 /* ty=int32 */, 3.12663e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %4 = qnn.quantize(meta[relay.Constant][1] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 3.12663e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %5 = add(%3, %4) /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %6 = qnn.requantize(%5, 3.12663e-09f /* ty=float32 */, 0 /* ty=int32 */, 3.12663e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %8 = qnn.requantize(%7, 3.12663e-09f /* ty=float32 */, 0 /* ty=int32 */, 3.23821e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 112, 112), int32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %10 = qnn.requantize(%9, 3.23821e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0543282f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %11 = qnn.quantize(meta[relay.Constant][2] /* ty=Tensor[(64, 64, 1, 1), float32] */ /* ty=Tensor[(64, 64, 1, 1), float32] */, 0.00609855f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 64, 1, 1), int8] */;
  %12 = nn.conv2d(%10, %11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %13 = qnn.requantize(%12, 0.000331323f /* ty=float32 */, 0 /* ty=int32 */, 2.95798e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %14 = qnn.quantize(meta[relay.Constant][3] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 2.95798e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %15 = add(%13, %14) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %16 = qnn.requantize(%15, 2.95798e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.95797e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %17 = qnn.quantize(meta[relay.Constant][4] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 2.95797e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %18 = add(%16, %17) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %19 = qnn.requantize(%18, 2.95797e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.95797e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %20 = nn.relu(%19) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %21 = qnn.requantize(%20, 2.95797e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0182585f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %22 = qnn.quantize(meta[relay.Constant][5] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 0.00417648f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 64, 3, 3), int8] */;
  %23 = nn.conv2d(%21, %22, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %24 = qnn.requantize(%23, 7.62562e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.85394e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %25 = qnn.quantize(meta[relay.Constant][6] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1.85394e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %26 = add(%24, %25) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %27 = qnn.requantize(%26, 1.85394e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.85394e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %29 = qnn.requantize(%28, 1.85394e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0212445f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %30 = qnn.quantize(meta[relay.Constant][7] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, 0.0152203f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 64, 1, 1), int8] */;
  %31 = nn.conv2d(%29, %30, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %32 = qnn.requantize(%31, 0.000323348f /* ty=float32 */, 0 /* ty=int32 */, 1.84014e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %33 = qnn.quantize(meta[relay.Constant][8] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.84014e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %34 = add(%32, %33) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %35 = qnn.requantize(%34, 1.84014e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.84034e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %36 = qnn.quantize(meta[relay.Constant][9] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.84034e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %37 = add(%35, %36) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %38 = qnn.requantize(%37, 1.84034e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.05906e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %39 = qnn.requantize(%9, 3.23821e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0543282f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %40 = qnn.quantize(meta[relay.Constant][10] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, 0.0078972f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 64, 1, 1), int8] */;
  %41 = nn.conv2d(%39, %40, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %42 = qnn.requantize(%41, 0.000429041f /* ty=float32 */, 0 /* ty=int32 */, 2.19437e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %43 = qnn.quantize(meta[relay.Constant][11] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 2.19437e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %44 = add(%42, %43) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %45 = qnn.requantize(%44, 2.19437e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.05906e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %46 = add(%38, %45) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %47 = qnn.requantize(%46, 2.05906e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.05906e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %48 = nn.relu(%47) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %49 = qnn.requantize(%48, 2.05906e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0253934f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 56, 56), int8] */;
  %50 = qnn.quantize(meta[relay.Constant][12] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 0.00485266f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 256, 1, 1), int8] */;
  %51 = nn.conv2d(%49, %50, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %52 = qnn.requantize(%51, 0.000123226f /* ty=float32 */, 0 /* ty=int32 */, 1.31247e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %53 = qnn.quantize(meta[relay.Constant][13] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1.31247e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %54 = add(%52, %53) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %55 = qnn.requantize(%54, 1.31247e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.31245e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %56 = qnn.quantize(meta[relay.Constant][14] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1.31245e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %57 = add(%55, %56) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %58 = qnn.requantize(%57, 1.31245e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.31245e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %59 = nn.relu(%58) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %60 = qnn.requantize(%59, 1.31245e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0168176f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %61 = qnn.quantize(meta[relay.Constant][15] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 0.00349414f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 64, 3, 3), int8] */;
  %62 = nn.conv2d(%60, %61, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %63 = qnn.requantize(%62, 5.87631e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.35254e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %64 = qnn.quantize(meta[relay.Constant][16] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1.35254e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %65 = add(%63, %64) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %66 = qnn.requantize(%65, 1.35254e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.35254e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %68 = qnn.requantize(%67, 1.35254e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.022049f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %69 = qnn.quantize(meta[relay.Constant][17] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, 0.0148717f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 64, 1, 1), int8] */;
  %70 = nn.conv2d(%68, %69, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %71 = qnn.requantize(%70, 0.000327906f /* ty=float32 */, 0 /* ty=int32 */, 1.81313e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %72 = qnn.quantize(meta[relay.Constant][18] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.81313e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %73 = add(%71, %72) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %74 = qnn.requantize(%73, 1.81313e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.81304e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %75 = qnn.quantize(meta[relay.Constant][19] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.81304e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %76 = add(%74, %75) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %77 = qnn.requantize(%76, 1.81304e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.71565e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %78 = qnn.requantize(%48, 2.05906e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.71565e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %79 = add(%77, %78) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %80 = qnn.requantize(%79, 1.71565e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.71565e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %81 = nn.relu(%80) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %82 = qnn.requantize(%81, 1.71565e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0276556f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 56, 56), int8] */;
  %83 = qnn.quantize(meta[relay.Constant][20] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 0.00310491f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 256, 1, 1), int8] */;
  %84 = nn.conv2d(%82, %83, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %85 = qnn.requantize(%84, 8.5868e-05f /* ty=float32 */, 0 /* ty=int32 */, 9.23456e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %86 = qnn.quantize(meta[relay.Constant][21] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 9.23456e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %87 = add(%85, %86) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %88 = qnn.requantize(%87, 9.23456e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.23425e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %89 = qnn.quantize(meta[relay.Constant][22] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 9.23425e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %90 = add(%88, %89) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %91 = qnn.requantize(%90, 9.23425e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.23425e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %92 = nn.relu(%91) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %93 = qnn.requantize(%92, 9.23425e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0163706f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %94 = qnn.quantize(meta[relay.Constant][23] /* ty=Tensor[(64, 64, 3, 3), float32] */ /* ty=Tensor[(64, 64, 3, 3), float32] */, 0.00350482f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 64, 3, 3), int8] */;
  %95 = nn.conv2d(%93, %94, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %96 = qnn.requantize(%95, 5.7376e-05f /* ty=float32 */, 0 /* ty=int32 */, 9.16049e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %97 = qnn.quantize(meta[relay.Constant][24] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 9.16049e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %98 = add(%96, %97) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %99 = qnn.requantize(%98, 9.16049e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.16049e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %100 = nn.relu(%99) /* ty=Tensor[(32, 64, 56, 56), int32] */;
  %101 = qnn.requantize(%100, 9.16049e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0161439f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 56, 56), int8] */;
  %102 = qnn.quantize(meta[relay.Constant][25] /* ty=Tensor[(256, 64, 1, 1), float32] */ /* ty=Tensor[(256, 64, 1, 1), float32] */, 0.0144349f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 64, 1, 1), int8] */;
  %103 = nn.conv2d(%101, %102, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %104 = qnn.requantize(%103, 0.000233035f /* ty=float32 */, 0 /* ty=int32 */, 1.54256e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %105 = qnn.quantize(meta[relay.Constant][26] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.54256e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %106 = add(%104, %105) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %107 = qnn.requantize(%106, 1.54256e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.54241e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %108 = qnn.quantize(meta[relay.Constant][27] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.54241e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %109 = add(%107, %108) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %110 = qnn.requantize(%109, 1.54241e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.6484e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %111 = qnn.requantize(%81, 1.71565e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.6484e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %112 = add(%110, %111) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %113 = qnn.requantize(%112, 1.6484e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.6484e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %114 = nn.relu(%113) /* ty=Tensor[(32, 256, 56, 56), int32] */;
  %115 = qnn.requantize(%114, 1.6484e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0279189f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 56, 56), int8] */;
  %116 = qnn.quantize(meta[relay.Constant][28] /* ty=Tensor[(128, 256, 1, 1), float32] */ /* ty=Tensor[(128, 256, 1, 1), float32] */, 0.00286972f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 256, 1, 1), int8] */;
  %117 = nn.conv2d(%115, %116, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %118 = qnn.requantize(%117, 8.01193e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.04587e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %119 = qnn.quantize(meta[relay.Constant][29] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.04587e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %120 = add(%118, %119) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %121 = qnn.requantize(%120, 1.04587e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.04587e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %122 = qnn.quantize(meta[relay.Constant][30] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.04587e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %123 = add(%121, %122) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %124 = qnn.requantize(%123, 1.04587e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.04587e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %125 = nn.relu(%124) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %126 = qnn.requantize(%125, 1.04587e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0142699f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %127 = qnn.quantize(meta[relay.Constant][31] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 0.0032833f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 3, 3), int8] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %129 = qnn.requantize(%128, 4.68524e-05f /* ty=float32 */, 0 /* ty=int32 */, 9.80875e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %130 = qnn.quantize(meta[relay.Constant][32] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 9.80875e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %131 = add(%129, %130) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %132 = qnn.requantize(%131, 9.80875e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.80875e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %133 = nn.relu(%132) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %134 = qnn.requantize(%133, 9.80875e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0146164f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %135 = qnn.quantize(meta[relay.Constant][33] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, 0.010646f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 128, 1, 1), int8] */;
  %136 = nn.conv2d(%134, %135, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %137 = qnn.requantize(%136, 0.000155605f /* ty=float32 */, 0 /* ty=int32 */, 1.29401e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %138 = qnn.quantize(meta[relay.Constant][34] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.29401e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %139 = add(%137, %138) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %140 = qnn.requantize(%139, 1.29401e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.29399e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %141 = qnn.quantize(meta[relay.Constant][35] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.29399e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %142 = add(%140, %141) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %143 = qnn.requantize(%142, 1.29399e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.38908e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %144 = qnn.requantize(%114, 1.6484e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0279189f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 56, 56), int8] */;
  %145 = qnn.quantize(meta[relay.Constant][36] /* ty=Tensor[(512, 256, 1, 1), float32] */ /* ty=Tensor[(512, 256, 1, 1), float32] */, 0.00767737f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 256, 1, 1), int8] */;
  %146 = nn.conv2d(%144, %145, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %147 = qnn.requantize(%146, 0.000214344f /* ty=float32 */, 0 /* ty=int32 */, 1.47811e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %148 = qnn.quantize(meta[relay.Constant][37] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.47811e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %149 = add(%147, %148) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %150 = qnn.requantize(%149, 1.47811e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.38908e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %151 = add(%143, %150) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %152 = qnn.requantize(%151, 1.38908e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.38908e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %153 = nn.relu(%152) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %154 = qnn.requantize(%153, 1.38908e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0242996f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 28, 28), int8] */;
  %155 = qnn.quantize(meta[relay.Constant][38] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, 0.00448611f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 512, 1, 1), int8] */;
  %156 = nn.conv2d(%154, %155, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %157 = qnn.requantize(%156, 0.000109011f /* ty=float32 */, 0 /* ty=int32 */, 1.17276e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %158 = qnn.quantize(meta[relay.Constant][39] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.17276e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %159 = add(%157, %158) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %160 = qnn.requantize(%159, 1.17276e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.17277e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %161 = qnn.quantize(meta[relay.Constant][40] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.17277e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %162 = add(%160, %161) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %163 = qnn.requantize(%162, 1.17277e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.17277e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %164 = nn.relu(%163) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %165 = qnn.requantize(%164, 1.17277e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0137866f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %166 = qnn.quantize(meta[relay.Constant][41] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 0.00369772f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 3, 3), int8] */;
  %167 = nn.conv2d(%165, %166, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %168 = qnn.requantize(%167, 5.0979e-05f /* ty=float32 */, 0 /* ty=int32 */, 9.18822e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %169 = qnn.quantize(meta[relay.Constant][42] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 9.18822e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %170 = add(%168, %169) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %171 = qnn.requantize(%170, 9.18822e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.18822e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %173 = qnn.requantize(%172, 9.18822e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0118365f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %174 = qnn.quantize(meta[relay.Constant][43] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, 0.00891572f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 128, 1, 1), int8] */;
  %175 = nn.conv2d(%173, %174, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %176 = qnn.requantize(%175, 0.000105531f /* ty=float32 */, 0 /* ty=int32 */, 9.71687e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %177 = qnn.quantize(meta[relay.Constant][44] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 9.71687e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %178 = add(%176, %177) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %179 = qnn.requantize(%178, 9.71687e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.71731e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %180 = qnn.quantize(meta[relay.Constant][45] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 9.71731e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %181 = add(%179, %180) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %182 = qnn.requantize(%181, 9.71731e-10f /* ty=float32 */, 0 /* ty=int32 */, 1.44837e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %183 = qnn.requantize(%153, 1.38908e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.44837e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %184 = add(%182, %183) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %185 = qnn.requantize(%184, 1.44837e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.44837e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %186 = nn.relu(%185) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %187 = qnn.requantize(%186, 1.44837e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0244258f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 28, 28), int8] */;
  %188 = qnn.quantize(meta[relay.Constant][46] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, 0.00447191f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 512, 1, 1), int8] */;
  %189 = nn.conv2d(%187, %188, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %190 = qnn.requantize(%189, 0.00010923f /* ty=float32 */, 0 /* ty=int32 */, 1.2403e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %191 = qnn.quantize(meta[relay.Constant][47] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.2403e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %192 = add(%190, %191) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %193 = qnn.requantize(%192, 1.2403e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.24031e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %194 = qnn.quantize(meta[relay.Constant][48] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.24031e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %195 = add(%193, %194) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %196 = qnn.requantize(%195, 1.24031e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.24031e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %197 = nn.relu(%196) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %198 = qnn.requantize(%197, 1.24031e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0204046f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %199 = qnn.quantize(meta[relay.Constant][49] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 0.00388262f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 3, 3), int8] */;
  %200 = nn.conv2d(%198, %199, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %201 = qnn.requantize(%200, 7.92233e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.07658e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %202 = qnn.quantize(meta[relay.Constant][50] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.07658e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %203 = add(%201, %202) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %204 = qnn.requantize(%203, 1.07658e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.07658e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %206 = qnn.requantize(%205, 1.07658e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0183218f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %207 = qnn.quantize(meta[relay.Constant][51] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, 0.00841142f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 128, 1, 1), int8] */;
  %208 = nn.conv2d(%206, %207, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %209 = qnn.requantize(%208, 0.000154112f /* ty=float32 */, 0 /* ty=int32 */, 9.94988e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %210 = qnn.quantize(meta[relay.Constant][52] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 9.94988e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %211 = add(%209, %210) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %212 = qnn.requantize(%211, 9.94988e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.95019e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %213 = qnn.quantize(meta[relay.Constant][53] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 9.95019e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %214 = add(%212, %213) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %215 = qnn.requantize(%214, 9.95019e-10f /* ty=float32 */, 0 /* ty=int32 */, 1.45589e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %216 = qnn.requantize(%186, 1.44837e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.45589e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %217 = add(%215, %216) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %218 = qnn.requantize(%217, 1.45589e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.45589e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %219 = nn.relu(%218) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %220 = qnn.requantize(%219, 1.45589e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.026306f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 28, 28), int8] */;
  %221 = qnn.quantize(meta[relay.Constant][54] /* ty=Tensor[(128, 512, 1, 1), float32] */ /* ty=Tensor[(128, 512, 1, 1), float32] */, 0.00394898f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 512, 1, 1), int8] */;
  %222 = nn.conv2d(%220, %221, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %223 = qnn.requantize(%222, 0.000103882f /* ty=float32 */, 0 /* ty=int32 */, 2.02124e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %224 = qnn.quantize(meta[relay.Constant][55] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 2.02124e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %225 = add(%223, %224) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %226 = qnn.requantize(%225, 2.02124e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.02125e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %227 = qnn.quantize(meta[relay.Constant][56] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 2.02125e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %228 = add(%226, %227) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %229 = qnn.requantize(%228, 2.02125e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.02125e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %230 = nn.relu(%229) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %231 = qnn.requantize(%230, 2.02125e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0316616f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %232 = qnn.quantize(meta[relay.Constant][57] /* ty=Tensor[(128, 128, 3, 3), float32] */ /* ty=Tensor[(128, 128, 3, 3), float32] */, 0.00302293f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 3, 3), int8] */;
  %233 = nn.conv2d(%231, %232, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %234 = qnn.requantize(%233, 9.57109e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.14735e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %235 = qnn.quantize(meta[relay.Constant][58] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1.14735e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %236 = add(%234, %235) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %237 = qnn.requantize(%236, 1.14735e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.14735e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %238 = nn.relu(%237) /* ty=Tensor[(32, 128, 28, 28), int32] */;
  %239 = qnn.requantize(%238, 1.14735e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0194376f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 28, 28), int8] */;
  %240 = qnn.quantize(meta[relay.Constant][59] /* ty=Tensor[(512, 128, 1, 1), float32] */ /* ty=Tensor[(512, 128, 1, 1), float32] */, 0.0101345f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 128, 1, 1), int8] */;
  %241 = nn.conv2d(%239, %240, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %242 = qnn.requantize(%241, 0.00019699f /* ty=float32 */, 0 /* ty=int32 */, 1.20215e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %243 = qnn.quantize(meta[relay.Constant][60] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.20215e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %244 = add(%242, %243) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %245 = qnn.requantize(%244, 1.20215e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.20218e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %246 = qnn.quantize(meta[relay.Constant][61] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.20218e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %247 = add(%245, %246) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %248 = qnn.requantize(%247, 1.20218e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.56796e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %249 = qnn.requantize(%219, 1.45589e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.56796e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %250 = add(%248, %249) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %251 = qnn.requantize(%250, 1.56796e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.56796e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %252 = nn.relu(%251) /* ty=Tensor[(32, 512, 28, 28), int32] */;
  %253 = qnn.requantize(%252, 1.56796e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.031925f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 28, 28), int8] */;
  %254 = qnn.quantize(meta[relay.Constant][62] /* ty=Tensor[(256, 512, 1, 1), float32] */ /* ty=Tensor[(256, 512, 1, 1), float32] */, 0.00359078f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 512, 1, 1), int8] */;
  %255 = nn.conv2d(%253, %254, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %256 = qnn.requantize(%255, 0.000114636f /* ty=float32 */, 0 /* ty=int32 */, 7.10539e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %257 = qnn.quantize(meta[relay.Constant][63] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 7.10539e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %258 = add(%256, %257) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %259 = qnn.requantize(%258, 7.10539e-10f /* ty=float32 */, 0 /* ty=int32 */, 7.10542e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %260 = qnn.quantize(meta[relay.Constant][64] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 7.10542e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %261 = add(%259, %260) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %262 = qnn.requantize(%261, 7.10542e-10f /* ty=float32 */, 0 /* ty=int32 */, 7.10542e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %263 = nn.relu(%262) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %264 = qnn.requantize(%263, 7.10542e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0114107f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %265 = qnn.quantize(meta[relay.Constant][65] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 0.00237224f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 256, 3, 3), int8] */;
  %266 = nn.conv2d(%264, %265, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %267 = qnn.requantize(%266, 2.7069e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.02309e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %268 = qnn.quantize(meta[relay.Constant][66] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.02309e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %269 = add(%267, %268) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %270 = qnn.requantize(%269, 1.02309e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.02309e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %271 = nn.relu(%270) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %272 = qnn.requantize(%271, 1.02309e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0111092f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %273 = qnn.quantize(meta[relay.Constant][67] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 0.0111844f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(1024, 256, 1, 1), int8] */;
  %274 = nn.conv2d(%272, %273, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %275 = qnn.requantize(%274, 0.00012425f /* ty=float32 */, 0 /* ty=int32 */, 9.66816e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %276 = qnn.quantize(meta[relay.Constant][68] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 9.66816e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %277 = add(%275, %276) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %278 = qnn.requantize(%277, 9.66816e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.66801e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %279 = qnn.quantize(meta[relay.Constant][69] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 9.66801e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %280 = add(%278, %279) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %281 = qnn.requantize(%280, 9.66801e-10f /* ty=float32 */, 0 /* ty=int32 */, 1.20767e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %282 = qnn.requantize(%252, 1.56796e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.031925f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 28, 28), int8] */;
  %283 = qnn.quantize(meta[relay.Constant][70] /* ty=Tensor[(1024, 512, 1, 1), float32] */ /* ty=Tensor[(1024, 512, 1, 1), float32] */, 0.00463721f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(1024, 512, 1, 1), int8] */;
  %284 = nn.conv2d(%282, %283, strides=[2, 2], padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %285 = qnn.requantize(%284, 0.000148043f /* ty=float32 */, 0 /* ty=int32 */, 1.24197e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %286 = qnn.quantize(meta[relay.Constant][71] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1.24197e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %287 = add(%285, %286) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %288 = qnn.requantize(%287, 1.24197e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.20767e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %289 = add(%281, %288) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %290 = qnn.requantize(%289, 1.20767e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.20767e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %291 = nn.relu(%290) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %292 = qnn.requantize(%291, 1.20767e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0236786f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1024, 14, 14), int8] */;
  %293 = qnn.quantize(meta[relay.Constant][72] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 0.00292385f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 1024, 1, 1), int8] */;
  %294 = nn.conv2d(%292, %293, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %295 = qnn.requantize(%294, 6.92327e-05f /* ty=float32 */, 0 /* ty=int32 */, 9.06848e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %296 = qnn.quantize(meta[relay.Constant][73] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 9.06848e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %297 = add(%295, %296) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %298 = qnn.requantize(%297, 9.06848e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.06852e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %299 = qnn.quantize(meta[relay.Constant][74] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 9.06852e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %300 = add(%298, %299) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %301 = qnn.requantize(%300, 9.06852e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.06852e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %302 = nn.relu(%301) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %303 = qnn.requantize(%302, 9.06852e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0132926f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %304 = qnn.quantize(meta[relay.Constant][75] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 0.00261371f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 256, 3, 3), int8] */;
  %305 = nn.conv2d(%303, %304, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %306 = qnn.requantize(%305, 3.4743e-05f /* ty=float32 */, 0 /* ty=int32 */, 8.57371e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %307 = qnn.quantize(meta[relay.Constant][76] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 8.57371e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %308 = add(%306, %307) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %309 = qnn.requantize(%308, 8.57371e-10f /* ty=float32 */, 0 /* ty=int32 */, 8.57371e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %310 = nn.relu(%309) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %311 = qnn.requantize(%310, 8.57371e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0128749f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %312 = qnn.quantize(meta[relay.Constant][77] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 0.00848259f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(1024, 256, 1, 1), int8] */;
  %313 = nn.conv2d(%311, %312, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %314 = qnn.requantize(%313, 0.000109212f /* ty=float32 */, 0 /* ty=int32 */, 7.32045e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %315 = qnn.quantize(meta[relay.Constant][78] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 7.32045e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %316 = add(%314, %315) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %317 = qnn.requantize(%316, 7.32045e-10f /* ty=float32 */, 0 /* ty=int32 */, 7.32014e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %318 = qnn.quantize(meta[relay.Constant][79] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 7.32014e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %319 = add(%317, %318) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %320 = qnn.requantize(%319, 7.32014e-10f /* ty=float32 */, 0 /* ty=int32 */, 1.41135e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %321 = qnn.requantize(%291, 1.20767e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.41135e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %322 = add(%320, %321) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %323 = qnn.requantize(%322, 1.41135e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.41135e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %324 = nn.relu(%323) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %325 = qnn.requantize(%324, 1.41135e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0217418f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1024, 14, 14), int8] */;
  %326 = qnn.quantize(meta[relay.Constant][80] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 0.00378073f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 1024, 1, 1), int8] */;
  %327 = nn.conv2d(%325, %326, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %328 = qnn.requantize(%327, 8.21998e-05f /* ty=float32 */, 0 /* ty=int32 */, 9.28803e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %329 = qnn.quantize(meta[relay.Constant][81] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 9.28803e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %330 = add(%328, %329) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %331 = qnn.requantize(%330, 9.28803e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.28807e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %332 = qnn.quantize(meta[relay.Constant][82] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 9.28807e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %333 = add(%331, %332) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %334 = qnn.requantize(%333, 9.28807e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.28807e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %335 = nn.relu(%334) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %336 = qnn.requantize(%335, 9.28807e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0143021f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %337 = qnn.quantize(meta[relay.Constant][83] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 0.00288576f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 256, 3, 3), int8] */;
  %338 = nn.conv2d(%336, %337, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %339 = qnn.requantize(%338, 4.12723e-05f /* ty=float32 */, 0 /* ty=int32 */, 8.60461e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %340 = qnn.quantize(meta[relay.Constant][84] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 8.60461e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %341 = add(%339, %340) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %342 = qnn.requantize(%341, 8.60461e-10f /* ty=float32 */, 0 /* ty=int32 */, 8.60461e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %343 = nn.relu(%342) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %344 = qnn.requantize(%343, 8.60461e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0145492f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %345 = qnn.quantize(meta[relay.Constant][85] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 0.0124584f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(1024, 256, 1, 1), int8] */;
  %346 = nn.conv2d(%344, %345, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %347 = qnn.requantize(%346, 0.000181259f /* ty=float32 */, 0 /* ty=int32 */, 1.07839e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %348 = qnn.quantize(meta[relay.Constant][86] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1.07839e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %349 = add(%347, %348) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %350 = qnn.requantize(%349, 1.07839e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.07843e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %351 = qnn.quantize(meta[relay.Constant][87] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1.07843e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %352 = add(%350, %351) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %353 = qnn.requantize(%352, 1.07843e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.29591e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %354 = qnn.requantize(%324, 1.41135e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.29591e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %355 = add(%353, %354) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %356 = qnn.requantize(%355, 1.29591e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.29591e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %357 = nn.relu(%356) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %358 = qnn.requantize(%357, 1.29591e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.022348f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1024, 14, 14), int8] */;
  %359 = qnn.quantize(meta[relay.Constant][88] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 0.00437263f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 1024, 1, 1), int8] */;
  %360 = nn.conv2d(%358, %359, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %361 = qnn.requantize(%360, 9.77198e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.12619e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %362 = qnn.quantize(meta[relay.Constant][89] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.12619e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %363 = add(%361, %362) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %364 = qnn.requantize(%363, 1.12619e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.1262e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %365 = qnn.quantize(meta[relay.Constant][90] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.1262e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %366 = add(%364, %365) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %367 = qnn.requantize(%366, 1.1262e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.1262e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %368 = nn.relu(%367) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %369 = qnn.requantize(%368, 1.1262e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0188223f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %370 = qnn.quantize(meta[relay.Constant][91] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 0.00424186f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 256, 3, 3), int8] */;
  %371 = nn.conv2d(%369, %370, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %372 = qnn.requantize(%371, 7.98413e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.03312e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %373 = qnn.quantize(meta[relay.Constant][92] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.03312e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %374 = add(%372, %373) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %375 = qnn.requantize(%374, 1.03312e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.03312e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %376 = nn.relu(%375) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %377 = qnn.requantize(%376, 1.03312e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0157012f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %378 = qnn.quantize(meta[relay.Constant][93] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 0.00946433f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(1024, 256, 1, 1), int8] */;
  %379 = nn.conv2d(%377, %378, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %380 = qnn.requantize(%379, 0.000148601f /* ty=float32 */, 0 /* ty=int32 */, 1.1081e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %381 = qnn.quantize(meta[relay.Constant][94] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1.1081e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %382 = add(%380, %381) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %383 = qnn.requantize(%382, 1.1081e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.10813e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %384 = qnn.quantize(meta[relay.Constant][95] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1.10813e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %385 = add(%383, %384) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %386 = qnn.requantize(%385, 1.10813e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.33205e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %387 = qnn.requantize(%357, 1.29591e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.33205e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %388 = add(%386, %387) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %389 = qnn.requantize(%388, 1.33205e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.33205e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %390 = nn.relu(%389) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %391 = qnn.requantize(%390, 1.33205e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0242511f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1024, 14, 14), int8] */;
  %392 = qnn.quantize(meta[relay.Constant][96] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 0.00525388f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 1024, 1, 1), int8] */;
  %393 = nn.conv2d(%391, %392, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %394 = qnn.requantize(%393, 0.000127412f /* ty=float32 */, 0 /* ty=int32 */, 1.40236e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %395 = qnn.quantize(meta[relay.Constant][97] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.40236e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %396 = add(%394, %395) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %397 = qnn.requantize(%396, 1.40236e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.40237e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %398 = qnn.quantize(meta[relay.Constant][98] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.40237e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %399 = add(%397, %398) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %400 = qnn.requantize(%399, 1.40237e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.40237e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %401 = nn.relu(%400) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %402 = qnn.requantize(%401, 1.40237e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0232779f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %403 = qnn.quantize(meta[relay.Constant][99] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 0.00649712f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 256, 3, 3), int8] */;
  %404 = nn.conv2d(%402, %403, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %405 = qnn.requantize(%404, 0.000151239f /* ty=float32 */, 0 /* ty=int32 */, 2.51246e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %406 = qnn.quantize(meta[relay.Constant][100] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 2.51246e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %407 = add(%405, %406) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %408 = qnn.requantize(%407, 2.51246e-09f /* ty=float32 */, 0 /* ty=int32 */, 2.51246e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %409 = nn.relu(%408) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %410 = qnn.requantize(%409, 2.51246e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0392821f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %411 = qnn.quantize(meta[relay.Constant][101] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 0.0086158f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(1024, 256, 1, 1), int8] */;
  %412 = nn.conv2d(%410, %411, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %413 = qnn.requantize(%412, 0.000338447f /* ty=float32 */, 0 /* ty=int32 */, 1.10404e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %414 = qnn.quantize(meta[relay.Constant][102] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1.10404e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %415 = add(%413, %414) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %416 = qnn.requantize(%415, 1.10404e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.10407e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %417 = qnn.quantize(meta[relay.Constant][103] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1.10407e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %418 = add(%416, %417) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %419 = qnn.requantize(%418, 1.10407e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.44548e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %420 = qnn.requantize(%390, 1.33205e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.44548e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %421 = add(%419, %420) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %422 = qnn.requantize(%421, 1.44548e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.44548e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %423 = nn.relu(%422) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %424 = qnn.requantize(%423, 1.44548e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0326502f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1024, 14, 14), int8] */;
  %425 = qnn.quantize(meta[relay.Constant][104] /* ty=Tensor[(256, 1024, 1, 1), float32] */ /* ty=Tensor[(256, 1024, 1, 1), float32] */, 0.00517882f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 1024, 1, 1), int8] */;
  %426 = nn.conv2d(%424, %425, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %427 = qnn.requantize(%426, 0.00016909f /* ty=float32 */, 0 /* ty=int32 */, 1.36276e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %428 = qnn.quantize(meta[relay.Constant][105] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.36276e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %429 = add(%427, %428) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %430 = qnn.requantize(%429, 1.36276e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.36276e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %431 = qnn.quantize(meta[relay.Constant][106] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.36276e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %432 = add(%430, %431) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %433 = qnn.requantize(%432, 1.36276e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.36276e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %434 = nn.relu(%433) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %435 = qnn.requantize(%434, 1.36276e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0219498f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %436 = qnn.quantize(meta[relay.Constant][107] /* ty=Tensor[(256, 256, 3, 3), float32] */ /* ty=Tensor[(256, 256, 3, 3), float32] */, 0.00370547f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(256, 256, 3, 3), int8] */;
  %437 = nn.conv2d(%435, %436, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %438 = qnn.requantize(%437, 8.13346e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.0169e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %439 = qnn.quantize(meta[relay.Constant][108] /* ty=Tensor[(256, 1, 1), float32] */ /* ty=Tensor[(256, 1, 1), float32] */, 1.0169e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(256, 1, 1), int32] */;
  %440 = add(%438, %439) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %441 = qnn.requantize(%440, 1.0169e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.0169e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %442 = nn.relu(%441) /* ty=Tensor[(32, 256, 14, 14), int32] */;
  %443 = qnn.requantize(%442, 1.0169e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0171255f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 14, 14), int8] */;
  %444 = qnn.quantize(meta[relay.Constant][109] /* ty=Tensor[(1024, 256, 1, 1), float32] */ /* ty=Tensor[(1024, 256, 1, 1), float32] */, 0.00871211f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(1024, 256, 1, 1), int8] */;
  %445 = nn.conv2d(%443, %444, padding=[0, 0, 0, 0], channels=1024, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %446 = qnn.requantize(%445, 0.000149199f /* ty=float32 */, 0 /* ty=int32 */, 1.23962e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %447 = qnn.quantize(meta[relay.Constant][110] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1.23962e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %448 = add(%446, %447) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %449 = qnn.requantize(%448, 1.23962e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.23959e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %450 = qnn.quantize(meta[relay.Constant][111] /* ty=Tensor[(1024, 1, 1), float32] */ /* ty=Tensor[(1024, 1, 1), float32] */, 1.23959e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(1024, 1, 1), int32] */;
  %451 = add(%449, %450) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %452 = qnn.requantize(%451, 1.23959e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.94611e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %453 = qnn.requantize(%423, 1.44548e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.94611e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %454 = add(%452, %453) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %455 = qnn.requantize(%454, 1.94611e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.94611e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %456 = nn.relu(%455) /* ty=Tensor[(32, 1024, 14, 14), int32] */;
  %457 = qnn.requantize(%456, 1.94611e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0328963f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1024, 14, 14), int8] */;
  %458 = qnn.quantize(meta[relay.Constant][112] /* ty=Tensor[(512, 1024, 1, 1), float32] */ /* ty=Tensor[(512, 1024, 1, 1), float32] */, 0.00388161f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 1024, 1, 1), int8] */;
  %459 = nn.conv2d(%457, %458, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %460 = qnn.requantize(%459, 0.000127691f /* ty=float32 */, 0 /* ty=int32 */, 8.83036e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %461 = qnn.quantize(meta[relay.Constant][113] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 8.83036e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %462 = add(%460, %461) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %463 = qnn.requantize(%462, 8.83036e-10f /* ty=float32 */, 0 /* ty=int32 */, 8.83036e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %464 = qnn.quantize(meta[relay.Constant][114] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 8.83036e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %465 = add(%463, %464) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %466 = qnn.requantize(%465, 8.83036e-10f /* ty=float32 */, 0 /* ty=int32 */, 8.83036e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %467 = nn.relu(%466) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %468 = qnn.requantize(%467, 8.83036e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0150482f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 7, 7), int8] */;
  %469 = qnn.quantize(meta[relay.Constant][115] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 0.00248865f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 512, 3, 3), int8] */;
  %470 = nn.conv2d(%468, %469, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %471 = qnn.requantize(%470, 3.74497e-05f /* ty=float32 */, 0 /* ty=int32 */, 7.54132e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %472 = qnn.quantize(meta[relay.Constant][116] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 7.54132e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %473 = add(%471, %472) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %474 = qnn.requantize(%473, 7.54132e-10f /* ty=float32 */, 0 /* ty=int32 */, 7.54132e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %475 = nn.relu(%474) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %476 = qnn.requantize(%475, 7.54132e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0100588f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 7, 7), int8] */;
  %477 = qnn.quantize(meta[relay.Constant][117] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, 0.00920399f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(2048, 512, 1, 1), int8] */;
  %478 = nn.conv2d(%476, %477, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %479 = qnn.requantize(%478, 9.25815e-05f /* ty=float32 */, 0 /* ty=int32 */, 6.46258e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %480 = qnn.quantize(meta[relay.Constant][118] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 6.46258e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(2048, 1, 1), int32] */;
  %481 = add(%479, %480) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %482 = qnn.requantize(%481, 6.46258e-10f /* ty=float32 */, 0 /* ty=int32 */, 6.46258e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %483 = qnn.quantize(meta[relay.Constant][119] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 6.46258e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(2048, 1, 1), int32] */;
  %484 = add(%482, %483) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %485 = qnn.requantize(%484, 6.46258e-10f /* ty=float32 */, 0 /* ty=int32 */, 8.71455e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %486 = qnn.requantize(%456, 1.94611e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0328963f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1024, 14, 14), int8] */;
  %487 = qnn.quantize(meta[relay.Constant][120] /* ty=Tensor[(2048, 1024, 1, 1), float32] */ /* ty=Tensor[(2048, 1024, 1, 1), float32] */, 0.00403221f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(2048, 1024, 1, 1), int8] */;
  %488 = nn.conv2d(%486, %487, strides=[2, 2], padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %489 = qnn.requantize(%488, 0.000132645f /* ty=float32 */, 0 /* ty=int32 */, 9.29792e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %490 = qnn.quantize(meta[relay.Constant][121] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 9.29792e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(2048, 1, 1), int32] */;
  %491 = add(%489, %490) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %492 = qnn.requantize(%491, 9.29792e-10f /* ty=float32 */, 0 /* ty=int32 */, 8.71455e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %493 = add(%485, %492) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %494 = qnn.requantize(%493, 8.71455e-10f /* ty=float32 */, 0 /* ty=int32 */, 8.71455e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %495 = nn.relu(%494) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %496 = qnn.requantize(%495, 8.71455e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0152692f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 2048, 7, 7), int8] */;
  %497 = qnn.quantize(meta[relay.Constant][122] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, 0.0042901f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 2048, 1, 1), int8] */;
  %498 = nn.conv2d(%496, %497, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %499 = qnn.requantize(%498, 6.55063e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.04501e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %500 = qnn.quantize(meta[relay.Constant][123] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.04501e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %501 = add(%499, %500) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %502 = qnn.requantize(%501, 1.04501e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.04501e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %503 = qnn.quantize(meta[relay.Constant][124] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.04501e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %504 = add(%502, %503) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %505 = qnn.requantize(%504, 1.04501e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.04501e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %506 = nn.relu(%505) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %507 = qnn.requantize(%506, 1.04501e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0176663f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 7, 7), int8] */;
  %508 = qnn.quantize(meta[relay.Constant][125] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 0.00272741f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 512, 3, 3), int8] */;
  %509 = nn.conv2d(%507, %508, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %510 = qnn.requantize(%509, 4.81833e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.04836e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %511 = qnn.quantize(meta[relay.Constant][126] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.04836e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %512 = add(%510, %511) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %513 = qnn.requantize(%512, 1.04836e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.04836e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %514 = nn.relu(%513) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %515 = qnn.requantize(%514, 1.04836e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0174499f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 7, 7), int8] */;
  %516 = qnn.quantize(meta[relay.Constant][127] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, 0.00738607f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(2048, 512, 1, 1), int8] */;
  %517 = nn.conv2d(%515, %516, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %518 = qnn.requantize(%517, 0.000128886f /* ty=float32 */, 0 /* ty=int32 */, 7.79854e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %519 = qnn.quantize(meta[relay.Constant][128] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 7.79854e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(2048, 1, 1), int32] */;
  %520 = add(%518, %519) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %521 = qnn.requantize(%520, 7.79854e-10f /* ty=float32 */, 0 /* ty=int32 */, 7.79854e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %522 = qnn.quantize(meta[relay.Constant][129] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 7.79854e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(2048, 1, 1), int32] */;
  %523 = add(%521, %522) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %524 = qnn.requantize(%523, 7.79854e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.10114e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %525 = qnn.requantize(%495, 8.71455e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.10114e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %526 = add(%524, %525) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %527 = qnn.requantize(%526, 9.10114e-10f /* ty=float32 */, 0 /* ty=int32 */, 9.10114e-10f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %528 = nn.relu(%527) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %529 = qnn.requantize(%528, 9.10114e-10f /* ty=float32 */, 0 /* ty=int32 */, 0.0207965f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 2048, 7, 7), int8] */;
  %530 = qnn.quantize(meta[relay.Constant][130] /* ty=Tensor[(512, 2048, 1, 1), float32] */ /* ty=Tensor[(512, 2048, 1, 1), float32] */, 0.00688873f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 2048, 1, 1), int8] */;
  %531 = nn.conv2d(%529, %530, padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %532 = qnn.requantize(%531, 0.000143261f /* ty=float32 */, 0 /* ty=int32 */, 1.81667e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %533 = qnn.quantize(meta[relay.Constant][131] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.81667e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %534 = add(%532, %533) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %535 = qnn.requantize(%534, 1.81667e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.81667e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %536 = qnn.quantize(meta[relay.Constant][132] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.81667e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %537 = add(%535, %536) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %538 = qnn.requantize(%537, 1.81667e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.81667e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %539 = nn.relu(%538) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %540 = qnn.requantize(%539, 1.81667e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0285672f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 7, 7), int8] */;
  %541 = qnn.quantize(meta[relay.Constant][133] /* ty=Tensor[(512, 512, 3, 3), float32] */ /* ty=Tensor[(512, 512, 3, 3), float32] */, 0.002343f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(512, 512, 3, 3), int8] */;
  %542 = nn.conv2d(%540, %541, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %543 = qnn.requantize(%542, 6.69329e-05f /* ty=float32 */, 0 /* ty=int32 */, 1.43354e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %544 = qnn.quantize(meta[relay.Constant][134] /* ty=Tensor[(512, 1, 1), float32] */ /* ty=Tensor[(512, 1, 1), float32] */, 1.43354e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(512, 1, 1), int32] */;
  %545 = add(%543, %544) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %546 = qnn.requantize(%545, 1.43354e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.43354e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %547 = nn.relu(%546) /* ty=Tensor[(32, 512, 7, 7), int32] */;
  %548 = qnn.requantize(%547, 1.43354e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0246551f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 512, 7, 7), int8] */;
  %549 = qnn.quantize(meta[relay.Constant][135] /* ty=Tensor[(2048, 512, 1, 1), float32] */ /* ty=Tensor[(2048, 512, 1, 1), float32] */, 0.166448f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(2048, 512, 1, 1), int8] */;
  %550 = nn.conv2d(%548, %549, padding=[0, 0, 0, 0], channels=2048, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %551 = qnn.requantize(%550, 0.00410379f /* ty=float32 */, 0 /* ty=int32 */, 1.45375e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %552 = qnn.quantize(meta[relay.Constant][136] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 1.45375e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(2048, 1, 1), int32] */;
  %553 = add(%551, %552) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %554 = qnn.requantize(%553, 1.45375e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.45375e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %555 = qnn.quantize(meta[relay.Constant][137] /* ty=Tensor[(2048, 1, 1), float32] */ /* ty=Tensor[(2048, 1, 1), float32] */, 1.45375e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(2048, 1, 1), int32] */;
  %556 = add(%554, %555) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %557 = qnn.requantize(%556, 1.45375e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.46121e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %558 = qnn.requantize(%528, 9.10114e-10f /* ty=float32 */, 0 /* ty=int32 */, 1.46121e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %559 = add(%557, %558) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %560 = qnn.requantize(%559, 1.46121e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.46121e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %561 = nn.relu(%560) /* ty=Tensor[(32, 2048, 7, 7), int32] */;
  %562 = qnn.dequantize(%561, 1.46121e-08f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 2048, 7, 7), float32] */;
  %563 = nn.global_avg_pool2d(%562) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %564 = nn.batch_flatten(%563) /* ty=Tensor[(32, 2048), float32] */;
  %565 = nn.dense(%564, meta[relay.Constant][138] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%565, meta[relay.Constant][139] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 64, 56, 56), 'int8'), ('TENSOR', (64, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 64, 56, 56), 'int8'), ('TENSOR', (256, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 256, 56, 56), 'int8'), ('TENSOR', (64, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 256, 56, 56), 'int8'), ('TENSOR', (128, 256, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 128, 28, 28), 'int8'), ('TENSOR', (512, 128, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 256, 56, 56), 'int8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 512, 28, 28), 'int8'), ('TENSOR', (128, 512, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 512, 28, 28), 'int8'), ('TENSOR', (256, 512, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 256, 14, 14), 'int8'), ('TENSOR', (1024, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 512, 28, 28), 'int8'), ('TENSOR', (1024, 512, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 1024, 14, 14), 'int8'), ('TENSOR', (256, 1024, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 1024, 14, 14), 'int8'), ('TENSOR', (512, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 512, 7, 7), 'int8'), ('TENSOR', (2048, 512, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 1024, 14, 14), 'int8'), ('TENSOR', (2048, 1024, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 2048, 7, 7), 'int8'), ('TENSOR', (512, 2048, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
learning_based_quantize.py:100: DeprecationWarning: legacy graph runtime behaviour of producing json / lib / params will be removed in the next release 
  graph, lib, params = relay.build(mod, target)
INFO:root:[3200 samples] validation: acc-top1=0.768437 acc-top5=0.938750
INFO:root:[6400 samples] validation: acc-top1=0.766094 acc-top5=0.932500
INFO:root:[9600 samples] validation: acc-top1=0.766458 acc-top5=0.929896
INFO:root:[12800 samples] validation: acc-top1=0.761797 acc-top5=0.929531
INFO:root:[16000 samples] validation: acc-top1=0.761875 acc-top5=0.929562
INFO:root:[19200 samples] validation: acc-top1=0.759375 acc-top5=0.928906
INFO:root:[22400 samples] validation: acc-top1=0.760134 acc-top5=0.928393
INFO:root:[25600 samples] validation: acc-top1=0.758906 acc-top5=0.928477
INFO:root:[28800 samples] validation: acc-top1=0.759479 acc-top5=0.929549
INFO:root:[32000 samples] validation: acc-top1=0.758844 acc-top5=0.929500
INFO:root:[35200 samples] validation: acc-top1=0.759687 acc-top5=0.929375
INFO:root:[38400 samples] validation: acc-top1=0.759766 acc-top5=0.929401
INFO:root:[41600 samples] validation: acc-top1=0.759327 acc-top5=0.929183
INFO:root:[44800 samples] validation: acc-top1=0.759241 acc-top5=0.929241
INFO:root:[48000 samples] validation: acc-top1=0.760146 acc-top5=0.929229
INFO:root:[final] validation: acc-top1=0.760146 acc-top5=0.929229
Final accuracy int8 0.7601458333333333
[20:34:57] src/io/iter_image_recordio_2.cc:178: ImageRecordIOParser2: /home/woongkyu/imagenet/val.rec, use 4 threads for decoding..
INFO:root:Model file not found. Downloading to /home/woongkyu/.mxnet/models/inceptionv3-ed47ec45.params.
Downloading /home/woongkyu/.mxnet/models/inceptionv3-ed47ec45.zip0de3a24f-7112-4b78-9243-1ee6c58f439a from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/inceptionv3-ed47ec45.zip...
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443 "GET /gluon/models/inceptionv3-ed47ec45.zip HTTP/1.1" 200 88795914
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op sqrt
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op negative
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op squeeze
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
INFO:compile_engine:Use implementation injective.cpu for op multiply
DEBUG:root:original
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 299, 299), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(32, 3, 3, 3), float32] */ /* ty=Tensor[(32, 3, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %3 = nn.conv2d(%2, meta[relay.Constant][2] /* ty=Tensor[(32, 32, 3, 3), float32] */ /* ty=Tensor[(32, 32, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %4 = add(%3, meta[relay.Constant][3] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %5 = nn.relu(%4) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %6 = nn.conv2d(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */ /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %7 = add(%6, meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %8 = nn.relu(%7) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 64, 73, 73), float32] */;
  %10 = nn.conv2d(%9, meta[relay.Constant][6] /* ty=Tensor[(80, 64, 1, 1), float32] */ /* ty=Tensor[(80, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=80, kernel_size=[1, 1]) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %11 = add(%10, meta[relay.Constant][7] /* ty=Tensor[(80, 1, 1), float32] */ /* ty=Tensor[(80, 1, 1), float32] */) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %12 = nn.relu(%11) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %13 = nn.conv2d(%12, meta[relay.Constant][8] /* ty=Tensor[(192, 80, 3, 3), float32] */ /* ty=Tensor[(192, 80, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %14 = add(%13, meta[relay.Constant][9] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %15 = nn.relu(%14) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %16 = nn.max_pool2d(%15, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %17 = nn.conv2d(%16, meta[relay.Constant][10] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %18 = add(%17, meta[relay.Constant][11] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %19 = nn.relu(%18) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %20 = nn.conv2d(%16, meta[relay.Constant][12] /* ty=Tensor[(48, 192, 1, 1), float32] */ /* ty=Tensor[(48, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %21 = add(%20, meta[relay.Constant][13] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %22 = nn.relu(%21) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %23 = nn.conv2d(%22, meta[relay.Constant][14] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %24 = add(%23, meta[relay.Constant][15] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %25 = nn.relu(%24) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %26 = nn.conv2d(%16, meta[relay.Constant][16] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %27 = add(%26, meta[relay.Constant][17] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %29 = nn.conv2d(%28, meta[relay.Constant][18] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %30 = add(%29, meta[relay.Constant][19] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %31 = nn.relu(%30) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %32 = nn.conv2d(%31, meta[relay.Constant][20] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %33 = add(%32, meta[relay.Constant][21] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %34 = nn.relu(%33) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %35 = nn.avg_pool2d(%16, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %36 = nn.conv2d(%35, meta[relay.Constant][22] /* ty=Tensor[(32, 192, 1, 1), float32] */ /* ty=Tensor[(32, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %37 = add(%36, meta[relay.Constant][23] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %38 = nn.relu(%37) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %39 = (%19, %25, %34, %38);
  %40 = concatenate(%39, axis=1) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %41 = nn.conv2d(%40, meta[relay.Constant][24] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %42 = add(%41, meta[relay.Constant][25] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %43 = nn.relu(%42) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %44 = nn.conv2d(%40, meta[relay.Constant][26] /* ty=Tensor[(48, 256, 1, 1), float32] */ /* ty=Tensor[(48, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %45 = add(%44, meta[relay.Constant][27] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %46 = nn.relu(%45) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %47 = nn.conv2d(%46, meta[relay.Constant][28] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %48 = add(%47, meta[relay.Constant][29] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %49 = nn.relu(%48) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %50 = nn.conv2d(%40, meta[relay.Constant][30] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %51 = add(%50, meta[relay.Constant][31] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %52 = nn.relu(%51) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %53 = nn.conv2d(%52, meta[relay.Constant][32] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %54 = add(%53, meta[relay.Constant][33] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %55 = nn.relu(%54) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %56 = nn.conv2d(%55, meta[relay.Constant][34] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %57 = add(%56, meta[relay.Constant][35] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %59 = nn.avg_pool2d(%40, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %60 = nn.conv2d(%59, meta[relay.Constant][36] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %61 = add(%60, meta[relay.Constant][37] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %63 = (%43, %49, %58, %62);
  %64 = concatenate(%63, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %65 = nn.conv2d(%64, meta[relay.Constant][38] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %66 = add(%65, meta[relay.Constant][39] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %68 = nn.conv2d(%64, meta[relay.Constant][40] /* ty=Tensor[(48, 288, 1, 1), float32] */ /* ty=Tensor[(48, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %69 = add(%68, meta[relay.Constant][41] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %70 = nn.relu(%69) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %71 = nn.conv2d(%70, meta[relay.Constant][42] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %72 = add(%71, meta[relay.Constant][43] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %73 = nn.relu(%72) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %74 = nn.conv2d(%64, meta[relay.Constant][44] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %75 = add(%74, meta[relay.Constant][45] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %76 = nn.relu(%75) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %77 = nn.conv2d(%76, meta[relay.Constant][46] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %78 = add(%77, meta[relay.Constant][47] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %79 = nn.relu(%78) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %80 = nn.conv2d(%79, meta[relay.Constant][48] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %81 = add(%80, meta[relay.Constant][49] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %82 = nn.relu(%81) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %83 = nn.avg_pool2d(%64, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %84 = nn.conv2d(%83, meta[relay.Constant][50] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %85 = add(%84, meta[relay.Constant][51] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %86 = nn.relu(%85) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %87 = (%67, %73, %82, %86);
  %88 = concatenate(%87, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %89 = nn.conv2d(%88, meta[relay.Constant][52] /* ty=Tensor[(384, 288, 3, 3), float32] */ /* ty=Tensor[(384, 288, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %90 = add(%89, meta[relay.Constant][53] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %92 = nn.conv2d(%88, meta[relay.Constant][54] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %93 = add(%92, meta[relay.Constant][55] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %94 = nn.relu(%93) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %95 = nn.conv2d(%94, meta[relay.Constant][56] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %96 = add(%95, meta[relay.Constant][57] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %97 = nn.relu(%96) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %98 = nn.conv2d(%97, meta[relay.Constant][58] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %99 = add(%98, meta[relay.Constant][59] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %100 = nn.relu(%99) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %101 = nn.max_pool2d(%88, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 288, 17, 17), float32] */;
  %102 = (%91, %100, %101);
  %103 = concatenate(%102, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %104 = nn.conv2d(%103, meta[relay.Constant][60] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %105 = add(%104, meta[relay.Constant][61] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %106 = nn.relu(%105) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %107 = nn.conv2d(%103, meta[relay.Constant][62] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %108 = add(%107, meta[relay.Constant][63] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %109 = nn.relu(%108) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %110 = nn.conv2d(%109, meta[relay.Constant][64] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %111 = add(%110, meta[relay.Constant][65] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %112 = nn.relu(%111) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %113 = nn.conv2d(%112, meta[relay.Constant][66] /* ty=Tensor[(192, 128, 7, 1), float32] */ /* ty=Tensor[(192, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %114 = add(%113, meta[relay.Constant][67] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %115 = nn.relu(%114) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %116 = nn.conv2d(%103, meta[relay.Constant][68] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %117 = add(%116, meta[relay.Constant][69] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %118 = nn.relu(%117) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %119 = nn.conv2d(%118, meta[relay.Constant][70] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %120 = add(%119, meta[relay.Constant][71] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %121 = nn.relu(%120) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %122 = nn.conv2d(%121, meta[relay.Constant][72] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %123 = add(%122, meta[relay.Constant][73] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %125 = nn.conv2d(%124, meta[relay.Constant][74] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %126 = add(%125, meta[relay.Constant][75] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %127 = nn.relu(%126) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %128 = nn.conv2d(%127, meta[relay.Constant][76] /* ty=Tensor[(192, 128, 1, 7), float32] */ /* ty=Tensor[(192, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %129 = add(%128, meta[relay.Constant][77] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %130 = nn.relu(%129) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %131 = nn.avg_pool2d(%103, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %132 = nn.conv2d(%131, meta[relay.Constant][78] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %133 = add(%132, meta[relay.Constant][79] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %134 = nn.relu(%133) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %135 = (%106, %115, %130, %134);
  %136 = concatenate(%135, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %137 = nn.conv2d(%136, meta[relay.Constant][80] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %138 = add(%137, meta[relay.Constant][81] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %139 = nn.relu(%138) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %140 = nn.conv2d(%136, meta[relay.Constant][82] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %141 = add(%140, meta[relay.Constant][83] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %142 = nn.relu(%141) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %143 = nn.conv2d(%142, meta[relay.Constant][84] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %144 = add(%143, meta[relay.Constant][85] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %145 = nn.relu(%144) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %146 = nn.conv2d(%145, meta[relay.Constant][86] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %147 = add(%146, meta[relay.Constant][87] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %148 = nn.relu(%147) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %149 = nn.conv2d(%136, meta[relay.Constant][88] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %150 = add(%149, meta[relay.Constant][89] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %151 = nn.relu(%150) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %152 = nn.conv2d(%151, meta[relay.Constant][90] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %153 = add(%152, meta[relay.Constant][91] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %154 = nn.relu(%153) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %155 = nn.conv2d(%154, meta[relay.Constant][92] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %156 = add(%155, meta[relay.Constant][93] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %158 = nn.conv2d(%157, meta[relay.Constant][94] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %159 = add(%158, meta[relay.Constant][95] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %161 = nn.conv2d(%160, meta[relay.Constant][96] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %162 = add(%161, meta[relay.Constant][97] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %163 = nn.relu(%162) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %164 = nn.avg_pool2d(%136, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %165 = nn.conv2d(%164, meta[relay.Constant][98] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %166 = add(%165, meta[relay.Constant][99] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %167 = nn.relu(%166) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %168 = (%139, %148, %163, %167);
  %169 = concatenate(%168, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %170 = nn.conv2d(%169, meta[relay.Constant][100] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %171 = add(%170, meta[relay.Constant][101] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %173 = nn.conv2d(%169, meta[relay.Constant][102] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %174 = add(%173, meta[relay.Constant][103] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %175 = nn.relu(%174) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %176 = nn.conv2d(%175, meta[relay.Constant][104] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %177 = add(%176, meta[relay.Constant][105] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %178 = nn.relu(%177) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %179 = nn.conv2d(%178, meta[relay.Constant][106] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %180 = add(%179, meta[relay.Constant][107] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %181 = nn.relu(%180) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %182 = nn.conv2d(%169, meta[relay.Constant][108] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %183 = add(%182, meta[relay.Constant][109] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %184 = nn.relu(%183) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %185 = nn.conv2d(%184, meta[relay.Constant][110] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %186 = add(%185, meta[relay.Constant][111] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %187 = nn.relu(%186) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %188 = nn.conv2d(%187, meta[relay.Constant][112] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %189 = add(%188, meta[relay.Constant][113] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %190 = nn.relu(%189) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %191 = nn.conv2d(%190, meta[relay.Constant][114] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %192 = add(%191, meta[relay.Constant][115] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %193 = nn.relu(%192) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %194 = nn.conv2d(%193, meta[relay.Constant][116] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %195 = add(%194, meta[relay.Constant][117] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %196 = nn.relu(%195) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %197 = nn.avg_pool2d(%169, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %198 = nn.conv2d(%197, meta[relay.Constant][118] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %199 = add(%198, meta[relay.Constant][119] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %200 = nn.relu(%199) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %201 = (%172, %181, %196, %200);
  %202 = concatenate(%201, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %203 = nn.conv2d(%202, meta[relay.Constant][120] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %204 = add(%203, meta[relay.Constant][121] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %206 = nn.conv2d(%202, meta[relay.Constant][122] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %207 = add(%206, meta[relay.Constant][123] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %208 = nn.relu(%207) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %209 = nn.conv2d(%208, meta[relay.Constant][124] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %210 = add(%209, meta[relay.Constant][125] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %211 = nn.relu(%210) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %212 = nn.conv2d(%211, meta[relay.Constant][126] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %213 = add(%212, meta[relay.Constant][127] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %214 = nn.relu(%213) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %215 = nn.conv2d(%202, meta[relay.Constant][128] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %216 = add(%215, meta[relay.Constant][129] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %217 = nn.relu(%216) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %218 = nn.conv2d(%217, meta[relay.Constant][130] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %219 = add(%218, meta[relay.Constant][131] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %220 = nn.relu(%219) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %221 = nn.conv2d(%220, meta[relay.Constant][132] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %222 = add(%221, meta[relay.Constant][133] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %223 = nn.relu(%222) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %224 = nn.conv2d(%223, meta[relay.Constant][134] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %225 = add(%224, meta[relay.Constant][135] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %226 = nn.relu(%225) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %227 = nn.conv2d(%226, meta[relay.Constant][136] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %228 = add(%227, meta[relay.Constant][137] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %229 = nn.relu(%228) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %230 = nn.avg_pool2d(%202, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %231 = nn.conv2d(%230, meta[relay.Constant][138] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %232 = add(%231, meta[relay.Constant][139] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %233 = nn.relu(%232) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %234 = (%205, %214, %229, %233);
  %235 = concatenate(%234, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %236 = nn.conv2d(%235, meta[relay.Constant][140] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %237 = add(%236, meta[relay.Constant][141] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %238 = nn.relu(%237) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %239 = nn.conv2d(%238, meta[relay.Constant][142] /* ty=Tensor[(320, 192, 3, 3), float32] */ /* ty=Tensor[(320, 192, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=320, kernel_size=[3, 3]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %240 = add(%239, meta[relay.Constant][143] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %241 = nn.relu(%240) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %242 = nn.conv2d(%235, meta[relay.Constant][144] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %243 = add(%242, meta[relay.Constant][145] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %244 = nn.relu(%243) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %245 = nn.conv2d(%244, meta[relay.Constant][146] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %246 = add(%245, meta[relay.Constant][147] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %247 = nn.relu(%246) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %248 = nn.conv2d(%247, meta[relay.Constant][148] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %249 = add(%248, meta[relay.Constant][149] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %250 = nn.relu(%249) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %251 = nn.conv2d(%250, meta[relay.Constant][150] /* ty=Tensor[(192, 192, 3, 3), float32] */ /* ty=Tensor[(192, 192, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %252 = add(%251, meta[relay.Constant][151] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %253 = nn.relu(%252) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %254 = nn.max_pool2d(%235, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %255 = (%241, %253, %254);
  %256 = concatenate(%255, axis=1) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %257 = nn.conv2d(%256, meta[relay.Constant][152] /* ty=Tensor[(320, 1280, 1, 1), float32] */ /* ty=Tensor[(320, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %258 = add(%257, meta[relay.Constant][153] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %259 = nn.relu(%258) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %260 = nn.conv2d(%256, meta[relay.Constant][154] /* ty=Tensor[(384, 1280, 1, 1), float32] */ /* ty=Tensor[(384, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %261 = add(%260, meta[relay.Constant][155] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %262 = nn.relu(%261) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %263 = nn.conv2d(%262, meta[relay.Constant][156] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %264 = add(%263, meta[relay.Constant][157] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %265 = nn.relu(%264) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %266 = nn.conv2d(%262, meta[relay.Constant][158] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %267 = add(%266, meta[relay.Constant][159] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %268 = nn.relu(%267) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %269 = (%265, %268);
  %270 = concatenate(%269, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %271 = nn.conv2d(%256, meta[relay.Constant][160] /* ty=Tensor[(448, 1280, 1, 1), float32] */ /* ty=Tensor[(448, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %272 = add(%271, meta[relay.Constant][161] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %273 = nn.relu(%272) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %274 = nn.conv2d(%273, meta[relay.Constant][162] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %275 = add(%274, meta[relay.Constant][163] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %276 = nn.relu(%275) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %277 = nn.conv2d(%276, meta[relay.Constant][164] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %278 = add(%277, meta[relay.Constant][165] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %279 = nn.relu(%278) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %280 = nn.conv2d(%276, meta[relay.Constant][166] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %281 = add(%280, meta[relay.Constant][167] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %282 = nn.relu(%281) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %283 = (%279, %282);
  %284 = concatenate(%283, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %285 = nn.avg_pool2d(%256, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %286 = nn.conv2d(%285, meta[relay.Constant][168] /* ty=Tensor[(192, 1280, 1, 1), float32] */ /* ty=Tensor[(192, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %287 = add(%286, meta[relay.Constant][169] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %288 = nn.relu(%287) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %289 = (%259, %270, %284, %288);
  %290 = concatenate(%289, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %291 = nn.conv2d(%290, meta[relay.Constant][170] /* ty=Tensor[(320, 2048, 1, 1), float32] */ /* ty=Tensor[(320, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %292 = add(%291, meta[relay.Constant][171] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %293 = nn.relu(%292) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %294 = nn.conv2d(%290, meta[relay.Constant][172] /* ty=Tensor[(384, 2048, 1, 1), float32] */ /* ty=Tensor[(384, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %295 = add(%294, meta[relay.Constant][173] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %296 = nn.relu(%295) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %297 = nn.conv2d(%296, meta[relay.Constant][174] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %298 = add(%297, meta[relay.Constant][175] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %299 = nn.relu(%298) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %300 = nn.conv2d(%296, meta[relay.Constant][176] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %301 = add(%300, meta[relay.Constant][177] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %302 = nn.relu(%301) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %303 = (%299, %302);
  %304 = concatenate(%303, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %305 = nn.conv2d(%290, meta[relay.Constant][178] /* ty=Tensor[(448, 2048, 1, 1), float32] */ /* ty=Tensor[(448, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %306 = add(%305, meta[relay.Constant][179] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %307 = nn.relu(%306) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %308 = nn.conv2d(%307, meta[relay.Constant][180] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %309 = add(%308, meta[relay.Constant][181] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %310 = nn.relu(%309) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %311 = nn.conv2d(%310, meta[relay.Constant][182] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %312 = add(%311, meta[relay.Constant][183] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %313 = nn.relu(%312) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %314 = nn.conv2d(%310, meta[relay.Constant][184] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %315 = add(%314, meta[relay.Constant][185] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %316 = nn.relu(%315) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %317 = (%313, %316);
  %318 = concatenate(%317, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %319 = nn.avg_pool2d(%290, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %320 = nn.conv2d(%319, meta[relay.Constant][186] /* ty=Tensor[(192, 2048, 1, 1), float32] */ /* ty=Tensor[(192, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %321 = add(%320, meta[relay.Constant][187] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %322 = nn.relu(%321) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %323 = (%293, %304, %318, %322);
  %324 = concatenate(%323, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %325 = nn.avg_pool2d(%324, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %326 = nn.batch_flatten(%325) /* ty=Tensor[(32, 2048), float32] */;
  %327 = nn.dense(%326, meta[relay.Constant][188] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%327, meta[relay.Constant][189] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:autotvm:Finish loading 688 records
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 3, 299, 299), 'float32'), ('TENSOR', (32, 3, 3, 3), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 32, 149, 149), 'float32'), ('TENSOR', (32, 32, 3, 3), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 32, 147, 147), 'float32'), ('TENSOR', (64, 32, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 64, 73, 73), 'float32'), ('TENSOR', (80, 64, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 80, 73, 73), 'float32'), ('TENSOR', (192, 80, 3, 3), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 192, 35, 35), 'float32'), ('TENSOR', (64, 192, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 192, 35, 35), 'float32'), ('TENSOR', (48, 192, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 48, 35, 35), 'float32'), ('TENSOR', (64, 48, 5, 5), 'float32'), (1, 1), (2, 2, 2, 2), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 64, 35, 35), 'float32'), ('TENSOR', (96, 64, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 96, 35, 35), 'float32'), ('TENSOR', (96, 96, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 192, 35, 35), 'float32'), ('TENSOR', (32, 192, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 256, 35, 35), 'float32'), ('TENSOR', (64, 256, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 256, 35, 35), 'float32'), ('TENSOR', (48, 256, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 288, 35, 35), 'float32'), ('TENSOR', (64, 288, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 288, 35, 35), 'float32'), ('TENSOR', (48, 288, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 288, 35, 35), 'float32'), ('TENSOR', (384, 288, 3, 3), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 96, 35, 35), 'float32'), ('TENSOR', (96, 96, 3, 3), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 768, 17, 17), 'float32'), ('TENSOR', (192, 768, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 768, 17, 17), 'float32'), ('TENSOR', (128, 768, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 128, 17, 17), 'float32'), ('TENSOR', (128, 128, 1, 7), 'float32'), (1, 1), (0, 3, 0, 3), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 128, 17, 17), 'float32'), ('TENSOR', (192, 128, 7, 1), 'float32'), (1, 1), (3, 0, 3, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 128, 17, 17), 'float32'), ('TENSOR', (128, 128, 7, 1), 'float32'), (1, 1), (3, 0, 3, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 128, 17, 17), 'float32'), ('TENSOR', (192, 128, 1, 7), 'float32'), (1, 1), (0, 3, 0, 3), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 768, 17, 17), 'float32'), ('TENSOR', (160, 768, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 160, 17, 17), 'float32'), ('TENSOR', (160, 160, 1, 7), 'float32'), (1, 1), (0, 3, 0, 3), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 160, 17, 17), 'float32'), ('TENSOR', (192, 160, 7, 1), 'float32'), (1, 1), (3, 0, 3, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 160, 17, 17), 'float32'), ('TENSOR', (160, 160, 7, 1), 'float32'), (1, 1), (3, 0, 3, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 160, 17, 17), 'float32'), ('TENSOR', (192, 160, 1, 7), 'float32'), (1, 1), (0, 3, 0, 3), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 192, 17, 17), 'float32'), ('TENSOR', (192, 192, 1, 7), 'float32'), (1, 1), (0, 3, 0, 3), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 192, 17, 17), 'float32'), ('TENSOR', (192, 192, 7, 1), 'float32'), (1, 1), (3, 0, 3, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 192, 17, 17), 'float32'), ('TENSOR', (320, 192, 3, 3), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 192, 17, 17), 'float32'), ('TENSOR', (192, 192, 3, 3), 'float32'), (2, 2), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 1280, 8, 8), 'float32'), ('TENSOR', (320, 1280, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 1280, 8, 8), 'float32'), ('TENSOR', (384, 1280, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 384, 8, 8), 'float32'), ('TENSOR', (384, 384, 1, 3), 'float32'), (1, 1), (0, 1, 0, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 384, 8, 8), 'float32'), ('TENSOR', (384, 384, 3, 1), 'float32'), (1, 1), (1, 0, 1, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 1280, 8, 8), 'float32'), ('TENSOR', (448, 1280, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 448, 8, 8), 'float32'), ('TENSOR', (384, 448, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 1280, 8, 8), 'float32'), ('TENSOR', (192, 1280, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 2048, 8, 8), 'float32'), ('TENSOR', (320, 2048, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 2048, 8, 8), 'float32'), ('TENSOR', (384, 2048, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 2048, 8, 8), 'float32'), ('TENSOR', (448, 2048, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_nchw.cuda', ('TENSOR', (32, 2048, 8, 8), 'float32'), ('TENSOR', (192, 2048, 1, 1), 'float32'), (1, 1), (0, 0, 0, 0), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
learning_based_quantize.py:100: DeprecationWarning: legacy graph runtime behaviour of producing json / lib / params will be removed in the next release 
  graph, lib, params = relay.build(mod, target)
INFO:root:[3200 samples] validation: acc-top1=0.780625 acc-top5=0.943125
INFO:root:[6400 samples] validation: acc-top1=0.771250 acc-top5=0.938906
INFO:root:[9600 samples] validation: acc-top1=0.773542 acc-top5=0.934063
INFO:root:[12800 samples] validation: acc-top1=0.769766 acc-top5=0.932187
INFO:root:[16000 samples] validation: acc-top1=0.770062 acc-top5=0.932688
INFO:root:[19200 samples] validation: acc-top1=0.768125 acc-top5=0.932240
INFO:root:[22400 samples] validation: acc-top1=0.768750 acc-top5=0.931652
INFO:root:[25600 samples] validation: acc-top1=0.768047 acc-top5=0.931914
INFO:root:[28800 samples] validation: acc-top1=0.768472 acc-top5=0.931806
INFO:root:[32000 samples] validation: acc-top1=0.767750 acc-top5=0.931438
INFO:root:[35200 samples] validation: acc-top1=0.768437 acc-top5=0.931847
INFO:root:[38400 samples] validation: acc-top1=0.768464 acc-top5=0.931823
INFO:root:[41600 samples] validation: acc-top1=0.767933 acc-top5=0.931851
INFO:root:[44800 samples] validation: acc-top1=0.767656 acc-top5=0.931942
INFO:root:[48000 samples] validation: acc-top1=0.767917 acc-top5=0.932417
INFO:root:[final] validation: acc-top1=0.767917 acc-top5=0.932417
Final accuracy fp32 0.7679166666666667
DEBUG:root:original
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 299, 299), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(32, 3, 3, 3), float32] */ /* ty=Tensor[(32, 3, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %3 = nn.conv2d(%2, meta[relay.Constant][2] /* ty=Tensor[(32, 32, 3, 3), float32] */ /* ty=Tensor[(32, 32, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %4 = add(%3, meta[relay.Constant][3] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %5 = nn.relu(%4) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %6 = nn.conv2d(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */ /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %7 = add(%6, meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %8 = nn.relu(%7) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 64, 73, 73), float32] */;
  %10 = nn.conv2d(%9, meta[relay.Constant][6] /* ty=Tensor[(80, 64, 1, 1), float32] */ /* ty=Tensor[(80, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=80, kernel_size=[1, 1]) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %11 = add(%10, meta[relay.Constant][7] /* ty=Tensor[(80, 1, 1), float32] */ /* ty=Tensor[(80, 1, 1), float32] */) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %12 = nn.relu(%11) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %13 = nn.conv2d(%12, meta[relay.Constant][8] /* ty=Tensor[(192, 80, 3, 3), float32] */ /* ty=Tensor[(192, 80, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %14 = add(%13, meta[relay.Constant][9] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %15 = nn.relu(%14) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %16 = nn.max_pool2d(%15, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %17 = nn.conv2d(%16, meta[relay.Constant][10] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %18 = add(%17, meta[relay.Constant][11] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %19 = nn.relu(%18) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %20 = nn.conv2d(%16, meta[relay.Constant][12] /* ty=Tensor[(48, 192, 1, 1), float32] */ /* ty=Tensor[(48, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %21 = add(%20, meta[relay.Constant][13] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %22 = nn.relu(%21) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %23 = nn.conv2d(%22, meta[relay.Constant][14] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %24 = add(%23, meta[relay.Constant][15] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %25 = nn.relu(%24) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %26 = nn.conv2d(%16, meta[relay.Constant][16] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %27 = add(%26, meta[relay.Constant][17] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %29 = nn.conv2d(%28, meta[relay.Constant][18] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %30 = add(%29, meta[relay.Constant][19] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %31 = nn.relu(%30) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %32 = nn.conv2d(%31, meta[relay.Constant][20] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %33 = add(%32, meta[relay.Constant][21] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %34 = nn.relu(%33) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %35 = nn.avg_pool2d(%16, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %36 = nn.conv2d(%35, meta[relay.Constant][22] /* ty=Tensor[(32, 192, 1, 1), float32] */ /* ty=Tensor[(32, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %37 = add(%36, meta[relay.Constant][23] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %38 = nn.relu(%37) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %39 = (%19, %25, %34, %38);
  %40 = concatenate(%39, axis=1) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %41 = nn.conv2d(%40, meta[relay.Constant][24] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %42 = add(%41, meta[relay.Constant][25] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %43 = nn.relu(%42) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %44 = nn.conv2d(%40, meta[relay.Constant][26] /* ty=Tensor[(48, 256, 1, 1), float32] */ /* ty=Tensor[(48, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %45 = add(%44, meta[relay.Constant][27] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %46 = nn.relu(%45) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %47 = nn.conv2d(%46, meta[relay.Constant][28] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %48 = add(%47, meta[relay.Constant][29] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %49 = nn.relu(%48) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %50 = nn.conv2d(%40, meta[relay.Constant][30] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %51 = add(%50, meta[relay.Constant][31] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %52 = nn.relu(%51) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %53 = nn.conv2d(%52, meta[relay.Constant][32] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %54 = add(%53, meta[relay.Constant][33] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %55 = nn.relu(%54) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %56 = nn.conv2d(%55, meta[relay.Constant][34] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %57 = add(%56, meta[relay.Constant][35] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %59 = nn.avg_pool2d(%40, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %60 = nn.conv2d(%59, meta[relay.Constant][36] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %61 = add(%60, meta[relay.Constant][37] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %63 = (%43, %49, %58, %62);
  %64 = concatenate(%63, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %65 = nn.conv2d(%64, meta[relay.Constant][38] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %66 = add(%65, meta[relay.Constant][39] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %68 = nn.conv2d(%64, meta[relay.Constant][40] /* ty=Tensor[(48, 288, 1, 1), float32] */ /* ty=Tensor[(48, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %69 = add(%68, meta[relay.Constant][41] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %70 = nn.relu(%69) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %71 = nn.conv2d(%70, meta[relay.Constant][42] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %72 = add(%71, meta[relay.Constant][43] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %73 = nn.relu(%72) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %74 = nn.conv2d(%64, meta[relay.Constant][44] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %75 = add(%74, meta[relay.Constant][45] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %76 = nn.relu(%75) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %77 = nn.conv2d(%76, meta[relay.Constant][46] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %78 = add(%77, meta[relay.Constant][47] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %79 = nn.relu(%78) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %80 = nn.conv2d(%79, meta[relay.Constant][48] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %81 = add(%80, meta[relay.Constant][49] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %82 = nn.relu(%81) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %83 = nn.avg_pool2d(%64, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %84 = nn.conv2d(%83, meta[relay.Constant][50] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %85 = add(%84, meta[relay.Constant][51] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %86 = nn.relu(%85) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %87 = (%67, %73, %82, %86);
  %88 = concatenate(%87, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %89 = nn.conv2d(%88, meta[relay.Constant][52] /* ty=Tensor[(384, 288, 3, 3), float32] */ /* ty=Tensor[(384, 288, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %90 = add(%89, meta[relay.Constant][53] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %92 = nn.conv2d(%88, meta[relay.Constant][54] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %93 = add(%92, meta[relay.Constant][55] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %94 = nn.relu(%93) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %95 = nn.conv2d(%94, meta[relay.Constant][56] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %96 = add(%95, meta[relay.Constant][57] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %97 = nn.relu(%96) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %98 = nn.conv2d(%97, meta[relay.Constant][58] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %99 = add(%98, meta[relay.Constant][59] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %100 = nn.relu(%99) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %101 = nn.max_pool2d(%88, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 288, 17, 17), float32] */;
  %102 = (%91, %100, %101);
  %103 = concatenate(%102, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %104 = nn.conv2d(%103, meta[relay.Constant][60] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %105 = add(%104, meta[relay.Constant][61] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %106 = nn.relu(%105) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %107 = nn.conv2d(%103, meta[relay.Constant][62] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %108 = add(%107, meta[relay.Constant][63] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %109 = nn.relu(%108) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %110 = nn.conv2d(%109, meta[relay.Constant][64] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %111 = add(%110, meta[relay.Constant][65] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %112 = nn.relu(%111) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %113 = nn.conv2d(%112, meta[relay.Constant][66] /* ty=Tensor[(192, 128, 7, 1), float32] */ /* ty=Tensor[(192, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %114 = add(%113, meta[relay.Constant][67] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %115 = nn.relu(%114) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %116 = nn.conv2d(%103, meta[relay.Constant][68] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %117 = add(%116, meta[relay.Constant][69] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %118 = nn.relu(%117) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %119 = nn.conv2d(%118, meta[relay.Constant][70] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %120 = add(%119, meta[relay.Constant][71] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %121 = nn.relu(%120) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %122 = nn.conv2d(%121, meta[relay.Constant][72] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %123 = add(%122, meta[relay.Constant][73] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %125 = nn.conv2d(%124, meta[relay.Constant][74] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %126 = add(%125, meta[relay.Constant][75] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %127 = nn.relu(%126) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %128 = nn.conv2d(%127, meta[relay.Constant][76] /* ty=Tensor[(192, 128, 1, 7), float32] */ /* ty=Tensor[(192, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %129 = add(%128, meta[relay.Constant][77] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %130 = nn.relu(%129) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %131 = nn.avg_pool2d(%103, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %132 = nn.conv2d(%131, meta[relay.Constant][78] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %133 = add(%132, meta[relay.Constant][79] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %134 = nn.relu(%133) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %135 = (%106, %115, %130, %134);
  %136 = concatenate(%135, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %137 = nn.conv2d(%136, meta[relay.Constant][80] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %138 = add(%137, meta[relay.Constant][81] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %139 = nn.relu(%138) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %140 = nn.conv2d(%136, meta[relay.Constant][82] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %141 = add(%140, meta[relay.Constant][83] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %142 = nn.relu(%141) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %143 = nn.conv2d(%142, meta[relay.Constant][84] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %144 = add(%143, meta[relay.Constant][85] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %145 = nn.relu(%144) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %146 = nn.conv2d(%145, meta[relay.Constant][86] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %147 = add(%146, meta[relay.Constant][87] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %148 = nn.relu(%147) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %149 = nn.conv2d(%136, meta[relay.Constant][88] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %150 = add(%149, meta[relay.Constant][89] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %151 = nn.relu(%150) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %152 = nn.conv2d(%151, meta[relay.Constant][90] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %153 = add(%152, meta[relay.Constant][91] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %154 = nn.relu(%153) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %155 = nn.conv2d(%154, meta[relay.Constant][92] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %156 = add(%155, meta[relay.Constant][93] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %158 = nn.conv2d(%157, meta[relay.Constant][94] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %159 = add(%158, meta[relay.Constant][95] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %161 = nn.conv2d(%160, meta[relay.Constant][96] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %162 = add(%161, meta[relay.Constant][97] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %163 = nn.relu(%162) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %164 = nn.avg_pool2d(%136, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %165 = nn.conv2d(%164, meta[relay.Constant][98] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %166 = add(%165, meta[relay.Constant][99] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %167 = nn.relu(%166) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %168 = (%139, %148, %163, %167);
  %169 = concatenate(%168, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %170 = nn.conv2d(%169, meta[relay.Constant][100] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %171 = add(%170, meta[relay.Constant][101] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %173 = nn.conv2d(%169, meta[relay.Constant][102] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %174 = add(%173, meta[relay.Constant][103] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %175 = nn.relu(%174) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %176 = nn.conv2d(%175, meta[relay.Constant][104] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %177 = add(%176, meta[relay.Constant][105] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %178 = nn.relu(%177) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %179 = nn.conv2d(%178, meta[relay.Constant][106] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %180 = add(%179, meta[relay.Constant][107] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %181 = nn.relu(%180) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %182 = nn.conv2d(%169, meta[relay.Constant][108] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %183 = add(%182, meta[relay.Constant][109] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %184 = nn.relu(%183) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %185 = nn.conv2d(%184, meta[relay.Constant][110] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %186 = add(%185, meta[relay.Constant][111] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %187 = nn.relu(%186) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %188 = nn.conv2d(%187, meta[relay.Constant][112] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %189 = add(%188, meta[relay.Constant][113] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %190 = nn.relu(%189) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %191 = nn.conv2d(%190, meta[relay.Constant][114] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %192 = add(%191, meta[relay.Constant][115] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %193 = nn.relu(%192) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %194 = nn.conv2d(%193, meta[relay.Constant][116] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %195 = add(%194, meta[relay.Constant][117] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %196 = nn.relu(%195) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %197 = nn.avg_pool2d(%169, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %198 = nn.conv2d(%197, meta[relay.Constant][118] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %199 = add(%198, meta[relay.Constant][119] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %200 = nn.relu(%199) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %201 = (%172, %181, %196, %200);
  %202 = concatenate(%201, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %203 = nn.conv2d(%202, meta[relay.Constant][120] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %204 = add(%203, meta[relay.Constant][121] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %206 = nn.conv2d(%202, meta[relay.Constant][122] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %207 = add(%206, meta[relay.Constant][123] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %208 = nn.relu(%207) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %209 = nn.conv2d(%208, meta[relay.Constant][124] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %210 = add(%209, meta[relay.Constant][125] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %211 = nn.relu(%210) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %212 = nn.conv2d(%211, meta[relay.Constant][126] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %213 = add(%212, meta[relay.Constant][127] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %214 = nn.relu(%213) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %215 = nn.conv2d(%202, meta[relay.Constant][128] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %216 = add(%215, meta[relay.Constant][129] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %217 = nn.relu(%216) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %218 = nn.conv2d(%217, meta[relay.Constant][130] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %219 = add(%218, meta[relay.Constant][131] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %220 = nn.relu(%219) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %221 = nn.conv2d(%220, meta[relay.Constant][132] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %222 = add(%221, meta[relay.Constant][133] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %223 = nn.relu(%222) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %224 = nn.conv2d(%223, meta[relay.Constant][134] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %225 = add(%224, meta[relay.Constant][135] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %226 = nn.relu(%225) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %227 = nn.conv2d(%226, meta[relay.Constant][136] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %228 = add(%227, meta[relay.Constant][137] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %229 = nn.relu(%228) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %230 = nn.avg_pool2d(%202, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %231 = nn.conv2d(%230, meta[relay.Constant][138] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %232 = add(%231, meta[relay.Constant][139] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %233 = nn.relu(%232) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %234 = (%205, %214, %229, %233);
  %235 = concatenate(%234, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %236 = nn.conv2d(%235, meta[relay.Constant][140] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %237 = add(%236, meta[relay.Constant][141] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %238 = nn.relu(%237) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %239 = nn.conv2d(%238, meta[relay.Constant][142] /* ty=Tensor[(320, 192, 3, 3), float32] */ /* ty=Tensor[(320, 192, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=320, kernel_size=[3, 3]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %240 = add(%239, meta[relay.Constant][143] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %241 = nn.relu(%240) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %242 = nn.conv2d(%235, meta[relay.Constant][144] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %243 = add(%242, meta[relay.Constant][145] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %244 = nn.relu(%243) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %245 = nn.conv2d(%244, meta[relay.Constant][146] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %246 = add(%245, meta[relay.Constant][147] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %247 = nn.relu(%246) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %248 = nn.conv2d(%247, meta[relay.Constant][148] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %249 = add(%248, meta[relay.Constant][149] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %250 = nn.relu(%249) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %251 = nn.conv2d(%250, meta[relay.Constant][150] /* ty=Tensor[(192, 192, 3, 3), float32] */ /* ty=Tensor[(192, 192, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %252 = add(%251, meta[relay.Constant][151] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %253 = nn.relu(%252) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %254 = nn.max_pool2d(%235, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %255 = (%241, %253, %254);
  %256 = concatenate(%255, axis=1) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %257 = nn.conv2d(%256, meta[relay.Constant][152] /* ty=Tensor[(320, 1280, 1, 1), float32] */ /* ty=Tensor[(320, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %258 = add(%257, meta[relay.Constant][153] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %259 = nn.relu(%258) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %260 = nn.conv2d(%256, meta[relay.Constant][154] /* ty=Tensor[(384, 1280, 1, 1), float32] */ /* ty=Tensor[(384, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %261 = add(%260, meta[relay.Constant][155] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %262 = nn.relu(%261) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %263 = nn.conv2d(%262, meta[relay.Constant][156] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %264 = add(%263, meta[relay.Constant][157] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %265 = nn.relu(%264) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %266 = nn.conv2d(%262, meta[relay.Constant][158] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %267 = add(%266, meta[relay.Constant][159] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %268 = nn.relu(%267) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %269 = (%265, %268);
  %270 = concatenate(%269, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %271 = nn.conv2d(%256, meta[relay.Constant][160] /* ty=Tensor[(448, 1280, 1, 1), float32] */ /* ty=Tensor[(448, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %272 = add(%271, meta[relay.Constant][161] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %273 = nn.relu(%272) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %274 = nn.conv2d(%273, meta[relay.Constant][162] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %275 = add(%274, meta[relay.Constant][163] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %276 = nn.relu(%275) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %277 = nn.conv2d(%276, meta[relay.Constant][164] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %278 = add(%277, meta[relay.Constant][165] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %279 = nn.relu(%278) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %280 = nn.conv2d(%276, meta[relay.Constant][166] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %281 = add(%280, meta[relay.Constant][167] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %282 = nn.relu(%281) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %283 = (%279, %282);
  %284 = concatenate(%283, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %285 = nn.avg_pool2d(%256, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %286 = nn.conv2d(%285, meta[relay.Constant][168] /* ty=Tensor[(192, 1280, 1, 1), float32] */ /* ty=Tensor[(192, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %287 = add(%286, meta[relay.Constant][169] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %288 = nn.relu(%287) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %289 = (%259, %270, %284, %288);
  %290 = concatenate(%289, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %291 = nn.conv2d(%290, meta[relay.Constant][170] /* ty=Tensor[(320, 2048, 1, 1), float32] */ /* ty=Tensor[(320, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %292 = add(%291, meta[relay.Constant][171] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %293 = nn.relu(%292) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %294 = nn.conv2d(%290, meta[relay.Constant][172] /* ty=Tensor[(384, 2048, 1, 1), float32] */ /* ty=Tensor[(384, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %295 = add(%294, meta[relay.Constant][173] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %296 = nn.relu(%295) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %297 = nn.conv2d(%296, meta[relay.Constant][174] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %298 = add(%297, meta[relay.Constant][175] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %299 = nn.relu(%298) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %300 = nn.conv2d(%296, meta[relay.Constant][176] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %301 = add(%300, meta[relay.Constant][177] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %302 = nn.relu(%301) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %303 = (%299, %302);
  %304 = concatenate(%303, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %305 = nn.conv2d(%290, meta[relay.Constant][178] /* ty=Tensor[(448, 2048, 1, 1), float32] */ /* ty=Tensor[(448, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %306 = add(%305, meta[relay.Constant][179] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %307 = nn.relu(%306) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %308 = nn.conv2d(%307, meta[relay.Constant][180] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %309 = add(%308, meta[relay.Constant][181] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %310 = nn.relu(%309) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %311 = nn.conv2d(%310, meta[relay.Constant][182] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %312 = add(%311, meta[relay.Constant][183] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %313 = nn.relu(%312) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %314 = nn.conv2d(%310, meta[relay.Constant][184] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %315 = add(%314, meta[relay.Constant][185] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %316 = nn.relu(%315) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %317 = (%313, %316);
  %318 = concatenate(%317, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %319 = nn.avg_pool2d(%290, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %320 = nn.conv2d(%319, meta[relay.Constant][186] /* ty=Tensor[(192, 2048, 1, 1), float32] */ /* ty=Tensor[(192, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %321 = add(%320, meta[relay.Constant][187] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %322 = nn.relu(%321) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %323 = (%293, %304, %318, %322);
  %324 = concatenate(%323, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %325 = nn.avg_pool2d(%324, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %326 = nn.batch_flatten(%325) /* ty=Tensor[(32, 2048), float32] */;
  %327 = nn.dense(%326, meta[relay.Constant][188] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%327, meta[relay.Constant][189] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:root:current quantize config
DEBUG:root:<tvm.hago.base.QConfig object at 0x7fa89e7e92e0>
data
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
nn.avg_pool2d
nn.batch_flatten
constant
nn.dense
constant
add
data -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.max_pool2d -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.max_pool2d
nn.relu -> concatenate
nn.relu -> concatenate
nn.max_pool2d -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.max_pool2d
nn.relu -> concatenate
nn.relu -> concatenate
nn.max_pool2d -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
concatenate -> concatenate
concatenate -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
concatenate -> concatenate
concatenate -> concatenate
nn.relu -> concatenate
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.batch_flatten
nn.batch_flatten -> nn.dense
constant -> nn.dense
nn.dense -> add
constant -> add
analyzed condition
node_conds: [False, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, False, True, False, True, True, True, False, False, False, False, False, False]
edge_conds: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]
bit limit
[8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32]
--------
nn.conv2d[%2]: [32]
  data[%0] -> nn.conv2d[%2] : 8
  constant[%1] -> nn.conv2d[%2] : 8
--------
add[%4]: [32]
  nn.conv2d[%2] -> add[%4] : 32
  constant[%3] -> add[%4] : 32
--------
nn.relu[%5]: [8]
  add[%4] -> nn.relu[%5] : 32
--------
nn.conv2d[%7]: [32]
  nn.relu[%5] -> nn.conv2d[%7] : 8
  constant[%6] -> nn.conv2d[%7] : 8
--------
add[%9]: [32]
  nn.conv2d[%7] -> add[%9] : 32
  constant[%8] -> add[%9] : 32
--------
nn.relu[%10]: [8]
  add[%9] -> nn.relu[%10] : 32
--------
nn.conv2d[%12]: [32]
  nn.relu[%10] -> nn.conv2d[%12] : 8
  constant[%11] -> nn.conv2d[%12] : 8
--------
add[%14]: [32]
  nn.conv2d[%12] -> add[%14] : 32
  constant[%13] -> add[%14] : 32
--------
nn.relu[%15]: [32]
  add[%14] -> nn.relu[%15] : 32
--------
nn.max_pool2d[%16]: [8]
  nn.relu[%15] -> nn.max_pool2d[%16] : 32
--------
nn.conv2d[%18]: [32]
  nn.max_pool2d[%16] -> nn.conv2d[%18] : 8
  constant[%17] -> nn.conv2d[%18] : 8
--------
add[%20]: [32]
  nn.conv2d[%18] -> add[%20] : 32
  constant[%19] -> add[%20] : 32
--------
nn.relu[%21]: [8]
  add[%20] -> nn.relu[%21] : 32
--------
nn.conv2d[%23]: [32]
  nn.relu[%21] -> nn.conv2d[%23] : 8
  constant[%22] -> nn.conv2d[%23] : 8
--------
add[%25]: [32]
  nn.conv2d[%23] -> add[%25] : 32
  constant[%24] -> add[%25] : 32
--------
nn.relu[%26]: [32]
  add[%25] -> nn.relu[%26] : 32
--------
nn.max_pool2d[%27]: [8, 8, 8, None]
  nn.relu[%26] -> nn.max_pool2d[%27] : 32
--------
nn.conv2d[%29]: [32]
  nn.max_pool2d[%27] -> nn.conv2d[%29] : 8
  constant[%28] -> nn.conv2d[%29] : 8
--------
add[%31]: [32]
  nn.conv2d[%29] -> add[%31] : 32
  constant[%30] -> add[%31] : 32
--------
nn.relu[%32]: [32]
  add[%31] -> nn.relu[%32] : 32
--------
nn.conv2d[%34]: [32]
  nn.max_pool2d[%27] -> nn.conv2d[%34] : 8
  constant[%33] -> nn.conv2d[%34] : 8
--------
add[%36]: [32]
  nn.conv2d[%34] -> add[%36] : 32
  constant[%35] -> add[%36] : 32
--------
nn.relu[%37]: [8]
  add[%36] -> nn.relu[%37] : 32
--------
nn.conv2d[%39]: [32]
  nn.relu[%37] -> nn.conv2d[%39] : 8
  constant[%38] -> nn.conv2d[%39] : 8
--------
add[%41]: [32]
  nn.conv2d[%39] -> add[%41] : 32
  constant[%40] -> add[%41] : 32
--------
nn.relu[%42]: [32]
  add[%41] -> nn.relu[%42] : 32
--------
nn.conv2d[%44]: [32]
  nn.max_pool2d[%27] -> nn.conv2d[%44] : 8
  constant[%43] -> nn.conv2d[%44] : 8
--------
add[%46]: [32]
  nn.conv2d[%44] -> add[%46] : 32
  constant[%45] -> add[%46] : 32
--------
nn.relu[%47]: [8]
  add[%46] -> nn.relu[%47] : 32
--------
nn.conv2d[%49]: [32]
  nn.relu[%47] -> nn.conv2d[%49] : 8
  constant[%48] -> nn.conv2d[%49] : 8
--------
add[%51]: [32]
  nn.conv2d[%49] -> add[%51] : 32
  constant[%50] -> add[%51] : 32
--------
nn.relu[%52]: [8]
  add[%51] -> nn.relu[%52] : 32
--------
nn.conv2d[%54]: [32]
  nn.relu[%52] -> nn.conv2d[%54] : 8
  constant[%53] -> nn.conv2d[%54] : 8
--------
add[%56]: [32]
  nn.conv2d[%54] -> add[%56] : 32
  constant[%55] -> add[%56] : 32
--------
nn.relu[%57]: [32]
  add[%56] -> nn.relu[%57] : 32
--------
nn.avg_pool2d[%58]: [8]
  nn.max_pool2d[%27] -> nn.avg_pool2d[%58] : None
--------
nn.conv2d[%60]: [32]
  nn.avg_pool2d[%58] -> nn.conv2d[%60] : 8
  constant[%59] -> nn.conv2d[%60] : 8
--------
add[%62]: [32]
  nn.conv2d[%60] -> add[%62] : 32
  constant[%61] -> add[%62] : 32
--------
nn.relu[%63]: [32]
  add[%62] -> nn.relu[%63] : 32
--------
concatenate[%64]: [8, 8, 8, None]
  nn.relu[%32] -> concatenate[%64] : 32
  nn.relu[%42] -> concatenate[%64] : 32
  nn.relu[%57] -> concatenate[%64] : 32
  nn.relu[%63] -> concatenate[%64] : 32
--------
nn.conv2d[%66]: [32]
  concatenate[%64] -> nn.conv2d[%66] : 8
  constant[%65] -> nn.conv2d[%66] : 8
--------
add[%68]: [32]
  nn.conv2d[%66] -> add[%68] : 32
  constant[%67] -> add[%68] : 32
--------
nn.relu[%69]: [32]
  add[%68] -> nn.relu[%69] : 32
--------
nn.conv2d[%71]: [32]
  concatenate[%64] -> nn.conv2d[%71] : 8
  constant[%70] -> nn.conv2d[%71] : 8
--------
add[%73]: [32]
  nn.conv2d[%71] -> add[%73] : 32
  constant[%72] -> add[%73] : 32
--------
nn.relu[%74]: [8]
  add[%73] -> nn.relu[%74] : 32
--------
nn.conv2d[%76]: [32]
  nn.relu[%74] -> nn.conv2d[%76] : 8
  constant[%75] -> nn.conv2d[%76] : 8
--------
add[%78]: [32]
  nn.conv2d[%76] -> add[%78] : 32
  constant[%77] -> add[%78] : 32
--------
nn.relu[%79]: [32]
  add[%78] -> nn.relu[%79] : 32
--------
nn.conv2d[%81]: [32]
  concatenate[%64] -> nn.conv2d[%81] : 8
  constant[%80] -> nn.conv2d[%81] : 8
--------
add[%83]: [32]
  nn.conv2d[%81] -> add[%83] : 32
  constant[%82] -> add[%83] : 32
--------
nn.relu[%84]: [8]
  add[%83] -> nn.relu[%84] : 32
--------
nn.conv2d[%86]: [32]
  nn.relu[%84] -> nn.conv2d[%86] : 8
  constant[%85] -> nn.conv2d[%86] : 8
--------
add[%88]: [32]
  nn.conv2d[%86] -> add[%88] : 32
  constant[%87] -> add[%88] : 32
--------
nn.relu[%89]: [8]
  add[%88] -> nn.relu[%89] : 32
--------
nn.conv2d[%91]: [32]
  nn.relu[%89] -> nn.conv2d[%91] : 8
  constant[%90] -> nn.conv2d[%91] : 8
--------
add[%93]: [32]
  nn.conv2d[%91] -> add[%93] : 32
  constant[%92] -> add[%93] : 32
--------
nn.relu[%94]: [32]
  add[%93] -> nn.relu[%94] : 32
--------
nn.avg_pool2d[%95]: [8]
  concatenate[%64] -> nn.avg_pool2d[%95] : None
--------
nn.conv2d[%97]: [32]
  nn.avg_pool2d[%95] -> nn.conv2d[%97] : 8
  constant[%96] -> nn.conv2d[%97] : 8
--------
add[%99]: [32]
  nn.conv2d[%97] -> add[%99] : 32
  constant[%98] -> add[%99] : 32
--------
nn.relu[%100]: [32]
  add[%99] -> nn.relu[%100] : 32
--------
concatenate[%101]: [8, 8, 8, None]
  nn.relu[%69] -> concatenate[%101] : 32
  nn.relu[%79] -> concatenate[%101] : 32
  nn.relu[%94] -> concatenate[%101] : 32
  nn.relu[%100] -> concatenate[%101] : 32
--------
nn.conv2d[%103]: [32]
  concatenate[%101] -> nn.conv2d[%103] : 8
  constant[%102] -> nn.conv2d[%103] : 8
--------
add[%105]: [32]
  nn.conv2d[%103] -> add[%105] : 32
  constant[%104] -> add[%105] : 32
--------
nn.relu[%106]: [32]
  add[%105] -> nn.relu[%106] : 32
--------
nn.conv2d[%108]: [32]
  concatenate[%101] -> nn.conv2d[%108] : 8
  constant[%107] -> nn.conv2d[%108] : 8
--------
add[%110]: [32]
  nn.conv2d[%108] -> add[%110] : 32
  constant[%109] -> add[%110] : 32
--------
nn.relu[%111]: [8]
  add[%110] -> nn.relu[%111] : 32
--------
nn.conv2d[%113]: [32]
  nn.relu[%111] -> nn.conv2d[%113] : 8
  constant[%112] -> nn.conv2d[%113] : 8
--------
add[%115]: [32]
  nn.conv2d[%113] -> add[%115] : 32
  constant[%114] -> add[%115] : 32
--------
nn.relu[%116]: [32]
  add[%115] -> nn.relu[%116] : 32
--------
nn.conv2d[%118]: [32]
  concatenate[%101] -> nn.conv2d[%118] : 8
  constant[%117] -> nn.conv2d[%118] : 8
--------
add[%120]: [32]
  nn.conv2d[%118] -> add[%120] : 32
  constant[%119] -> add[%120] : 32
--------
nn.relu[%121]: [8]
  add[%120] -> nn.relu[%121] : 32
--------
nn.conv2d[%123]: [32]
  nn.relu[%121] -> nn.conv2d[%123] : 8
  constant[%122] -> nn.conv2d[%123] : 8
--------
add[%125]: [32]
  nn.conv2d[%123] -> add[%125] : 32
  constant[%124] -> add[%125] : 32
--------
nn.relu[%126]: [8]
  add[%125] -> nn.relu[%126] : 32
--------
nn.conv2d[%128]: [32]
  nn.relu[%126] -> nn.conv2d[%128] : 8
  constant[%127] -> nn.conv2d[%128] : 8
--------
add[%130]: [32]
  nn.conv2d[%128] -> add[%130] : 32
  constant[%129] -> add[%130] : 32
--------
nn.relu[%131]: [32]
  add[%130] -> nn.relu[%131] : 32
--------
nn.avg_pool2d[%132]: [8]
  concatenate[%101] -> nn.avg_pool2d[%132] : None
--------
nn.conv2d[%134]: [32]
  nn.avg_pool2d[%132] -> nn.conv2d[%134] : 8
  constant[%133] -> nn.conv2d[%134] : 8
--------
add[%136]: [32]
  nn.conv2d[%134] -> add[%136] : 32
  constant[%135] -> add[%136] : 32
--------
nn.relu[%137]: [32]
  add[%136] -> nn.relu[%137] : 32
--------
concatenate[%138]: [8, 8, 32]
  nn.relu[%106] -> concatenate[%138] : 32
  nn.relu[%116] -> concatenate[%138] : 32
  nn.relu[%131] -> concatenate[%138] : 32
  nn.relu[%137] -> concatenate[%138] : 32
--------
nn.conv2d[%140]: [32]
  concatenate[%138] -> nn.conv2d[%140] : 8
  constant[%139] -> nn.conv2d[%140] : 8
--------
add[%142]: [32]
  nn.conv2d[%140] -> add[%142] : 32
  constant[%141] -> add[%142] : 32
--------
nn.relu[%143]: [32]
  add[%142] -> nn.relu[%143] : 32
--------
nn.conv2d[%145]: [32]
  concatenate[%138] -> nn.conv2d[%145] : 8
  constant[%144] -> nn.conv2d[%145] : 8
--------
add[%147]: [32]
  nn.conv2d[%145] -> add[%147] : 32
  constant[%146] -> add[%147] : 32
--------
nn.relu[%148]: [8]
  add[%147] -> nn.relu[%148] : 32
--------
nn.conv2d[%150]: [32]
  nn.relu[%148] -> nn.conv2d[%150] : 8
  constant[%149] -> nn.conv2d[%150] : 8
--------
add[%152]: [32]
  nn.conv2d[%150] -> add[%152] : 32
  constant[%151] -> add[%152] : 32
--------
nn.relu[%153]: [8]
  add[%152] -> nn.relu[%153] : 32
--------
nn.conv2d[%155]: [32]
  nn.relu[%153] -> nn.conv2d[%155] : 8
  constant[%154] -> nn.conv2d[%155] : 8
--------
add[%157]: [32]
  nn.conv2d[%155] -> add[%157] : 32
  constant[%156] -> add[%157] : 32
--------
nn.relu[%158]: [32]
  add[%157] -> nn.relu[%158] : 32
--------
nn.max_pool2d[%159]: [32]
  concatenate[%138] -> nn.max_pool2d[%159] : 32
--------
concatenate[%160]: [8, 8, 8, None]
  nn.relu[%143] -> concatenate[%160] : 32
  nn.relu[%158] -> concatenate[%160] : 32
  nn.max_pool2d[%159] -> concatenate[%160] : 32
--------
nn.conv2d[%162]: [32]
  concatenate[%160] -> nn.conv2d[%162] : 8
  constant[%161] -> nn.conv2d[%162] : 8
--------
add[%164]: [32]
  nn.conv2d[%162] -> add[%164] : 32
  constant[%163] -> add[%164] : 32
--------
nn.relu[%165]: [32]
  add[%164] -> nn.relu[%165] : 32
--------
nn.conv2d[%167]: [32]
  concatenate[%160] -> nn.conv2d[%167] : 8
  constant[%166] -> nn.conv2d[%167] : 8
--------
add[%169]: [32]
  nn.conv2d[%167] -> add[%169] : 32
  constant[%168] -> add[%169] : 32
--------
nn.relu[%170]: [8]
  add[%169] -> nn.relu[%170] : 32
--------
nn.conv2d[%172]: [32]
  nn.relu[%170] -> nn.conv2d[%172] : 8
  constant[%171] -> nn.conv2d[%172] : 8
--------
add[%174]: [32]
  nn.conv2d[%172] -> add[%174] : 32
  constant[%173] -> add[%174] : 32
--------
nn.relu[%175]: [8]
  add[%174] -> nn.relu[%175] : 32
--------
nn.conv2d[%177]: [32]
  nn.relu[%175] -> nn.conv2d[%177] : 8
  constant[%176] -> nn.conv2d[%177] : 8
--------
add[%179]: [32]
  nn.conv2d[%177] -> add[%179] : 32
  constant[%178] -> add[%179] : 32
--------
nn.relu[%180]: [32]
  add[%179] -> nn.relu[%180] : 32
--------
nn.conv2d[%182]: [32]
  concatenate[%160] -> nn.conv2d[%182] : 8
  constant[%181] -> nn.conv2d[%182] : 8
--------
add[%184]: [32]
  nn.conv2d[%182] -> add[%184] : 32
  constant[%183] -> add[%184] : 32
--------
nn.relu[%185]: [8]
  add[%184] -> nn.relu[%185] : 32
--------
nn.conv2d[%187]: [32]
  nn.relu[%185] -> nn.conv2d[%187] : 8
  constant[%186] -> nn.conv2d[%187] : 8
--------
add[%189]: [32]
  nn.conv2d[%187] -> add[%189] : 32
  constant[%188] -> add[%189] : 32
--------
nn.relu[%190]: [8]
  add[%189] -> nn.relu[%190] : 32
--------
nn.conv2d[%192]: [32]
  nn.relu[%190] -> nn.conv2d[%192] : 8
  constant[%191] -> nn.conv2d[%192] : 8
--------
add[%194]: [32]
  nn.conv2d[%192] -> add[%194] : 32
  constant[%193] -> add[%194] : 32
--------
nn.relu[%195]: [8]
  add[%194] -> nn.relu[%195] : 32
--------
nn.conv2d[%197]: [32]
  nn.relu[%195] -> nn.conv2d[%197] : 8
  constant[%196] -> nn.conv2d[%197] : 8
--------
add[%199]: [32]
  nn.conv2d[%197] -> add[%199] : 32
  constant[%198] -> add[%199] : 32
--------
nn.relu[%200]: [8]
  add[%199] -> nn.relu[%200] : 32
--------
nn.conv2d[%202]: [32]
  nn.relu[%200] -> nn.conv2d[%202] : 8
  constant[%201] -> nn.conv2d[%202] : 8
--------
add[%204]: [32]
  nn.conv2d[%202] -> add[%204] : 32
  constant[%203] -> add[%204] : 32
--------
nn.relu[%205]: [32]
  add[%204] -> nn.relu[%205] : 32
--------
nn.avg_pool2d[%206]: [8]
  concatenate[%160] -> nn.avg_pool2d[%206] : None
--------
nn.conv2d[%208]: [32]
  nn.avg_pool2d[%206] -> nn.conv2d[%208] : 8
  constant[%207] -> nn.conv2d[%208] : 8
--------
add[%210]: [32]
  nn.conv2d[%208] -> add[%210] : 32
  constant[%209] -> add[%210] : 32
--------
nn.relu[%211]: [32]
  add[%210] -> nn.relu[%211] : 32
--------
concatenate[%212]: [8, 8, 8, None]
  nn.relu[%165] -> concatenate[%212] : 32
  nn.relu[%180] -> concatenate[%212] : 32
  nn.relu[%205] -> concatenate[%212] : 32
  nn.relu[%211] -> concatenate[%212] : 32
--------
nn.conv2d[%214]: [32]
  concatenate[%212] -> nn.conv2d[%214] : 8
  constant[%213] -> nn.conv2d[%214] : 8
--------
add[%216]: [32]
  nn.conv2d[%214] -> add[%216] : 32
  constant[%215] -> add[%216] : 32
--------
nn.relu[%217]: [32]
  add[%216] -> nn.relu[%217] : 32
--------
nn.conv2d[%219]: [32]
  concatenate[%212] -> nn.conv2d[%219] : 8
  constant[%218] -> nn.conv2d[%219] : 8
--------
add[%221]: [32]
  nn.conv2d[%219] -> add[%221] : 32
  constant[%220] -> add[%221] : 32
--------
nn.relu[%222]: [8]
  add[%221] -> nn.relu[%222] : 32
--------
nn.conv2d[%224]: [32]
  nn.relu[%222] -> nn.conv2d[%224] : 8
  constant[%223] -> nn.conv2d[%224] : 8
--------
add[%226]: [32]
  nn.conv2d[%224] -> add[%226] : 32
  constant[%225] -> add[%226] : 32
--------
nn.relu[%227]: [8]
  add[%226] -> nn.relu[%227] : 32
--------
nn.conv2d[%229]: [32]
  nn.relu[%227] -> nn.conv2d[%229] : 8
  constant[%228] -> nn.conv2d[%229] : 8
--------
add[%231]: [32]
  nn.conv2d[%229] -> add[%231] : 32
  constant[%230] -> add[%231] : 32
--------
nn.relu[%232]: [32]
  add[%231] -> nn.relu[%232] : 32
--------
nn.conv2d[%234]: [32]
  concatenate[%212] -> nn.conv2d[%234] : 8
  constant[%233] -> nn.conv2d[%234] : 8
--------
add[%236]: [32]
  nn.conv2d[%234] -> add[%236] : 32
  constant[%235] -> add[%236] : 32
--------
nn.relu[%237]: [8]
  add[%236] -> nn.relu[%237] : 32
--------
nn.conv2d[%239]: [32]
  nn.relu[%237] -> nn.conv2d[%239] : 8
  constant[%238] -> nn.conv2d[%239] : 8
--------
add[%241]: [32]
  nn.conv2d[%239] -> add[%241] : 32
  constant[%240] -> add[%241] : 32
--------
nn.relu[%242]: [8]
  add[%241] -> nn.relu[%242] : 32
--------
nn.conv2d[%244]: [32]
  nn.relu[%242] -> nn.conv2d[%244] : 8
  constant[%243] -> nn.conv2d[%244] : 8
--------
add[%246]: [32]
  nn.conv2d[%244] -> add[%246] : 32
  constant[%245] -> add[%246] : 32
--------
nn.relu[%247]: [8]
  add[%246] -> nn.relu[%247] : 32
--------
nn.conv2d[%249]: [32]
  nn.relu[%247] -> nn.conv2d[%249] : 8
  constant[%248] -> nn.conv2d[%249] : 8
--------
add[%251]: [32]
  nn.conv2d[%249] -> add[%251] : 32
  constant[%250] -> add[%251] : 32
--------
nn.relu[%252]: [8]
  add[%251] -> nn.relu[%252] : 32
--------
nn.conv2d[%254]: [32]
  nn.relu[%252] -> nn.conv2d[%254] : 8
  constant[%253] -> nn.conv2d[%254] : 8
--------
add[%256]: [32]
  nn.conv2d[%254] -> add[%256] : 32
  constant[%255] -> add[%256] : 32
--------
nn.relu[%257]: [32]
  add[%256] -> nn.relu[%257] : 32
--------
nn.avg_pool2d[%258]: [8]
  concatenate[%212] -> nn.avg_pool2d[%258] : None
--------
nn.conv2d[%260]: [32]
  nn.avg_pool2d[%258] -> nn.conv2d[%260] : 8
  constant[%259] -> nn.conv2d[%260] : 8
--------
add[%262]: [32]
  nn.conv2d[%260] -> add[%262] : 32
  constant[%261] -> add[%262] : 32
--------
nn.relu[%263]: [32]
  add[%262] -> nn.relu[%263] : 32
--------
concatenate[%264]: [8, 8, 8, None]
  nn.relu[%217] -> concatenate[%264] : 32
  nn.relu[%232] -> concatenate[%264] : 32
  nn.relu[%257] -> concatenate[%264] : 32
  nn.relu[%263] -> concatenate[%264] : 32
--------
nn.conv2d[%266]: [32]
  concatenate[%264] -> nn.conv2d[%266] : 8
  constant[%265] -> nn.conv2d[%266] : 8
--------
add[%268]: [32]
  nn.conv2d[%266] -> add[%268] : 32
  constant[%267] -> add[%268] : 32
--------
nn.relu[%269]: [32]
  add[%268] -> nn.relu[%269] : 32
--------
nn.conv2d[%271]: [32]
  concatenate[%264] -> nn.conv2d[%271] : 8
  constant[%270] -> nn.conv2d[%271] : 8
--------
add[%273]: [32]
  nn.conv2d[%271] -> add[%273] : 32
  constant[%272] -> add[%273] : 32
--------
nn.relu[%274]: [8]
  add[%273] -> nn.relu[%274] : 32
--------
nn.conv2d[%276]: [32]
  nn.relu[%274] -> nn.conv2d[%276] : 8
  constant[%275] -> nn.conv2d[%276] : 8
--------
add[%278]: [32]
  nn.conv2d[%276] -> add[%278] : 32
  constant[%277] -> add[%278] : 32
--------
nn.relu[%279]: [8]
  add[%278] -> nn.relu[%279] : 32
--------
nn.conv2d[%281]: [32]
  nn.relu[%279] -> nn.conv2d[%281] : 8
  constant[%280] -> nn.conv2d[%281] : 8
--------
add[%283]: [32]
  nn.conv2d[%281] -> add[%283] : 32
  constant[%282] -> add[%283] : 32
--------
nn.relu[%284]: [32]
  add[%283] -> nn.relu[%284] : 32
--------
nn.conv2d[%286]: [32]
  concatenate[%264] -> nn.conv2d[%286] : 8
  constant[%285] -> nn.conv2d[%286] : 8
--------
add[%288]: [32]
  nn.conv2d[%286] -> add[%288] : 32
  constant[%287] -> add[%288] : 32
--------
nn.relu[%289]: [8]
  add[%288] -> nn.relu[%289] : 32
--------
nn.conv2d[%291]: [32]
  nn.relu[%289] -> nn.conv2d[%291] : 8
  constant[%290] -> nn.conv2d[%291] : 8
--------
add[%293]: [32]
  nn.conv2d[%291] -> add[%293] : 32
  constant[%292] -> add[%293] : 32
--------
nn.relu[%294]: [8]
  add[%293] -> nn.relu[%294] : 32
--------
nn.conv2d[%296]: [32]
  nn.relu[%294] -> nn.conv2d[%296] : 8
  constant[%295] -> nn.conv2d[%296] : 8
--------
add[%298]: [32]
  nn.conv2d[%296] -> add[%298] : 32
  constant[%297] -> add[%298] : 32
--------
nn.relu[%299]: [8]
  add[%298] -> nn.relu[%299] : 32
--------
nn.conv2d[%301]: [32]
  nn.relu[%299] -> nn.conv2d[%301] : 8
  constant[%300] -> nn.conv2d[%301] : 8
--------
add[%303]: [32]
  nn.conv2d[%301] -> add[%303] : 32
  constant[%302] -> add[%303] : 32
--------
nn.relu[%304]: [8]
  add[%303] -> nn.relu[%304] : 32
--------
nn.conv2d[%306]: [32]
  nn.relu[%304] -> nn.conv2d[%306] : 8
  constant[%305] -> nn.conv2d[%306] : 8
--------
add[%308]: [32]
  nn.conv2d[%306] -> add[%308] : 32
  constant[%307] -> add[%308] : 32
--------
nn.relu[%309]: [32]
  add[%308] -> nn.relu[%309] : 32
--------
nn.avg_pool2d[%310]: [8]
  concatenate[%264] -> nn.avg_pool2d[%310] : None
--------
nn.conv2d[%312]: [32]
  nn.avg_pool2d[%310] -> nn.conv2d[%312] : 8
  constant[%311] -> nn.conv2d[%312] : 8
--------
add[%314]: [32]
  nn.conv2d[%312] -> add[%314] : 32
  constant[%313] -> add[%314] : 32
--------
nn.relu[%315]: [32]
  add[%314] -> nn.relu[%315] : 32
--------
concatenate[%316]: [8, 8, 8, None]
  nn.relu[%269] -> concatenate[%316] : 32
  nn.relu[%284] -> concatenate[%316] : 32
  nn.relu[%309] -> concatenate[%316] : 32
  nn.relu[%315] -> concatenate[%316] : 32
--------
nn.conv2d[%318]: [32]
  concatenate[%316] -> nn.conv2d[%318] : 8
  constant[%317] -> nn.conv2d[%318] : 8
--------
add[%320]: [32]
  nn.conv2d[%318] -> add[%320] : 32
  constant[%319] -> add[%320] : 32
--------
nn.relu[%321]: [32]
  add[%320] -> nn.relu[%321] : 32
--------
nn.conv2d[%323]: [32]
  concatenate[%316] -> nn.conv2d[%323] : 8
  constant[%322] -> nn.conv2d[%323] : 8
--------
add[%325]: [32]
  nn.conv2d[%323] -> add[%325] : 32
  constant[%324] -> add[%325] : 32
--------
nn.relu[%326]: [8]
  add[%325] -> nn.relu[%326] : 32
--------
nn.conv2d[%328]: [32]
  nn.relu[%326] -> nn.conv2d[%328] : 8
  constant[%327] -> nn.conv2d[%328] : 8
--------
add[%330]: [32]
  nn.conv2d[%328] -> add[%330] : 32
  constant[%329] -> add[%330] : 32
--------
nn.relu[%331]: [8]
  add[%330] -> nn.relu[%331] : 32
--------
nn.conv2d[%333]: [32]
  nn.relu[%331] -> nn.conv2d[%333] : 8
  constant[%332] -> nn.conv2d[%333] : 8
--------
add[%335]: [32]
  nn.conv2d[%333] -> add[%335] : 32
  constant[%334] -> add[%335] : 32
--------
nn.relu[%336]: [32]
  add[%335] -> nn.relu[%336] : 32
--------
nn.conv2d[%338]: [32]
  concatenate[%316] -> nn.conv2d[%338] : 8
  constant[%337] -> nn.conv2d[%338] : 8
--------
add[%340]: [32]
  nn.conv2d[%338] -> add[%340] : 32
  constant[%339] -> add[%340] : 32
--------
nn.relu[%341]: [8]
  add[%340] -> nn.relu[%341] : 32
--------
nn.conv2d[%343]: [32]
  nn.relu[%341] -> nn.conv2d[%343] : 8
  constant[%342] -> nn.conv2d[%343] : 8
--------
add[%345]: [32]
  nn.conv2d[%343] -> add[%345] : 32
  constant[%344] -> add[%345] : 32
--------
nn.relu[%346]: [8]
  add[%345] -> nn.relu[%346] : 32
--------
nn.conv2d[%348]: [32]
  nn.relu[%346] -> nn.conv2d[%348] : 8
  constant[%347] -> nn.conv2d[%348] : 8
--------
add[%350]: [32]
  nn.conv2d[%348] -> add[%350] : 32
  constant[%349] -> add[%350] : 32
--------
nn.relu[%351]: [8]
  add[%350] -> nn.relu[%351] : 32
--------
nn.conv2d[%353]: [32]
  nn.relu[%351] -> nn.conv2d[%353] : 8
  constant[%352] -> nn.conv2d[%353] : 8
--------
add[%355]: [32]
  nn.conv2d[%353] -> add[%355] : 32
  constant[%354] -> add[%355] : 32
--------
nn.relu[%356]: [8]
  add[%355] -> nn.relu[%356] : 32
--------
nn.conv2d[%358]: [32]
  nn.relu[%356] -> nn.conv2d[%358] : 8
  constant[%357] -> nn.conv2d[%358] : 8
--------
add[%360]: [32]
  nn.conv2d[%358] -> add[%360] : 32
  constant[%359] -> add[%360] : 32
--------
nn.relu[%361]: [32]
  add[%360] -> nn.relu[%361] : 32
--------
nn.avg_pool2d[%362]: [8]
  concatenate[%316] -> nn.avg_pool2d[%362] : None
--------
nn.conv2d[%364]: [32]
  nn.avg_pool2d[%362] -> nn.conv2d[%364] : 8
  constant[%363] -> nn.conv2d[%364] : 8
--------
add[%366]: [32]
  nn.conv2d[%364] -> add[%366] : 32
  constant[%365] -> add[%366] : 32
--------
nn.relu[%367]: [32]
  add[%366] -> nn.relu[%367] : 32
--------
concatenate[%368]: [8, 8, 32]
  nn.relu[%321] -> concatenate[%368] : 32
  nn.relu[%336] -> concatenate[%368] : 32
  nn.relu[%361] -> concatenate[%368] : 32
  nn.relu[%367] -> concatenate[%368] : 32
--------
nn.conv2d[%370]: [32]
  concatenate[%368] -> nn.conv2d[%370] : 8
  constant[%369] -> nn.conv2d[%370] : 8
--------
add[%372]: [32]
  nn.conv2d[%370] -> add[%372] : 32
  constant[%371] -> add[%372] : 32
--------
nn.relu[%373]: [8]
  add[%372] -> nn.relu[%373] : 32
--------
nn.conv2d[%375]: [32]
  nn.relu[%373] -> nn.conv2d[%375] : 8
  constant[%374] -> nn.conv2d[%375] : 8
--------
add[%377]: [32]
  nn.conv2d[%375] -> add[%377] : 32
  constant[%376] -> add[%377] : 32
--------
nn.relu[%378]: [32]
  add[%377] -> nn.relu[%378] : 32
--------
nn.conv2d[%380]: [32]
  concatenate[%368] -> nn.conv2d[%380] : 8
  constant[%379] -> nn.conv2d[%380] : 8
--------
add[%382]: [32]
  nn.conv2d[%380] -> add[%382] : 32
  constant[%381] -> add[%382] : 32
--------
nn.relu[%383]: [8]
  add[%382] -> nn.relu[%383] : 32
--------
nn.conv2d[%385]: [32]
  nn.relu[%383] -> nn.conv2d[%385] : 8
  constant[%384] -> nn.conv2d[%385] : 8
--------
add[%387]: [32]
  nn.conv2d[%385] -> add[%387] : 32
  constant[%386] -> add[%387] : 32
--------
nn.relu[%388]: [8]
  add[%387] -> nn.relu[%388] : 32
--------
nn.conv2d[%390]: [32]
  nn.relu[%388] -> nn.conv2d[%390] : 8
  constant[%389] -> nn.conv2d[%390] : 8
--------
add[%392]: [32]
  nn.conv2d[%390] -> add[%392] : 32
  constant[%391] -> add[%392] : 32
--------
nn.relu[%393]: [8]
  add[%392] -> nn.relu[%393] : 32
--------
nn.conv2d[%395]: [32]
  nn.relu[%393] -> nn.conv2d[%395] : 8
  constant[%394] -> nn.conv2d[%395] : 8
--------
add[%397]: [32]
  nn.conv2d[%395] -> add[%397] : 32
  constant[%396] -> add[%397] : 32
--------
nn.relu[%398]: [32]
  add[%397] -> nn.relu[%398] : 32
--------
nn.max_pool2d[%399]: [32]
  concatenate[%368] -> nn.max_pool2d[%399] : 32
--------
concatenate[%400]: [8, 8, 8, None]
  nn.relu[%378] -> concatenate[%400] : 32
  nn.relu[%398] -> concatenate[%400] : 32
  nn.max_pool2d[%399] -> concatenate[%400] : 32
--------
nn.conv2d[%402]: [32]
  concatenate[%400] -> nn.conv2d[%402] : 8
  constant[%401] -> nn.conv2d[%402] : 8
--------
add[%404]: [32]
  nn.conv2d[%402] -> add[%404] : 32
  constant[%403] -> add[%404] : 32
--------
nn.relu[%405]: [32]
  add[%404] -> nn.relu[%405] : 32
--------
nn.conv2d[%407]: [32]
  concatenate[%400] -> nn.conv2d[%407] : 8
  constant[%406] -> nn.conv2d[%407] : 8
--------
add[%409]: [32]
  nn.conv2d[%407] -> add[%409] : 32
  constant[%408] -> add[%409] : 32
--------
nn.relu[%410]: [8, 8]
  add[%409] -> nn.relu[%410] : 32
--------
nn.conv2d[%412]: [32]
  nn.relu[%410] -> nn.conv2d[%412] : 8
  constant[%411] -> nn.conv2d[%412] : 8
--------
add[%414]: [32]
  nn.conv2d[%412] -> add[%414] : 32
  constant[%413] -> add[%414] : 32
--------
nn.relu[%415]: [32]
  add[%414] -> nn.relu[%415] : 32
--------
nn.conv2d[%417]: [32]
  nn.relu[%410] -> nn.conv2d[%417] : 8
  constant[%416] -> nn.conv2d[%417] : 8
--------
add[%419]: [32]
  nn.conv2d[%417] -> add[%419] : 32
  constant[%418] -> add[%419] : 32
--------
nn.relu[%420]: [32]
  add[%419] -> nn.relu[%420] : 32
--------
concatenate[%421]: [32]
  nn.relu[%415] -> concatenate[%421] : 32
  nn.relu[%420] -> concatenate[%421] : 32
--------
nn.conv2d[%423]: [32]
  concatenate[%400] -> nn.conv2d[%423] : 8
  constant[%422] -> nn.conv2d[%423] : 8
--------
add[%425]: [32]
  nn.conv2d[%423] -> add[%425] : 32
  constant[%424] -> add[%425] : 32
--------
nn.relu[%426]: [8]
  add[%425] -> nn.relu[%426] : 32
--------
nn.conv2d[%428]: [32]
  nn.relu[%426] -> nn.conv2d[%428] : 8
  constant[%427] -> nn.conv2d[%428] : 8
--------
add[%430]: [32]
  nn.conv2d[%428] -> add[%430] : 32
  constant[%429] -> add[%430] : 32
--------
nn.relu[%431]: [8, 8]
  add[%430] -> nn.relu[%431] : 32
--------
nn.conv2d[%433]: [32]
  nn.relu[%431] -> nn.conv2d[%433] : 8
  constant[%432] -> nn.conv2d[%433] : 8
--------
add[%435]: [32]
  nn.conv2d[%433] -> add[%435] : 32
  constant[%434] -> add[%435] : 32
--------
nn.relu[%436]: [32]
  add[%435] -> nn.relu[%436] : 32
--------
nn.conv2d[%438]: [32]
  nn.relu[%431] -> nn.conv2d[%438] : 8
  constant[%437] -> nn.conv2d[%438] : 8
--------
add[%440]: [32]
  nn.conv2d[%438] -> add[%440] : 32
  constant[%439] -> add[%440] : 32
--------
nn.relu[%441]: [32]
  add[%440] -> nn.relu[%441] : 32
--------
concatenate[%442]: [32]
  nn.relu[%436] -> concatenate[%442] : 32
  nn.relu[%441] -> concatenate[%442] : 32
--------
nn.avg_pool2d[%443]: [8]
  concatenate[%400] -> nn.avg_pool2d[%443] : None
--------
nn.conv2d[%445]: [32]
  nn.avg_pool2d[%443] -> nn.conv2d[%445] : 8
  constant[%444] -> nn.conv2d[%445] : 8
--------
add[%447]: [32]
  nn.conv2d[%445] -> add[%447] : 32
  constant[%446] -> add[%447] : 32
--------
nn.relu[%448]: [32]
  add[%447] -> nn.relu[%448] : 32
--------
concatenate[%449]: [8, 8, 8, None]
  nn.relu[%405] -> concatenate[%449] : 32
  concatenate[%421] -> concatenate[%449] : 32
  concatenate[%442] -> concatenate[%449] : 32
  nn.relu[%448] -> concatenate[%449] : 32
--------
nn.conv2d[%451]: [32]
  concatenate[%449] -> nn.conv2d[%451] : 8
  constant[%450] -> nn.conv2d[%451] : 8
--------
add[%453]: [32]
  nn.conv2d[%451] -> add[%453] : 32
  constant[%452] -> add[%453] : 32
--------
nn.relu[%454]: [32]
  add[%453] -> nn.relu[%454] : 32
--------
nn.conv2d[%456]: [32]
  concatenate[%449] -> nn.conv2d[%456] : 8
  constant[%455] -> nn.conv2d[%456] : 8
--------
add[%458]: [32]
  nn.conv2d[%456] -> add[%458] : 32
  constant[%457] -> add[%458] : 32
--------
nn.relu[%459]: [8, 8]
  add[%458] -> nn.relu[%459] : 32
--------
nn.conv2d[%461]: [32]
  nn.relu[%459] -> nn.conv2d[%461] : 8
  constant[%460] -> nn.conv2d[%461] : 8
--------
add[%463]: [32]
  nn.conv2d[%461] -> add[%463] : 32
  constant[%462] -> add[%463] : 32
--------
nn.relu[%464]: [32]
  add[%463] -> nn.relu[%464] : 32
--------
nn.conv2d[%466]: [32]
  nn.relu[%459] -> nn.conv2d[%466] : 8
  constant[%465] -> nn.conv2d[%466] : 8
--------
add[%468]: [32]
  nn.conv2d[%466] -> add[%468] : 32
  constant[%467] -> add[%468] : 32
--------
nn.relu[%469]: [32]
  add[%468] -> nn.relu[%469] : 32
--------
concatenate[%470]: [32]
  nn.relu[%464] -> concatenate[%470] : 32
  nn.relu[%469] -> concatenate[%470] : 32
--------
nn.conv2d[%472]: [32]
  concatenate[%449] -> nn.conv2d[%472] : 8
  constant[%471] -> nn.conv2d[%472] : 8
--------
add[%474]: [32]
  nn.conv2d[%472] -> add[%474] : 32
  constant[%473] -> add[%474] : 32
--------
nn.relu[%475]: [8]
  add[%474] -> nn.relu[%475] : 32
--------
nn.conv2d[%477]: [32]
  nn.relu[%475] -> nn.conv2d[%477] : 8
  constant[%476] -> nn.conv2d[%477] : 8
--------
add[%479]: [32]
  nn.conv2d[%477] -> add[%479] : 32
  constant[%478] -> add[%479] : 32
--------
nn.relu[%480]: [8, 8]
  add[%479] -> nn.relu[%480] : 32
--------
nn.conv2d[%482]: [32]
  nn.relu[%480] -> nn.conv2d[%482] : 8
  constant[%481] -> nn.conv2d[%482] : 8
--------
add[%484]: [32]
  nn.conv2d[%482] -> add[%484] : 32
  constant[%483] -> add[%484] : 32
--------
nn.relu[%485]: [32]
  add[%484] -> nn.relu[%485] : 32
--------
nn.conv2d[%487]: [32]
  nn.relu[%480] -> nn.conv2d[%487] : 8
  constant[%486] -> nn.conv2d[%487] : 8
--------
add[%489]: [32]
  nn.conv2d[%487] -> add[%489] : 32
  constant[%488] -> add[%489] : 32
--------
nn.relu[%490]: [32]
  add[%489] -> nn.relu[%490] : 32
--------
concatenate[%491]: [32]
  nn.relu[%485] -> concatenate[%491] : 32
  nn.relu[%490] -> concatenate[%491] : 32
--------
nn.avg_pool2d[%492]: [8]
  concatenate[%449] -> nn.avg_pool2d[%492] : None
--------
nn.conv2d[%494]: [32]
  nn.avg_pool2d[%492] -> nn.conv2d[%494] : 8
  constant[%493] -> nn.conv2d[%494] : 8
--------
add[%496]: [32]
  nn.conv2d[%494] -> add[%496] : 32
  constant[%495] -> add[%496] : 32
--------
nn.relu[%497]: [32]
  add[%496] -> nn.relu[%497] : 32
--------
concatenate[%498]: [None]
  nn.relu[%454] -> concatenate[%498] : 32
  concatenate[%470] -> concatenate[%498] : 32
  concatenate[%491] -> concatenate[%498] : 32
  nn.relu[%497] -> concatenate[%498] : 32
--------
nn.avg_pool2d[%499]: [None]
  concatenate[%498] -> nn.avg_pool2d[%499] : None
--------
nn.batch_flatten[%500]: [None]
  nn.avg_pool2d[%499] -> nn.batch_flatten[%500] : None
--------
nn.dense[%502]: [None]
  nn.batch_flatten[%500] -> nn.dense[%502] : None
  constant[%501] -> nn.dense[%502] : None
--------
add[%504]: []
  nn.dense[%502] -> add[%504] : None
  constant[%503] -> add[%504] : None
ops in graph:
{'nn.conv2d', 'nn.dense', 'nn.relu', 'concatenate', 'nn.avg_pool2d', 'nn.max_pool2d', 'nn.batch_flatten', 'add'}
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
original acc: 0.8203125
data
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
nn.avg_pool2d
nn.batch_flatten
constant
nn.dense
constant
add
data -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.max_pool2d -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.max_pool2d
nn.relu -> concatenate
nn.relu -> concatenate
nn.max_pool2d -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.max_pool2d
nn.relu -> concatenate
nn.relu -> concatenate
nn.max_pool2d -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
concatenate -> concatenate
concatenate -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
concatenate -> concatenate
concatenate -> concatenate
nn.relu -> concatenate
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.batch_flatten
nn.batch_flatten -> nn.dense
constant -> nn.dense
nn.dense -> add
constant -> add
analyzed condition
node_conds: [False, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, False, True, False, True, True, True, False, False, False, False, False, False]
edge_conds: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]
INFO:root:collecting statistics for calibration...
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:compile_engine:Use implementation conv2d_nchw.cuda for op nn.conv2d
INFO:root:statistics collected

select descriptor
---------
nn.conv2d[%2]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%4]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%5]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%7]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%9]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%10]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%12]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%14]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%15]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%16]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%18]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%20]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%21]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%23]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%25]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%26]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%27]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%29]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%31]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%32]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%34]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%36]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%37]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%39]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%41]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%42]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%44]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%46]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%47]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%49]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%51]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%52]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%54]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%56]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%57]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%60]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%62]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%63]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%64]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%66]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%68]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%69]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%71]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%73]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%74]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%76]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%78]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%79]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%81]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%83]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%84]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%86]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%88]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%89]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%91]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%93]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%94]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%97]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%99]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%100]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%101]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%103]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%105]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%106]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%108]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%110]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%111]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%113]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%115]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%116]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%118]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%120]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%121]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%123]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%125]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%126]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%128]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%130]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%131]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%134]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%136]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%137]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%138]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%140]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%142]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%143]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%145]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%147]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%148]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%150]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%152]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%153]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%155]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%157]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%158]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%159]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%160]
  in bits: [32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%162]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%164]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%165]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%167]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%169]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%170]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%172]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%174]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%175]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%177]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%179]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%180]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%182]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%184]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%185]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%187]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%189]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%190]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%192]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%194]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%195]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%197]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%199]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%200]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%202]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%204]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%205]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%208]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%210]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%211]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%212]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%214]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%216]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%217]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%219]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%221]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%222]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%224]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%226]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%227]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%229]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%231]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%232]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%234]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%236]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%237]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%239]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%241]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%242]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%244]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%246]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%247]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%249]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%251]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%252]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%254]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%256]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%257]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%260]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%262]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%263]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%264]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%266]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%268]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%269]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%271]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%273]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%274]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%276]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%278]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%279]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%281]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%283]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%284]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%286]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%288]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%289]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%291]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%293]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%294]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%296]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%298]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%299]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%301]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%303]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%304]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%306]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%308]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%309]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%312]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%314]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%315]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%316]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%318]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%320]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%321]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%323]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%325]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%326]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%328]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%330]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%331]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%333]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%335]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%336]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%338]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%340]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%341]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%343]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%345]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%346]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%348]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%350]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%351]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%353]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%355]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%356]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%358]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%360]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%361]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%364]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%366]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%367]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%368]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%370]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%372]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%373]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%375]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%377]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%378]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%380]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%382]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%383]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%385]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%387]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%388]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%390]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%392]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%393]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%395]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%397]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%398]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%399]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%400]
  in bits: [32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%402]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%404]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%405]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%407]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%409]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%410]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%412]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%414]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%415]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%417]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%419]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%420]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%421]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%423]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%425]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%426]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%428]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%430]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%431]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%433]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%435]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%436]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%438]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%440]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%441]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%442]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%445]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%447]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%448]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%449]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%451]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%453]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%454]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%456]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%458]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%459]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%461]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%463]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%464]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%466]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%468]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%469]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%470]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%472]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%474]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%475]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%477]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%479]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%480]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%482]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%484]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%485]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%487]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%489]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%490]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%491]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%494]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%496]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%497]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%498]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
fn (%data: Tensor[(32, 3, 299, 299), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(32, 3, 3, 3), float32] */ /* ty=Tensor[(32, 3, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %3 = nn.conv2d(%2, meta[relay.Constant][2] /* ty=Tensor[(32, 32, 3, 3), float32] */ /* ty=Tensor[(32, 32, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %4 = add(%3, meta[relay.Constant][3] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %5 = nn.relu(%4) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %6 = nn.conv2d(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */ /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %7 = add(%6, meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %8 = nn.relu(%7) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 64, 73, 73), float32] */;
  %10 = nn.conv2d(%9, meta[relay.Constant][6] /* ty=Tensor[(80, 64, 1, 1), float32] */ /* ty=Tensor[(80, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=80, kernel_size=[1, 1]) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %11 = add(%10, meta[relay.Constant][7] /* ty=Tensor[(80, 1, 1), float32] */ /* ty=Tensor[(80, 1, 1), float32] */) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %12 = nn.relu(%11) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %13 = nn.conv2d(%12, meta[relay.Constant][8] /* ty=Tensor[(192, 80, 3, 3), float32] */ /* ty=Tensor[(192, 80, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %14 = add(%13, meta[relay.Constant][9] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %15 = nn.relu(%14) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %16 = nn.max_pool2d(%15, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %17 = nn.conv2d(%16, meta[relay.Constant][10] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %18 = add(%17, meta[relay.Constant][11] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %19 = nn.relu(%18) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %20 = nn.conv2d(%16, meta[relay.Constant][12] /* ty=Tensor[(48, 192, 1, 1), float32] */ /* ty=Tensor[(48, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %21 = add(%20, meta[relay.Constant][13] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %22 = nn.relu(%21) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %23 = nn.conv2d(%22, meta[relay.Constant][14] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %24 = add(%23, meta[relay.Constant][15] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %25 = nn.relu(%24) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %26 = nn.conv2d(%16, meta[relay.Constant][16] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %27 = add(%26, meta[relay.Constant][17] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %29 = nn.conv2d(%28, meta[relay.Constant][18] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %30 = add(%29, meta[relay.Constant][19] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %31 = nn.relu(%30) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %32 = nn.conv2d(%31, meta[relay.Constant][20] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %33 = add(%32, meta[relay.Constant][21] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %34 = nn.relu(%33) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %35 = nn.avg_pool2d(%16, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %36 = nn.conv2d(%35, meta[relay.Constant][22] /* ty=Tensor[(32, 192, 1, 1), float32] */ /* ty=Tensor[(32, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %37 = add(%36, meta[relay.Constant][23] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %38 = nn.relu(%37) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %39 = (%19, %25, %34, %38);
  %40 = concatenate(%39, axis=1) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %41 = nn.conv2d(%40, meta[relay.Constant][24] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %42 = add(%41, meta[relay.Constant][25] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %43 = nn.relu(%42) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %44 = nn.conv2d(%40, meta[relay.Constant][26] /* ty=Tensor[(48, 256, 1, 1), float32] */ /* ty=Tensor[(48, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %45 = add(%44, meta[relay.Constant][27] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %46 = nn.relu(%45) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %47 = nn.conv2d(%46, meta[relay.Constant][28] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %48 = add(%47, meta[relay.Constant][29] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %49 = nn.relu(%48) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %50 = nn.conv2d(%40, meta[relay.Constant][30] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %51 = add(%50, meta[relay.Constant][31] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %52 = nn.relu(%51) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %53 = nn.conv2d(%52, meta[relay.Constant][32] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %54 = add(%53, meta[relay.Constant][33] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %55 = nn.relu(%54) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %56 = nn.conv2d(%55, meta[relay.Constant][34] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %57 = add(%56, meta[relay.Constant][35] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %59 = nn.avg_pool2d(%40, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %60 = nn.conv2d(%59, meta[relay.Constant][36] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %61 = add(%60, meta[relay.Constant][37] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %63 = (%43, %49, %58, %62);
  %64 = concatenate(%63, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %65 = nn.conv2d(%64, meta[relay.Constant][38] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %66 = add(%65, meta[relay.Constant][39] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %68 = nn.conv2d(%64, meta[relay.Constant][40] /* ty=Tensor[(48, 288, 1, 1), float32] */ /* ty=Tensor[(48, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %69 = add(%68, meta[relay.Constant][41] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %70 = nn.relu(%69) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %71 = nn.conv2d(%70, meta[relay.Constant][42] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %72 = add(%71, meta[relay.Constant][43] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %73 = nn.relu(%72) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %74 = nn.conv2d(%64, meta[relay.Constant][44] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %75 = add(%74, meta[relay.Constant][45] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %76 = nn.relu(%75) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %77 = nn.conv2d(%76, meta[relay.Constant][46] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %78 = add(%77, meta[relay.Constant][47] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %79 = nn.relu(%78) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %80 = nn.conv2d(%79, meta[relay.Constant][48] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %81 = add(%80, meta[relay.Constant][49] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %82 = nn.relu(%81) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %83 = nn.avg_pool2d(%64, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %84 = nn.conv2d(%83, meta[relay.Constant][50] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %85 = add(%84, meta[relay.Constant][51] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %86 = nn.relu(%85) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %87 = (%67, %73, %82, %86);
  %88 = concatenate(%87, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %89 = nn.conv2d(%88, meta[relay.Constant][52] /* ty=Tensor[(384, 288, 3, 3), float32] */ /* ty=Tensor[(384, 288, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %90 = add(%89, meta[relay.Constant][53] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %92 = nn.conv2d(%88, meta[relay.Constant][54] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %93 = add(%92, meta[relay.Constant][55] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %94 = nn.relu(%93) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %95 = nn.conv2d(%94, meta[relay.Constant][56] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %96 = add(%95, meta[relay.Constant][57] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %97 = nn.relu(%96) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %98 = nn.conv2d(%97, meta[relay.Constant][58] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %99 = add(%98, meta[relay.Constant][59] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %100 = nn.relu(%99) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %101 = nn.max_pool2d(%88, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 288, 17, 17), float32] */;
  %102 = (%91, %100, %101);
  %103 = concatenate(%102, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %104 = nn.conv2d(%103, meta[relay.Constant][60] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %105 = add(%104, meta[relay.Constant][61] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %106 = nn.relu(%105) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %107 = nn.conv2d(%103, meta[relay.Constant][62] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %108 = add(%107, meta[relay.Constant][63] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %109 = nn.relu(%108) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %110 = nn.conv2d(%109, meta[relay.Constant][64] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %111 = add(%110, meta[relay.Constant][65] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %112 = nn.relu(%111) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %113 = nn.conv2d(%112, meta[relay.Constant][66] /* ty=Tensor[(192, 128, 7, 1), float32] */ /* ty=Tensor[(192, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %114 = add(%113, meta[relay.Constant][67] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %115 = nn.relu(%114) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %116 = nn.conv2d(%103, meta[relay.Constant][68] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %117 = add(%116, meta[relay.Constant][69] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %118 = nn.relu(%117) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %119 = nn.conv2d(%118, meta[relay.Constant][70] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %120 = add(%119, meta[relay.Constant][71] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %121 = nn.relu(%120) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %122 = nn.conv2d(%121, meta[relay.Constant][72] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %123 = add(%122, meta[relay.Constant][73] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %125 = nn.conv2d(%124, meta[relay.Constant][74] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %126 = add(%125, meta[relay.Constant][75] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %127 = nn.relu(%126) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %128 = nn.conv2d(%127, meta[relay.Constant][76] /* ty=Tensor[(192, 128, 1, 7), float32] */ /* ty=Tensor[(192, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %129 = add(%128, meta[relay.Constant][77] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %130 = nn.relu(%129) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %131 = nn.avg_pool2d(%103, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %132 = nn.conv2d(%131, meta[relay.Constant][78] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %133 = add(%132, meta[relay.Constant][79] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %134 = nn.relu(%133) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %135 = (%106, %115, %130, %134);
  %136 = concatenate(%135, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %137 = nn.conv2d(%136, meta[relay.Constant][80] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %138 = add(%137, meta[relay.Constant][81] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %139 = nn.relu(%138) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %140 = nn.conv2d(%136, meta[relay.Constant][82] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %141 = add(%140, meta[relay.Constant][83] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %142 = nn.relu(%141) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %143 = nn.conv2d(%142, meta[relay.Constant][84] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %144 = add(%143, meta[relay.Constant][85] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %145 = nn.relu(%144) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %146 = nn.conv2d(%145, meta[relay.Constant][86] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %147 = add(%146, meta[relay.Constant][87] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %148 = nn.relu(%147) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %149 = nn.conv2d(%136, meta[relay.Constant][88] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %150 = add(%149, meta[relay.Constant][89] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %151 = nn.relu(%150) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %152 = nn.conv2d(%151, meta[relay.Constant][90] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %153 = add(%152, meta[relay.Constant][91] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %154 = nn.relu(%153) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %155 = nn.conv2d(%154, meta[relay.Constant][92] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %156 = add(%155, meta[relay.Constant][93] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %158 = nn.conv2d(%157, meta[relay.Constant][94] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %159 = add(%158, meta[relay.Constant][95] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %161 = nn.conv2d(%160, meta[relay.Constant][96] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %162 = add(%161, meta[relay.Constant][97] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %163 = nn.relu(%162) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %164 = nn.avg_pool2d(%136, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %165 = nn.conv2d(%164, meta[relay.Constant][98] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %166 = add(%165, meta[relay.Constant][99] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %167 = nn.relu(%166) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %168 = (%139, %148, %163, %167);
  %169 = concatenate(%168, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %170 = nn.conv2d(%169, meta[relay.Constant][100] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %171 = add(%170, meta[relay.Constant][101] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %173 = nn.conv2d(%169, meta[relay.Constant][102] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %174 = add(%173, meta[relay.Constant][103] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %175 = nn.relu(%174) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %176 = nn.conv2d(%175, meta[relay.Constant][104] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %177 = add(%176, meta[relay.Constant][105] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %178 = nn.relu(%177) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %179 = nn.conv2d(%178, meta[relay.Constant][106] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %180 = add(%179, meta[relay.Constant][107] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %181 = nn.relu(%180) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %182 = nn.conv2d(%169, meta[relay.Constant][108] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %183 = add(%182, meta[relay.Constant][109] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %184 = nn.relu(%183) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %185 = nn.conv2d(%184, meta[relay.Constant][110] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %186 = add(%185, meta[relay.Constant][111] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %187 = nn.relu(%186) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %188 = nn.conv2d(%187, meta[relay.Constant][112] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %189 = add(%188, meta[relay.Constant][113] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %190 = nn.relu(%189) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %191 = nn.conv2d(%190, meta[relay.Constant][114] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %192 = add(%191, meta[relay.Constant][115] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %193 = nn.relu(%192) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %194 = nn.conv2d(%193, meta[relay.Constant][116] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %195 = add(%194, meta[relay.Constant][117] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %196 = nn.relu(%195) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %197 = nn.avg_pool2d(%169, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %198 = nn.conv2d(%197, meta[relay.Constant][118] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %199 = add(%198, meta[relay.Constant][119] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %200 = nn.relu(%199) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %201 = (%172, %181, %196, %200);
  %202 = concatenate(%201, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %203 = nn.conv2d(%202, meta[relay.Constant][120] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %204 = add(%203, meta[relay.Constant][121] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %206 = nn.conv2d(%202, meta[relay.Constant][122] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %207 = add(%206, meta[relay.Constant][123] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %208 = nn.relu(%207) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %209 = nn.conv2d(%208, meta[relay.Constant][124] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %210 = add(%209, meta[relay.Constant][125] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %211 = nn.relu(%210) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %212 = nn.conv2d(%211, meta[relay.Constant][126] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %213 = add(%212, meta[relay.Constant][127] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %214 = nn.relu(%213) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %215 = nn.conv2d(%202, meta[relay.Constant][128] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %216 = add(%215, meta[relay.Constant][129] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %217 = nn.relu(%216) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %218 = nn.conv2d(%217, meta[relay.Constant][130] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %219 = add(%218, meta[relay.Constant][131] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %220 = nn.relu(%219) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %221 = nn.conv2d(%220, meta[relay.Constant][132] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %222 = add(%221, meta[relay.Constant][133] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %223 = nn.relu(%222) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %224 = nn.conv2d(%223, meta[relay.Constant][134] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %225 = add(%224, meta[relay.Constant][135] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %226 = nn.relu(%225) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %227 = nn.conv2d(%226, meta[relay.Constant][136] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %228 = add(%227, meta[relay.Constant][137] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %229 = nn.relu(%228) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %230 = nn.avg_pool2d(%202, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %231 = nn.conv2d(%230, meta[relay.Constant][138] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %232 = add(%231, meta[relay.Constant][139] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %233 = nn.relu(%232) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %234 = (%205, %214, %229, %233);
  %235 = concatenate(%234, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %236 = nn.conv2d(%235, meta[relay.Constant][140] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %237 = add(%236, meta[relay.Constant][141] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %238 = nn.relu(%237) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %239 = nn.conv2d(%238, meta[relay.Constant][142] /* ty=Tensor[(320, 192, 3, 3), float32] */ /* ty=Tensor[(320, 192, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=320, kernel_size=[3, 3]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %240 = add(%239, meta[relay.Constant][143] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %241 = nn.relu(%240) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %242 = nn.conv2d(%235, meta[relay.Constant][144] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %243 = add(%242, meta[relay.Constant][145] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %244 = nn.relu(%243) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %245 = nn.conv2d(%244, meta[relay.Constant][146] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %246 = add(%245, meta[relay.Constant][147] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %247 = nn.relu(%246) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %248 = nn.conv2d(%247, meta[relay.Constant][148] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %249 = add(%248, meta[relay.Constant][149] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %250 = nn.relu(%249) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %251 = nn.conv2d(%250, meta[relay.Constant][150] /* ty=Tensor[(192, 192, 3, 3), float32] */ /* ty=Tensor[(192, 192, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %252 = add(%251, meta[relay.Constant][151] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %253 = nn.relu(%252) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %254 = nn.max_pool2d(%235, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %255 = (%241, %253, %254);
  %256 = concatenate(%255, axis=1) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %257 = nn.conv2d(%256, meta[relay.Constant][152] /* ty=Tensor[(320, 1280, 1, 1), float32] */ /* ty=Tensor[(320, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %258 = add(%257, meta[relay.Constant][153] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %259 = nn.relu(%258) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %260 = nn.conv2d(%256, meta[relay.Constant][154] /* ty=Tensor[(384, 1280, 1, 1), float32] */ /* ty=Tensor[(384, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %261 = add(%260, meta[relay.Constant][155] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %262 = nn.relu(%261) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %263 = nn.conv2d(%262, meta[relay.Constant][156] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %264 = add(%263, meta[relay.Constant][157] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %265 = nn.relu(%264) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %266 = nn.conv2d(%262, meta[relay.Constant][158] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %267 = add(%266, meta[relay.Constant][159] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %268 = nn.relu(%267) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %269 = (%265, %268);
  %270 = concatenate(%269, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %271 = nn.conv2d(%256, meta[relay.Constant][160] /* ty=Tensor[(448, 1280, 1, 1), float32] */ /* ty=Tensor[(448, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %272 = add(%271, meta[relay.Constant][161] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %273 = nn.relu(%272) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %274 = nn.conv2d(%273, meta[relay.Constant][162] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %275 = add(%274, meta[relay.Constant][163] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %276 = nn.relu(%275) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %277 = nn.conv2d(%276, meta[relay.Constant][164] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %278 = add(%277, meta[relay.Constant][165] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %279 = nn.relu(%278) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %280 = nn.conv2d(%276, meta[relay.Constant][166] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %281 = add(%280, meta[relay.Constant][167] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %282 = nn.relu(%281) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %283 = (%279, %282);
  %284 = concatenate(%283, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %285 = nn.avg_pool2d(%256, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %286 = nn.conv2d(%285, meta[relay.Constant][168] /* ty=Tensor[(192, 1280, 1, 1), float32] */ /* ty=Tensor[(192, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %287 = add(%286, meta[relay.Constant][169] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %288 = nn.relu(%287) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %289 = (%259, %270, %284, %288);
  %290 = concatenate(%289, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %291 = nn.conv2d(%290, meta[relay.Constant][170] /* ty=Tensor[(320, 2048, 1, 1), float32] */ /* ty=Tensor[(320, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %292 = add(%291, meta[relay.Constant][171] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %293 = nn.relu(%292) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %294 = nn.conv2d(%290, meta[relay.Constant][172] /* ty=Tensor[(384, 2048, 1, 1), float32] */ /* ty=Tensor[(384, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %295 = add(%294, meta[relay.Constant][173] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %296 = nn.relu(%295) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %297 = nn.conv2d(%296, meta[relay.Constant][174] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %298 = add(%297, meta[relay.Constant][175] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %299 = nn.relu(%298) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %300 = nn.conv2d(%296, meta[relay.Constant][176] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %301 = add(%300, meta[relay.Constant][177] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %302 = nn.relu(%301) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %303 = (%299, %302);
  %304 = concatenate(%303, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %305 = nn.conv2d(%290, meta[relay.Constant][178] /* ty=Tensor[(448, 2048, 1, 1), float32] */ /* ty=Tensor[(448, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %306 = add(%305, meta[relay.Constant][179] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %307 = nn.relu(%306) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %308 = nn.conv2d(%307, meta[relay.Constant][180] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %309 = add(%308, meta[relay.Constant][181] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %310 = nn.relu(%309) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %311 = nn.conv2d(%310, meta[relay.Constant][182] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %312 = add(%311, meta[relay.Constant][183] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %313 = nn.relu(%312) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %314 = nn.conv2d(%310, meta[relay.Constant][184] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %315 = add(%314, meta[relay.Constant][185] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %316 = nn.relu(%315) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %317 = (%313, %316);
  %318 = concatenate(%317, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %319 = nn.avg_pool2d(%290, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %320 = nn.conv2d(%319, meta[relay.Constant][186] /* ty=Tensor[(192, 2048, 1, 1), float32] */ /* ty=Tensor[(192, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %321 = add(%320, meta[relay.Constant][187] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %322 = nn.relu(%321) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %323 = (%293, %304, %318, %322);
  %324 = concatenate(%323, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %325 = nn.avg_pool2d(%324, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %326 = nn.batch_flatten(%325) /* ty=Tensor[(32, 2048), float32] */;
  %327 = nn.dense(%326, meta[relay.Constant][188] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%327, meta[relay.Constant][189] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
Simulated graph fn (%data: Tensor[(32, 3, 299, 299), float32], %in_scale0: float32, %out_scale0: float32, %clip_min0: float32, %clip_max0: float32, %in_scale1: float32, %out_scale1: float32, %clip_min1: float32, %clip_max1: float32, %in_scale2: float32, %out_scale2: float32, %clip_min2: float32, %clip_max2: float32, %in_scale3: float32, %out_scale3: float32, %clip_min3: float32, %clip_max3: float32, %in_scale4: float32, %out_scale4: float32, %clip_min4: float32, %clip_max4: float32, %in_scale5: float32, %out_scale5: float32, %clip_min5: float32, %clip_max5: float32, %in_scale6: float32, %out_scale6: float32, %clip_min6: float32, %clip_max6: float32, %in_scale7: float32, %out_scale7: float32, %clip_min7: float32, %clip_max7: float32, %in_scale8: float32, %out_scale8: float32, %clip_min8: float32, %clip_max8: float32, %in_scale9: float32, %out_scale9: float32, %clip_min9: float32, %clip_max9: float32, %in_scale10: float32, %out_scale10: float32, %clip_min10: float32, %clip_max10: float32, %in_scale11: float32, %out_scale11: float32, %clip_min11: float32, %clip_max11: float32, %in_scale12: float32, %out_scale12: float32, %clip_min12: float32, %clip_max12: float32, %in_scale13: float32, %out_scale13: float32, %clip_min13: float32, %clip_max13: float32, %in_scale14: float32, %out_scale14: float32, %clip_min14: float32, %clip_max14: float32, %in_scale15: float32, %out_scale15: float32, %clip_min15: float32, %clip_max15: float32, %in_scale16: float32, %out_scale16: float32, %clip_min16: float32, %clip_max16: float32, %in_scale17: float32, %out_scale17: float32, %clip_min17: float32, %clip_max17: float32, %in_scale18: float32, %out_scale18: float32, %clip_min18: float32, %clip_max18: float32, %in_scale19: float32, %out_scale19: float32, %clip_min19: float32, %clip_max19: float32, %in_scale20: float32, %out_scale20: float32, %clip_min20: float32, %clip_max20: float32, %in_scale21: float32, %out_scale21: float32, %clip_min21: float32, %clip_max21: float32, %in_scale22: float32, %out_scale22: float32, %clip_min22: float32, %clip_max22: float32, %in_scale23: float32, %out_scale23: float32, %clip_min23: float32, %clip_max23: float32, %in_scale24: float32, %out_scale24: float32, %clip_min24: float32, %clip_max24: float32, %in_scale25: float32, %out_scale25: float32, %clip_min25: float32, %clip_max25: float32, %in_scale26: float32, %out_scale26: float32, %clip_min26: float32, %clip_max26: float32, %in_scale27: float32, %out_scale27: float32, %clip_min27: float32, %clip_max27: float32, %in_scale28: float32, %out_scale28: float32, %clip_min28: float32, %clip_max28: float32, %in_scale29: float32, %out_scale29: float32, %clip_min29: float32, %clip_max29: float32, %in_scale30: float32, %out_scale30: float32, %clip_min30: float32, %clip_max30: float32, %in_scale31: float32, %out_scale31: float32, %clip_min31: float32, %clip_max31: float32, %in_scale63: float32, %out_scale63: float32, %clip_min63: float32, %clip_max63: float32, %in_scale32: float32, %out_scale32: float32, %clip_min32: float32, %clip_max32: float32, %in_scale33: float32, %out_scale33: float32, %clip_min33: float32, %clip_max33: float32, %in_scale34: float32, %out_scale34: float32, %clip_min34: float32, %clip_max34: float32, %in_scale35: float32, %out_scale35: float32, %clip_min35: float32, %clip_max35: float32, %in_scale36: float32, %out_scale36: float32, %clip_min36: float32, %clip_max36: float32, %in_scale37: float32, %out_scale37: float32, %clip_min37: float32, %clip_max37: float32, %in_scale38: float32, %out_scale38: float32, %clip_min38: float32, %clip_max38: float32, %in_scale39: float32, %out_scale39: float32, %clip_min39: float32, %clip_max39: float32, %in_scale40: float32, %out_scale40: float32, %clip_min40: float32, %clip_max40: float32, %in_scale41: float32, %out_scale41: float32, %clip_min41: float32, %clip_max41: float32, %in_scale64: float32, %out_scale64: float32, %clip_min64: float32, %clip_max64: float32, %in_scale42: float32, %out_scale42: float32, %clip_min42: float32, %clip_max42: float32, %in_scale43: float32, %out_scale43: float32, %clip_min43: float32, %clip_max43: float32, %in_scale44: float32, %out_scale44: float32, %clip_min44: float32, %clip_max44: float32, %in_scale45: float32, %out_scale45: float32, %clip_min45: float32, %clip_max45: float32, %in_scale46: float32, %out_scale46: float32, %clip_min46: float32, %clip_max46: float32, %in_scale47: float32, %out_scale47: float32, %clip_min47: float32, %clip_max47: float32, %in_scale48: float32, %out_scale48: float32, %clip_min48: float32, %clip_max48: float32, %in_scale49: float32, %out_scale49: float32, %clip_min49: float32, %clip_max49: float32, %in_scale50: float32, %out_scale50: float32, %clip_min50: float32, %clip_max50: float32, %in_scale51: float32, %out_scale51: float32, %clip_min51: float32, %clip_max51: float32, %in_scale52: float32, %out_scale52: float32, %clip_min52: float32, %clip_max52: float32, %in_scale53: float32, %out_scale53: float32, %clip_min53: float32, %clip_max53: float32, %in_scale54: float32, %out_scale54: float32, %clip_min54: float32, %clip_max54: float32, %in_scale55: float32, %out_scale55: float32, %clip_min55: float32, %clip_max55: float32, %in_scale56: float32, %out_scale56: float32, %clip_min56: float32, %clip_max56: float32, %in_scale65: float32, %out_scale65: float32, %clip_min65: float32, %clip_max65: float32, %in_scale57: float32, %out_scale57: float32, %clip_min57: float32, %clip_max57: float32, %in_scale58: float32, %out_scale58: float32, %clip_min58: float32, %clip_max58: float32, %in_scale59: float32, %out_scale59: float32, %clip_min59: float32, %clip_max59: float32, %in_scale60: float32, %out_scale60: float32, %clip_min60: float32, %clip_max60: float32, %in_scale61: float32, %out_scale61: float32, %clip_min61: float32, %clip_max61: float32, %in_scale62: float32, %out_scale62: float32, %clip_min62: float32, %clip_max62: float32, %in_scale66: float32, %out_scale66: float32, %clip_min66: float32, %clip_max66: float32, %in_scale67: float32, %out_scale67: float32, %clip_min67: float32, %clip_max67: float32, %in_scale68: float32, %out_scale68: float32, %clip_min68: float32, %clip_max68: float32, %in_scale69: float32, %out_scale69: float32, %clip_min69: float32, %clip_max69: float32, %in_scale70: float32, %out_scale70: float32, %clip_min70: float32, %clip_max70: float32, %in_scale71: float32, %out_scale71: float32, %clip_min71: float32, %clip_max71: float32, %in_scale103: float32, %out_scale103: float32, %clip_min103: float32, %clip_max103: float32, %in_scale72: float32, %out_scale72: float32, %clip_min72: float32, %clip_max72: float32, %in_scale73: float32, %out_scale73: float32, %clip_min73: float32, %clip_max73: float32, %in_scale74: float32, %out_scale74: float32, %clip_min74: float32, %clip_max74: float32, %in_scale75: float32, %out_scale75: float32, %clip_min75: float32, %clip_max75: float32, %in_scale76: float32, %out_scale76: float32, %clip_min76: float32, %clip_max76: float32, %in_scale77: float32, %out_scale77: float32, %clip_min77: float32, %clip_max77: float32, %in_scale78: float32, %out_scale78: float32, %clip_min78: float32, %clip_max78: float32, %in_scale79: float32, %out_scale79: float32, %clip_min79: float32, %clip_max79: float32, %in_scale80: float32, %out_scale80: float32, %clip_min80: float32, %clip_max80: float32, %in_scale81: float32, %out_scale81: float32, %clip_min81: float32, %clip_max81: float32, %in_scale104: float32, %out_scale104: float32, %clip_min104: float32, %clip_max104: float32, %in_scale82: float32, %out_scale82: float32, %clip_min82: float32, %clip_max82: float32, %in_scale83: float32, %out_scale83: float32, %clip_min83: float32, %clip_max83: float32, %in_scale84: float32, %out_scale84: float32, %clip_min84: float32, %clip_max84: float32, %in_scale85: float32, %out_scale85: float32, %clip_min85: float32, %clip_max85: float32, %in_scale86: float32, %out_scale86: float32, %clip_min86: float32, %clip_max86: float32, %in_scale87: float32, %out_scale87: float32, %clip_min87: float32, %clip_max87: float32, %in_scale88: float32, %out_scale88: float32, %clip_min88: float32, %clip_max88: float32, %in_scale89: float32, %out_scale89: float32, %clip_min89: float32, %clip_max89: float32, %in_scale90: float32, %out_scale90: float32, %clip_min90: float32, %clip_max90: float32, %in_scale91: float32, %out_scale91: float32, %clip_min91: float32, %clip_max91: float32, %in_scale92: float32, %out_scale92: float32, %clip_min92: float32, %clip_max92: float32, %in_scale93: float32, %out_scale93: float32, %clip_min93: float32, %clip_max93: float32, %in_scale94: float32, %out_scale94: float32, %clip_min94: float32, %clip_max94: float32, %in_scale95: float32, %out_scale95: float32, %clip_min95: float32, %clip_max95: float32, %in_scale96: float32, %out_scale96: float32, %clip_min96: float32, %clip_max96: float32, %in_scale105: float32, %out_scale105: float32, %clip_min105: float32, %clip_max105: float32, %in_scale97: float32, %out_scale97: float32, %clip_min97: float32, %clip_max97: float32, %in_scale98: float32, %out_scale98: float32, %clip_min98: float32, %clip_max98: float32, %in_scale99: float32, %out_scale99: float32, %clip_min99: float32, %clip_max99: float32, %in_scale100: float32, %out_scale100: float32, %clip_min100: float32, %clip_max100: float32, %in_scale101: float32, %out_scale101: float32, %clip_min101: float32, %clip_max101: float32, %in_scale102: float32, %out_scale102: float32, %clip_min102: float32, %clip_max102: float32, %in_scale106: float32, %out_scale106: float32, %clip_min106: float32, %clip_max106: float32, %in_scale107: float32, %out_scale107: float32, %clip_min107: float32, %clip_max107: float32, %in_scale108: float32, %out_scale108: float32, %clip_min108: float32, %clip_max108: float32, %in_scale109: float32, %out_scale109: float32, %clip_min109: float32, %clip_max109: float32, %in_scale110: float32, %out_scale110: float32, %clip_min110: float32, %clip_max110: float32, %in_scale111: float32, %out_scale111: float32, %clip_min111: float32, %clip_max111: float32, %in_scale143: float32, %out_scale143: float32, %clip_min143: float32, %clip_max143: float32, %in_scale112: float32, %out_scale112: float32, %clip_min112: float32, %clip_max112: float32, %in_scale113: float32, %out_scale113: float32, %clip_min113: float32, %clip_max113: float32, %in_scale114: float32, %out_scale114: float32, %clip_min114: float32, %clip_max114: float32, %in_scale115: float32, %out_scale115: float32, %clip_min115: float32, %clip_max115: float32, %in_scale116: float32, %out_scale116: float32, %clip_min116: float32, %clip_max116: float32, %in_scale117: float32, %out_scale117: float32, %clip_min117: float32, %clip_max117: float32, %in_scale118: float32, %out_scale118: float32, %clip_min118: float32, %clip_max118: float32, %in_scale119: float32, %out_scale119: float32, %clip_min119: float32, %clip_max119: float32, %in_scale120: float32, %out_scale120: float32, %clip_min120: float32, %clip_max120: float32, %in_scale121: float32, %out_scale121: float32, %clip_min121: float32, %clip_max121: float32, %in_scale144: float32, %out_scale144: float32, %clip_min144: float32, %clip_max144: float32, %in_scale122: float32, %out_scale122: float32, %clip_min122: float32, %clip_max122: float32, %in_scale123: float32, %out_scale123: float32, %clip_min123: float32, %clip_max123: float32, %in_scale124: float32, %out_scale124: float32, %clip_min124: float32, %clip_max124: float32, %in_scale125: float32, %out_scale125: float32, %clip_min125: float32, %clip_max125: float32, %in_scale126: float32, %out_scale126: float32, %clip_min126: float32, %clip_max126: float32, %in_scale127: float32, %out_scale127: float32, %clip_min127: float32, %clip_max127: float32, %in_scale128: float32, %out_scale128: float32, %clip_min128: float32, %clip_max128: float32, %in_scale129: float32, %out_scale129: float32, %clip_min129: float32, %clip_max129: float32, %in_scale130: float32, %out_scale130: float32, %clip_min130: float32, %clip_max130: float32, %in_scale131: float32, %out_scale131: float32, %clip_min131: float32, %clip_max131: float32, %in_scale132: float32, %out_scale132: float32, %clip_min132: float32, %clip_max132: float32, %in_scale133: float32, %out_scale133: float32, %clip_min133: float32, %clip_max133: float32, %in_scale134: float32, %out_scale134: float32, %clip_min134: float32, %clip_max134: float32, %in_scale135: float32, %out_scale135: float32, %clip_min135: float32, %clip_max135: float32, %in_scale136: float32, %out_scale136: float32, %clip_min136: float32, %clip_max136: float32, %in_scale145: float32, %out_scale145: float32, %clip_min145: float32, %clip_max145: float32, %in_scale137: float32, %out_scale137: float32, %clip_min137: float32, %clip_max137: float32, %in_scale138: float32, %out_scale138: float32, %clip_min138: float32, %clip_max138: float32, %in_scale139: float32, %out_scale139: float32, %clip_min139: float32, %clip_max139: float32, %in_scale140: float32, %out_scale140: float32, %clip_min140: float32, %clip_max140: float32, %in_scale141: float32, %out_scale141: float32, %clip_min141: float32, %clip_max141: float32, %in_scale142: float32, %out_scale142: float32, %clip_min142: float32, %clip_max142: float32, %in_scale146: float32, %out_scale146: float32, %clip_min146: float32, %clip_max146: float32, %in_scale147: float32, %out_scale147: float32, %clip_min147: float32, %clip_max147: float32, %in_scale148: float32, %out_scale148: float32, %clip_min148: float32, %clip_max148: float32, %in_scale149: float32, %out_scale149: float32, %clip_min149: float32, %clip_max149: float32, %in_scale150: float32, %out_scale150: float32, %clip_min150: float32, %clip_max150: float32, %in_scale151: float32, %out_scale151: float32, %clip_min151: float32, %clip_max151: float32, %in_scale168: float32, %out_scale168: float32, %clip_min168: float32, %clip_max168: float32, %in_scale152: float32, %out_scale152: float32, %clip_min152: float32, %clip_max152: float32, %in_scale153: float32, %out_scale153: float32, %clip_min153: float32, %clip_max153: float32, %in_scale154: float32, %out_scale154: float32, %clip_min154: float32, %clip_max154: float32, %in_scale155: float32, %out_scale155: float32, %clip_min155: float32, %clip_max155: float32, %in_scale156: float32, %out_scale156: float32, %clip_min156: float32, %clip_max156: float32, %in_scale157: float32, %out_scale157: float32, %clip_min157: float32, %clip_max157: float32, %in_scale158: float32, %out_scale158: float32, %clip_min158: float32, %clip_max158: float32, %in_scale159: float32, %out_scale159: float32, %clip_min159: float32, %clip_max159: float32, %in_scale160: float32, %out_scale160: float32, %clip_min160: float32, %clip_max160: float32, %in_scale161: float32, %out_scale161: float32, %clip_min161: float32, %clip_max161: float32, %in_scale162: float32, %out_scale162: float32, %clip_min162: float32, %clip_max162: float32, %in_scale163: float32, %out_scale163: float32, %clip_min163: float32, %clip_max163: float32, %in_scale164: float32, %out_scale164: float32, %clip_min164: float32, %clip_max164: float32, %in_scale165: float32, %out_scale165: float32, %clip_min165: float32, %clip_max165: float32, %in_scale166: float32, %out_scale166: float32, %clip_min166: float32, %clip_max166: float32, %in_scale169: float32, %out_scale169: float32, %clip_min169: float32, %clip_max169: float32, %in_scale167: float32, %out_scale167: float32, %clip_min167: float32, %clip_max167: float32, %in_scale170: float32, %out_scale170: float32, %clip_min170: float32, %clip_max170: float32, %in_scale171: float32, %out_scale171: float32, %clip_min171: float32, %clip_max171: float32, %in_scale172: float32, %out_scale172: float32, %clip_min172: float32, %clip_max172: float32, %in_scale173: float32, %out_scale173: float32, %clip_min173: float32, %clip_max173: float32, %in_scale174: float32, %out_scale174: float32, %clip_min174: float32, %clip_max174: float32, %in_scale175: float32, %out_scale175: float32, %clip_min175: float32, %clip_max175: float32, %in_scale222: float32, %out_scale222: float32, %clip_min222: float32, %clip_max222: float32, %in_scale176: float32, %out_scale176: float32, %clip_min176: float32, %clip_max176: float32, %in_scale177: float32, %out_scale177: float32, %clip_min177: float32, %clip_max177: float32, %in_scale178: float32, %out_scale178: float32, %clip_min178: float32, %clip_max178: float32, %in_scale179: float32, %out_scale179: float32, %clip_min179: float32, %clip_max179: float32, %in_scale180: float32, %out_scale180: float32, %clip_min180: float32, %clip_max180: float32, %in_scale181: float32, %out_scale181: float32, %clip_min181: float32, %clip_max181: float32, %in_scale182: float32, %out_scale182: float32, %clip_min182: float32, %clip_max182: float32, %in_scale183: float32, %out_scale183: float32, %clip_min183: float32, %clip_max183: float32, %in_scale184: float32, %out_scale184: float32, %clip_min184: float32, %clip_max184: float32, %in_scale185: float32, %out_scale185: float32, %clip_min185: float32, %clip_max185: float32, %in_scale186: float32, %out_scale186: float32, %clip_min186: float32, %clip_max186: float32, %in_scale187: float32, %out_scale187: float32, %clip_min187: float32, %clip_max187: float32, %in_scale188: float32, %out_scale188: float32, %clip_min188: float32, %clip_max188: float32, %in_scale189: float32, %out_scale189: float32, %clip_min189: float32, %clip_max189: float32, %in_scale190: float32, %out_scale190: float32, %clip_min190: float32, %clip_max190: float32, %in_scale223: float32, %out_scale223: float32, %clip_min223: float32, %clip_max223: float32, %in_scale191: float32, %out_scale191: float32, %clip_min191: float32, %clip_max191: float32, %in_scale192: float32, %out_scale192: float32, %clip_min192: float32, %clip_max192: float32, %in_scale193: float32, %out_scale193: float32, %clip_min193: float32, %clip_max193: float32, %in_scale194: float32, %out_scale194: float32, %clip_min194: float32, %clip_max194: float32, %in_scale195: float32, %out_scale195: float32, %clip_min195: float32, %clip_max195: float32, %in_scale196: float32, %out_scale196: float32, %clip_min196: float32, %clip_max196: float32, %in_scale197: float32, %out_scale197: float32, %clip_min197: float32, %clip_max197: float32, %in_scale198: float32, %out_scale198: float32, %clip_min198: float32, %clip_max198: float32, %in_scale199: float32, %out_scale199: float32, %clip_min199: float32, %clip_max199: float32, %in_scale200: float32, %out_scale200: float32, %clip_min200: float32, %clip_max200: float32, %in_scale201: float32, %out_scale201: float32, %clip_min201: float32, %clip_max201: float32, %in_scale202: float32, %out_scale202: float32, %clip_min202: float32, %clip_max202: float32, %in_scale203: float32, %out_scale203: float32, %clip_min203: float32, %clip_max203: float32, %in_scale204: float32, %out_scale204: float32, %clip_min204: float32, %clip_max204: float32, %in_scale205: float32, %out_scale205: float32, %clip_min205: float32, %clip_max205: float32, %in_scale206: float32, %out_scale206: float32, %clip_min206: float32, %clip_max206: float32, %in_scale207: float32, %out_scale207: float32, %clip_min207: float32, %clip_max207: float32, %in_scale208: float32, %out_scale208: float32, %clip_min208: float32, %clip_max208: float32, %in_scale209: float32, %out_scale209: float32, %clip_min209: float32, %clip_max209: float32, %in_scale210: float32, %out_scale210: float32, %clip_min210: float32, %clip_max210: float32, %in_scale211: float32, %out_scale211: float32, %clip_min211: float32, %clip_max211: float32, %in_scale212: float32, %out_scale212: float32, %clip_min212: float32, %clip_max212: float32, %in_scale213: float32, %out_scale213: float32, %clip_min213: float32, %clip_max213: float32, %in_scale214: float32, %out_scale214: float32, %clip_min214: float32, %clip_max214: float32, %in_scale215: float32, %out_scale215: float32, %clip_min215: float32, %clip_max215: float32, %in_scale224: float32, %out_scale224: float32, %clip_min224: float32, %clip_max224: float32, %in_scale216: float32, %out_scale216: float32, %clip_min216: float32, %clip_max216: float32, %in_scale217: float32, %out_scale217: float32, %clip_min217: float32, %clip_max217: float32, %in_scale218: float32, %out_scale218: float32, %clip_min218: float32, %clip_max218: float32, %in_scale219: float32, %out_scale219: float32, %clip_min219: float32, %clip_max219: float32, %in_scale220: float32, %out_scale220: float32, %clip_min220: float32, %clip_max220: float32, %in_scale221: float32, %out_scale221: float32, %clip_min221: float32, %clip_max221: float32, %in_scale225: float32, %out_scale225: float32, %clip_min225: float32, %clip_max225: float32, %in_scale226: float32, %out_scale226: float32, %clip_min226: float32, %clip_max226: float32, %in_scale227: float32, %out_scale227: float32, %clip_min227: float32, %clip_max227: float32, %in_scale228: float32, %out_scale228: float32, %clip_min228: float32, %clip_max228: float32, %in_scale229: float32, %out_scale229: float32, %clip_min229: float32, %clip_max229: float32, %in_scale230: float32, %out_scale230: float32, %clip_min230: float32, %clip_max230: float32, %in_scale277: float32, %out_scale277: float32, %clip_min277: float32, %clip_max277: float32, %in_scale231: float32, %out_scale231: float32, %clip_min231: float32, %clip_max231: float32, %in_scale232: float32, %out_scale232: float32, %clip_min232: float32, %clip_max232: float32, %in_scale233: float32, %out_scale233: float32, %clip_min233: float32, %clip_max233: float32, %in_scale234: float32, %out_scale234: float32, %clip_min234: float32, %clip_max234: float32, %in_scale235: float32, %out_scale235: float32, %clip_min235: float32, %clip_max235: float32, %in_scale236: float32, %out_scale236: float32, %clip_min236: float32, %clip_max236: float32, %in_scale237: float32, %out_scale237: float32, %clip_min237: float32, %clip_max237: float32, %in_scale238: float32, %out_scale238: float32, %clip_min238: float32, %clip_max238: float32, %in_scale239: float32, %out_scale239: float32, %clip_min239: float32, %clip_max239: float32, %in_scale240: float32, %out_scale240: float32, %clip_min240: float32, %clip_max240: float32, %in_scale241: float32, %out_scale241: float32, %clip_min241: float32, %clip_max241: float32, %in_scale242: float32, %out_scale242: float32, %clip_min242: float32, %clip_max242: float32, %in_scale243: float32, %out_scale243: float32, %clip_min243: float32, %clip_max243: float32, %in_scale244: float32, %out_scale244: float32, %clip_min244: float32, %clip_max244: float32, %in_scale245: float32, %out_scale245: float32, %clip_min245: float32, %clip_max245: float32, %in_scale278: float32, %out_scale278: float32, %clip_min278: float32, %clip_max278: float32, %in_scale246: float32, %out_scale246: float32, %clip_min246: float32, %clip_max246: float32, %in_scale247: float32, %out_scale247: float32, %clip_min247: float32, %clip_max247: float32, %in_scale248: float32, %out_scale248: float32, %clip_min248: float32, %clip_max248: float32, %in_scale249: float32, %out_scale249: float32, %clip_min249: float32, %clip_max249: float32, %in_scale250: float32, %out_scale250: float32, %clip_min250: float32, %clip_max250: float32, %in_scale251: float32, %out_scale251: float32, %clip_min251: float32, %clip_max251: float32, %in_scale252: float32, %out_scale252: float32, %clip_min252: float32, %clip_max252: float32, %in_scale253: float32, %out_scale253: float32, %clip_min253: float32, %clip_max253: float32, %in_scale254: float32, %out_scale254: float32, %clip_min254: float32, %clip_max254: float32, %in_scale255: float32, %out_scale255: float32, %clip_min255: float32, %clip_max255: float32, %in_scale256: float32, %out_scale256: float32, %clip_min256: float32, %clip_max256: float32, %in_scale257: float32, %out_scale257: float32, %clip_min257: float32, %clip_max257: float32, %in_scale258: float32, %out_scale258: float32, %clip_min258: float32, %clip_max258: float32, %in_scale259: float32, %out_scale259: float32, %clip_min259: float32, %clip_max259: float32, %in_scale260: float32, %out_scale260: float32, %clip_min260: float32, %clip_max260: float32, %in_scale261: float32, %out_scale261: float32, %clip_min261: float32, %clip_max261: float32, %in_scale262: float32, %out_scale262: float32, %clip_min262: float32, %clip_max262: float32, %in_scale263: float32, %out_scale263: float32, %clip_min263: float32, %clip_max263: float32, %in_scale264: float32, %out_scale264: float32, %clip_min264: float32, %clip_max264: float32, %in_scale265: float32, %out_scale265: float32, %clip_min265: float32, %clip_max265: float32, %in_scale266: float32, %out_scale266: float32, %clip_min266: float32, %clip_max266: float32, %in_scale267: float32, %out_scale267: float32, %clip_min267: float32, %clip_max267: float32, %in_scale268: float32, %out_scale268: float32, %clip_min268: float32, %clip_max268: float32, %in_scale269: float32, %out_scale269: float32, %clip_min269: float32, %clip_max269: float32, %in_scale270: float32, %out_scale270: float32, %clip_min270: float32, %clip_max270: float32, %in_scale279: float32, %out_scale279: float32, %clip_min279: float32, %clip_max279: float32, %in_scale271: float32, %out_scale271: float32, %clip_min271: float32, %clip_max271: float32, %in_scale272: float32, %out_scale272: float32, %clip_min272: float32, %clip_max272: float32, %in_scale273: float32, %out_scale273: float32, %clip_min273: float32, %clip_max273: float32, %in_scale274: float32, %out_scale274: float32, %clip_min274: float32, %clip_max274: float32, %in_scale275: float32, %out_scale275: float32, %clip_min275: float32, %clip_max275: float32, %in_scale276: float32, %out_scale276: float32, %clip_min276: float32, %clip_max276: float32, %in_scale280: float32, %out_scale280: float32, %clip_min280: float32, %clip_max280: float32, %in_scale281: float32, %out_scale281: float32, %clip_min281: float32, %clip_max281: float32, %in_scale282: float32, %out_scale282: float32, %clip_min282: float32, %clip_max282: float32, %in_scale283: float32, %out_scale283: float32, %clip_min283: float32, %clip_max283: float32, %in_scale284: float32, %out_scale284: float32, %clip_min284: float32, %clip_max284: float32, %in_scale285: float32, %out_scale285: float32, %clip_min285: float32, %clip_max285: float32, %in_scale332: float32, %out_scale332: float32, %clip_min332: float32, %clip_max332: float32, %in_scale286: float32, %out_scale286: float32, %clip_min286: float32, %clip_max286: float32, %in_scale287: float32, %out_scale287: float32, %clip_min287: float32, %clip_max287: float32, %in_scale288: float32, %out_scale288: float32, %clip_min288: float32, %clip_max288: float32, %in_scale289: float32, %out_scale289: float32, %clip_min289: float32, %clip_max289: float32, %in_scale290: float32, %out_scale290: float32, %clip_min290: float32, %clip_max290: float32, %in_scale291: float32, %out_scale291: float32, %clip_min291: float32, %clip_max291: float32, %in_scale292: float32, %out_scale292: float32, %clip_min292: float32, %clip_max292: float32, %in_scale293: float32, %out_scale293: float32, %clip_min293: float32, %clip_max293: float32, %in_scale294: float32, %out_scale294: float32, %clip_min294: float32, %clip_max294: float32, %in_scale295: float32, %out_scale295: float32, %clip_min295: float32, %clip_max295: float32, %in_scale296: float32, %out_scale296: float32, %clip_min296: float32, %clip_max296: float32, %in_scale297: float32, %out_scale297: float32, %clip_min297: float32, %clip_max297: float32, %in_scale298: float32, %out_scale298: float32, %clip_min298: float32, %clip_max298: float32, %in_scale299: float32, %out_scale299: float32, %clip_min299: float32, %clip_max299: float32, %in_scale300: float32, %out_scale300: float32, %clip_min300: float32, %clip_max300: float32, %in_scale333: float32, %out_scale333: float32, %clip_min333: float32, %clip_max333: float32, %in_scale301: float32, %out_scale301: float32, %clip_min301: float32, %clip_max301: float32, %in_scale302: float32, %out_scale302: float32, %clip_min302: float32, %clip_max302: float32, %in_scale303: float32, %out_scale303: float32, %clip_min303: float32, %clip_max303: float32, %in_scale304: float32, %out_scale304: float32, %clip_min304: float32, %clip_max304: float32, %in_scale305: float32, %out_scale305: float32, %clip_min305: float32, %clip_max305: float32, %in_scale306: float32, %out_scale306: float32, %clip_min306: float32, %clip_max306: float32, %in_scale307: float32, %out_scale307: float32, %clip_min307: float32, %clip_max307: float32, %in_scale308: float32, %out_scale308: float32, %clip_min308: float32, %clip_max308: float32, %in_scale309: float32, %out_scale309: float32, %clip_min309: float32, %clip_max309: float32, %in_scale310: float32, %out_scale310: float32, %clip_min310: float32, %clip_max310: float32, %in_scale311: float32, %out_scale311: float32, %clip_min311: float32, %clip_max311: float32, %in_scale312: float32, %out_scale312: float32, %clip_min312: float32, %clip_max312: float32, %in_scale313: float32, %out_scale313: float32, %clip_min313: float32, %clip_max313: float32, %in_scale314: float32, %out_scale314: float32, %clip_min314: float32, %clip_max314: float32, %in_scale315: float32, %out_scale315: float32, %clip_min315: float32, %clip_max315: float32, %in_scale316: float32, %out_scale316: float32, %clip_min316: float32, %clip_max316: float32, %in_scale317: float32, %out_scale317: float32, %clip_min317: float32, %clip_max317: float32, %in_scale318: float32, %out_scale318: float32, %clip_min318: float32, %clip_max318: float32, %in_scale319: float32, %out_scale319: float32, %clip_min319: float32, %clip_max319: float32, %in_scale320: float32, %out_scale320: float32, %clip_min320: float32, %clip_max320: float32, %in_scale321: float32, %out_scale321: float32, %clip_min321: float32, %clip_max321: float32, %in_scale322: float32, %out_scale322: float32, %clip_min322: float32, %clip_max322: float32, %in_scale323: float32, %out_scale323: float32, %clip_min323: float32, %clip_max323: float32, %in_scale324: float32, %out_scale324: float32, %clip_min324: float32, %clip_max324: float32, %in_scale325: float32, %out_scale325: float32, %clip_min325: float32, %clip_max325: float32, %in_scale334: float32, %out_scale334: float32, %clip_min334: float32, %clip_max334: float32, %in_scale326: float32, %out_scale326: float32, %clip_min326: float32, %clip_max326: float32, %in_scale327: float32, %out_scale327: float32, %clip_min327: float32, %clip_max327: float32, %in_scale328: float32, %out_scale328: float32, %clip_min328: float32, %clip_max328: float32, %in_scale329: float32, %out_scale329: float32, %clip_min329: float32, %clip_max329: float32, %in_scale330: float32, %out_scale330: float32, %clip_min330: float32, %clip_max330: float32, %in_scale331: float32, %out_scale331: float32, %clip_min331: float32, %clip_max331: float32, %in_scale335: float32, %out_scale335: float32, %clip_min335: float32, %clip_max335: float32, %in_scale336: float32, %out_scale336: float32, %clip_min336: float32, %clip_max336: float32, %in_scale337: float32, %out_scale337: float32, %clip_min337: float32, %clip_max337: float32, %in_scale338: float32, %out_scale338: float32, %clip_min338: float32, %clip_max338: float32, %in_scale339: float32, %out_scale339: float32, %clip_min339: float32, %clip_max339: float32, %in_scale340: float32, %out_scale340: float32, %clip_min340: float32, %clip_max340: float32, %in_scale387: float32, %out_scale387: float32, %clip_min387: float32, %clip_max387: float32, %in_scale341: float32, %out_scale341: float32, %clip_min341: float32, %clip_max341: float32, %in_scale342: float32, %out_scale342: float32, %clip_min342: float32, %clip_max342: float32, %in_scale343: float32, %out_scale343: float32, %clip_min343: float32, %clip_max343: float32, %in_scale344: float32, %out_scale344: float32, %clip_min344: float32, %clip_max344: float32, %in_scale345: float32, %out_scale345: float32, %clip_min345: float32, %clip_max345: float32, %in_scale346: float32, %out_scale346: float32, %clip_min346: float32, %clip_max346: float32, %in_scale347: float32, %out_scale347: float32, %clip_min347: float32, %clip_max347: float32, %in_scale348: float32, %out_scale348: float32, %clip_min348: float32, %clip_max348: float32, %in_scale349: float32, %out_scale349: float32, %clip_min349: float32, %clip_max349: float32, %in_scale350: float32, %out_scale350: float32, %clip_min350: float32, %clip_max350: float32, %in_scale351: float32, %out_scale351: float32, %clip_min351: float32, %clip_max351: float32, %in_scale352: float32, %out_scale352: float32, %clip_min352: float32, %clip_max352: float32, %in_scale353: float32, %out_scale353: float32, %clip_min353: float32, %clip_max353: float32, %in_scale354: float32, %out_scale354: float32, %clip_min354: float32, %clip_max354: float32, %in_scale355: float32, %out_scale355: float32, %clip_min355: float32, %clip_max355: float32, %in_scale388: float32, %out_scale388: float32, %clip_min388: float32, %clip_max388: float32, %in_scale356: float32, %out_scale356: float32, %clip_min356: float32, %clip_max356: float32, %in_scale357: float32, %out_scale357: float32, %clip_min357: float32, %clip_max357: float32, %in_scale358: float32, %out_scale358: float32, %clip_min358: float32, %clip_max358: float32, %in_scale359: float32, %out_scale359: float32, %clip_min359: float32, %clip_max359: float32, %in_scale360: float32, %out_scale360: float32, %clip_min360: float32, %clip_max360: float32, %in_scale361: float32, %out_scale361: float32, %clip_min361: float32, %clip_max361: float32, %in_scale362: float32, %out_scale362: float32, %clip_min362: float32, %clip_max362: float32, %in_scale363: float32, %out_scale363: float32, %clip_min363: float32, %clip_max363: float32, %in_scale364: float32, %out_scale364: float32, %clip_min364: float32, %clip_max364: float32, %in_scale365: float32, %out_scale365: float32, %clip_min365: float32, %clip_max365: float32, %in_scale366: float32, %out_scale366: float32, %clip_min366: float32, %clip_max366: float32, %in_scale367: float32, %out_scale367: float32, %clip_min367: float32, %clip_max367: float32, %in_scale368: float32, %out_scale368: float32, %clip_min368: float32, %clip_max368: float32, %in_scale369: float32, %out_scale369: float32, %clip_min369: float32, %clip_max369: float32, %in_scale370: float32, %out_scale370: float32, %clip_min370: float32, %clip_max370: float32, %in_scale371: float32, %out_scale371: float32, %clip_min371: float32, %clip_max371: float32, %in_scale372: float32, %out_scale372: float32, %clip_min372: float32, %clip_max372: float32, %in_scale373: float32, %out_scale373: float32, %clip_min373: float32, %clip_max373: float32, %in_scale374: float32, %out_scale374: float32, %clip_min374: float32, %clip_max374: float32, %in_scale375: float32, %out_scale375: float32, %clip_min375: float32, %clip_max375: float32, %in_scale376: float32, %out_scale376: float32, %clip_min376: float32, %clip_max376: float32, %in_scale377: float32, %out_scale377: float32, %clip_min377: float32, %clip_max377: float32, %in_scale378: float32, %out_scale378: float32, %clip_min378: float32, %clip_max378: float32, %in_scale379: float32, %out_scale379: float32, %clip_min379: float32, %clip_max379: float32, %in_scale380: float32, %out_scale380: float32, %clip_min380: float32, %clip_max380: float32, %in_scale389: float32, %out_scale389: float32, %clip_min389: float32, %clip_max389: float32, %in_scale381: float32, %out_scale381: float32, %clip_min381: float32, %clip_max381: float32, %in_scale382: float32, %out_scale382: float32, %clip_min382: float32, %clip_max382: float32, %in_scale383: float32, %out_scale383: float32, %clip_min383: float32, %clip_max383: float32, %in_scale384: float32, %out_scale384: float32, %clip_min384: float32, %clip_max384: float32, %in_scale385: float32, %out_scale385: float32, %clip_min385: float32, %clip_max385: float32, %in_scale386: float32, %out_scale386: float32, %clip_min386: float32, %clip_max386: float32, %in_scale390: float32, %out_scale390: float32, %clip_min390: float32, %clip_max390: float32, %in_scale391: float32, %out_scale391: float32, %clip_min391: float32, %clip_max391: float32, %in_scale392: float32, %out_scale392: float32, %clip_min392: float32, %clip_max392: float32, %in_scale393: float32, %out_scale393: float32, %clip_min393: float32, %clip_max393: float32, %in_scale394: float32, %out_scale394: float32, %clip_min394: float32, %clip_max394: float32, %in_scale395: float32, %out_scale395: float32, %clip_min395: float32, %clip_max395: float32, %in_scale396: float32, %out_scale396: float32, %clip_min396: float32, %clip_max396: float32, %in_scale397: float32, %out_scale397: float32, %clip_min397: float32, %clip_max397: float32, %in_scale398: float32, %out_scale398: float32, %clip_min398: float32, %clip_max398: float32, %in_scale399: float32, %out_scale399: float32, %clip_min399: float32, %clip_max399: float32, %in_scale400: float32, %out_scale400: float32, %clip_min400: float32, %clip_max400: float32, %in_scale422: float32, %out_scale422: float32, %clip_min422: float32, %clip_max422: float32, %in_scale401: float32, %out_scale401: float32, %clip_min401: float32, %clip_max401: float32, %in_scale402: float32, %out_scale402: float32, %clip_min402: float32, %clip_max402: float32, %in_scale403: float32, %out_scale403: float32, %clip_min403: float32, %clip_max403: float32, %in_scale404: float32, %out_scale404: float32, %clip_min404: float32, %clip_max404: float32, %in_scale405: float32, %out_scale405: float32, %clip_min405: float32, %clip_max405: float32, %in_scale406: float32, %out_scale406: float32, %clip_min406: float32, %clip_max406: float32, %in_scale407: float32, %out_scale407: float32, %clip_min407: float32, %clip_max407: float32, %in_scale408: float32, %out_scale408: float32, %clip_min408: float32, %clip_max408: float32, %in_scale409: float32, %out_scale409: float32, %clip_min409: float32, %clip_max409: float32, %in_scale410: float32, %out_scale410: float32, %clip_min410: float32, %clip_max410: float32, %in_scale411: float32, %out_scale411: float32, %clip_min411: float32, %clip_max411: float32, %in_scale412: float32, %out_scale412: float32, %clip_min412: float32, %clip_max412: float32, %in_scale413: float32, %out_scale413: float32, %clip_min413: float32, %clip_max413: float32, %in_scale414: float32, %out_scale414: float32, %clip_min414: float32, %clip_max414: float32, %in_scale415: float32, %out_scale415: float32, %clip_min415: float32, %clip_max415: float32, %in_scale416: float32, %out_scale416: float32, %clip_min416: float32, %clip_max416: float32, %in_scale417: float32, %out_scale417: float32, %clip_min417: float32, %clip_max417: float32, %in_scale418: float32, %out_scale418: float32, %clip_min418: float32, %clip_max418: float32, %in_scale419: float32, %out_scale419: float32, %clip_min419: float32, %clip_max419: float32, %in_scale420: float32, %out_scale420: float32, %clip_min420: float32, %clip_max420: float32, %in_scale423: float32, %out_scale423: float32, %clip_min423: float32, %clip_max423: float32, %in_scale421: float32, %out_scale421: float32, %clip_min421: float32, %clip_max421: float32, %in_scale424: float32, %out_scale424: float32, %clip_min424: float32, %clip_max424: float32, %in_scale425: float32, %out_scale425: float32, %clip_min425: float32, %clip_max425: float32, %in_scale426: float32, %out_scale426: float32, %clip_min426: float32, %clip_max426: float32, %in_scale427: float32, %out_scale427: float32, %clip_min427: float32, %clip_max427: float32, %in_scale428: float32, %out_scale428: float32, %clip_min428: float32, %clip_max428: float32, %in_scale429: float32, %out_scale429: float32, %clip_min429: float32, %clip_max429: float32, %in_scale475: float32, %out_scale475: float32, %clip_min475: float32, %clip_max475: float32, %in_scale430: float32, %out_scale430: float32, %clip_min430: float32, %clip_max430: float32, %in_scale431: float32, %out_scale431: float32, %clip_min431: float32, %clip_max431: float32, %in_scale432: float32, %out_scale432: float32, %clip_min432: float32, %clip_max432: float32, %in_scale433: float32, %out_scale433: float32, %clip_min433: float32, %clip_max433: float32, %in_scale434: float32, %out_scale434: float32, %clip_min434: float32, %clip_max434: float32, %in_scale435: float32, %out_scale435: float32, %clip_min435: float32, %clip_max435: float32, %in_scale436: float32, %out_scale436: float32, %clip_min436: float32, %clip_max436: float32, %in_scale437: float32, %out_scale437: float32, %clip_min437: float32, %clip_max437: float32, %in_scale438: float32, %out_scale438: float32, %clip_min438: float32, %clip_max438: float32, %in_scale439: float32, %out_scale439: float32, %clip_min439: float32, %clip_max439: float32, %in_scale445: float32, %out_scale445: float32, %clip_min445: float32, %clip_max445: float32, %in_scale440: float32, %out_scale440: float32, %clip_min440: float32, %clip_max440: float32, %in_scale441: float32, %out_scale441: float32, %clip_min441: float32, %clip_max441: float32, %in_scale442: float32, %out_scale442: float32, %clip_min442: float32, %clip_max442: float32, %in_scale443: float32, %out_scale443: float32, %clip_min443: float32, %clip_max443: float32, %in_scale444: float32, %out_scale444: float32, %clip_min444: float32, %clip_max444: float32, %in_scale446: float32, %out_scale446: float32, %clip_min446: float32, %clip_max446: float32, %in_scale476: float32, %out_scale476: float32, %clip_min476: float32, %clip_max476: float32, %in_scale447: float32, %out_scale447: float32, %clip_min447: float32, %clip_max447: float32, %in_scale448: float32, %out_scale448: float32, %clip_min448: float32, %clip_max448: float32, %in_scale449: float32, %out_scale449: float32, %clip_min449: float32, %clip_max449: float32, %in_scale450: float32, %out_scale450: float32, %clip_min450: float32, %clip_max450: float32, %in_scale451: float32, %out_scale451: float32, %clip_min451: float32, %clip_max451: float32, %in_scale452: float32, %out_scale452: float32, %clip_min452: float32, %clip_max452: float32, %in_scale453: float32, %out_scale453: float32, %clip_min453: float32, %clip_max453: float32, %in_scale454: float32, %out_scale454: float32, %clip_min454: float32, %clip_max454: float32, %in_scale455: float32, %out_scale455: float32, %clip_min455: float32, %clip_max455: float32, %in_scale456: float32, %out_scale456: float32, %clip_min456: float32, %clip_max456: float32, %in_scale457: float32, %out_scale457: float32, %clip_min457: float32, %clip_max457: float32, %in_scale458: float32, %out_scale458: float32, %clip_min458: float32, %clip_max458: float32, %in_scale459: float32, %out_scale459: float32, %clip_min459: float32, %clip_max459: float32, %in_scale460: float32, %out_scale460: float32, %clip_min460: float32, %clip_max460: float32, %in_scale461: float32, %out_scale461: float32, %clip_min461: float32, %clip_max461: float32, %in_scale467: float32, %out_scale467: float32, %clip_min467: float32, %clip_max467: float32, %in_scale462: float32, %out_scale462: float32, %clip_min462: float32, %clip_max462: float32, %in_scale463: float32, %out_scale463: float32, %clip_min463: float32, %clip_max463: float32, %in_scale464: float32, %out_scale464: float32, %clip_min464: float32, %clip_max464: float32, %in_scale465: float32, %out_scale465: float32, %clip_min465: float32, %clip_max465: float32, %in_scale466: float32, %out_scale466: float32, %clip_min466: float32, %clip_max466: float32, %in_scale468: float32, %out_scale468: float32, %clip_min468: float32, %clip_max468: float32, %in_scale477: float32, %out_scale477: float32, %clip_min477: float32, %clip_max477: float32, %in_scale469: float32, %out_scale469: float32, %clip_min469: float32, %clip_max469: float32, %in_scale470: float32, %out_scale470: float32, %clip_min470: float32, %clip_max470: float32, %in_scale471: float32, %out_scale471: float32, %clip_min471: float32, %clip_max471: float32, %in_scale472: float32, %out_scale472: float32, %clip_min472: float32, %clip_max472: float32, %in_scale473: float32, %out_scale473: float32, %clip_min473: float32, %clip_max473: float32, %in_scale474: float32, %out_scale474: float32, %clip_min474: float32, %clip_max474: float32, %in_scale478: float32, %out_scale478: float32, %clip_min478: float32, %clip_max478: float32, %in_scale479: float32, %out_scale479: float32, %clip_min479: float32, %clip_max479: float32, %in_scale480: float32, %out_scale480: float32, %clip_min480: float32, %clip_max480: float32, %in_scale481: float32, %out_scale481: float32, %clip_min481: float32, %clip_max481: float32, %in_scale482: float32, %out_scale482: float32, %clip_min482: float32, %clip_max482: float32, %in_scale483: float32, %out_scale483: float32, %clip_min483: float32, %clip_max483: float32, %in_scale529: float32, %out_scale529: float32, %clip_min529: float32, %clip_max529: float32, %in_scale484: float32, %out_scale484: float32, %clip_min484: float32, %clip_max484: float32, %in_scale485: float32, %out_scale485: float32, %clip_min485: float32, %clip_max485: float32, %in_scale486: float32, %out_scale486: float32, %clip_min486: float32, %clip_max486: float32, %in_scale487: float32, %out_scale487: float32, %clip_min487: float32, %clip_max487: float32, %in_scale488: float32, %out_scale488: float32, %clip_min488: float32, %clip_max488: float32, %in_scale489: float32, %out_scale489: float32, %clip_min489: float32, %clip_max489: float32, %in_scale490: float32, %out_scale490: float32, %clip_min490: float32, %clip_max490: float32, %in_scale491: float32, %out_scale491: float32, %clip_min491: float32, %clip_max491: float32, %in_scale492: float32, %out_scale492: float32, %clip_min492: float32, %clip_max492: float32, %in_scale493: float32, %out_scale493: float32, %clip_min493: float32, %clip_max493: float32, %in_scale499: float32, %out_scale499: float32, %clip_min499: float32, %clip_max499: float32, %in_scale494: float32, %out_scale494: float32, %clip_min494: float32, %clip_max494: float32, %in_scale495: float32, %out_scale495: float32, %clip_min495: float32, %clip_max495: float32, %in_scale496: float32, %out_scale496: float32, %clip_min496: float32, %clip_max496: float32, %in_scale497: float32, %out_scale497: float32, %clip_min497: float32, %clip_max497: float32, %in_scale498: float32, %out_scale498: float32, %clip_min498: float32, %clip_max498: float32, %in_scale500: float32, %out_scale500: float32, %clip_min500: float32, %clip_max500: float32, %in_scale530: float32, %out_scale530: float32, %clip_min530: float32, %clip_max530: float32, %in_scale501: float32, %out_scale501: float32, %clip_min501: float32, %clip_max501: float32, %in_scale502: float32, %out_scale502: float32, %clip_min502: float32, %clip_max502: float32, %in_scale503: float32, %out_scale503: float32, %clip_min503: float32, %clip_max503: float32, %in_scale504: float32, %out_scale504: float32, %clip_min504: float32, %clip_max504: float32, %in_scale505: float32, %out_scale505: float32, %clip_min505: float32, %clip_max505: float32, %in_scale506: float32, %out_scale506: float32, %clip_min506: float32, %clip_max506: float32, %in_scale507: float32, %out_scale507: float32, %clip_min507: float32, %clip_max507: float32, %in_scale508: float32, %out_scale508: float32, %clip_min508: float32, %clip_max508: float32, %in_scale509: float32, %out_scale509: float32, %clip_min509: float32, %clip_max509: float32, %in_scale510: float32, %out_scale510: float32, %clip_min510: float32, %clip_max510: float32, %in_scale511: float32, %out_scale511: float32, %clip_min511: float32, %clip_max511: float32, %in_scale512: float32, %out_scale512: float32, %clip_min512: float32, %clip_max512: float32, %in_scale513: float32, %out_scale513: float32, %clip_min513: float32, %clip_max513: float32, %in_scale514: float32, %out_scale514: float32, %clip_min514: float32, %clip_max514: float32, %in_scale515: float32, %out_scale515: float32, %clip_min515: float32, %clip_max515: float32, %in_scale521: float32, %out_scale521: float32, %clip_min521: float32, %clip_max521: float32, %in_scale516: float32, %out_scale516: float32, %clip_min516: float32, %clip_max516: float32, %in_scale517: float32, %out_scale517: float32, %clip_min517: float32, %clip_max517: float32, %in_scale518: float32, %out_scale518: float32, %clip_min518: float32, %clip_max518: float32, %in_scale519: float32, %out_scale519: float32, %clip_min519: float32, %clip_max519: float32, %in_scale520: float32, %out_scale520: float32, %clip_min520: float32, %clip_max520: float32, %in_scale522: float32, %out_scale522: float32, %clip_min522: float32, %clip_max522: float32, %in_scale531: float32, %out_scale531: float32, %clip_min531: float32, %clip_max531: float32, %in_scale523: float32, %out_scale523: float32, %clip_min523: float32, %clip_max523: float32, %in_scale524: float32, %out_scale524: float32, %clip_min524: float32, %clip_max524: float32, %in_scale525: float32, %out_scale525: float32, %clip_min525: float32, %clip_max525: float32, %in_scale526: float32, %out_scale526: float32, %clip_min526: float32, %clip_max526: float32, %in_scale527: float32, %out_scale527: float32, %clip_min527: float32, %clip_max527: float32, %in_scale528: float32, %out_scale528: float32, %clip_min528: float32, %clip_max528: float32, %in_scale532: float32, %out_scale532: float32, %clip_min532: float32, %clip_max532: float32, %in_scale533: float32, %out_scale533: float32, %clip_min533: float32, %clip_max533: float32, %in_scale534: float32, %out_scale534: float32, %clip_min534: float32, %clip_max534: float32, %in_scale535: float32, %out_scale535: float32, %clip_min535: float32, %clip_max535: float32, %in_scale536: float32, %out_scale536: float32, %clip_min536: float32, %clip_max536: float32, %in_scale537: float32, %out_scale537: float32, %clip_min537: float32, %clip_max537: float32, %in_scale538: float32, %out_scale538: float32, %clip_min538: float32, %clip_max538: float32, %in_scale539: float32, %out_scale539: float32, %clip_min539: float32, %clip_max539: float32) -> Tensor[(32, 1000), float32] {
  %0 = nn.simulated_quantize(%data, %in_scale0, %out_scale0, %clip_min0, %clip_max0, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 3, 299, 299), float32] */;
  %1 = nn.simulated_quantize(meta[relay.Constant][0] /* ty=Tensor[(32, 3, 3, 3), float32] */ /* ty=Tensor[(32, 3, 3, 3), float32] */, %in_scale1, %out_scale1, %clip_min1, %clip_max1, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 3, 3, 3), float32] */;
  %2 = nn.conv2d(%0, %1, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %3 = nn.simulated_quantize(%2, %in_scale2, %out_scale2, %clip_min2, %clip_max2, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %4 = nn.simulated_quantize(meta[relay.Constant][1] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */, %in_scale3, %out_scale3, %clip_min3, %clip_max3, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1, 1), float32] */;
  %5 = add(%3, %4) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %6 = nn.simulated_quantize(%5, %in_scale4, %out_scale4, %clip_min4, %clip_max4, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %8 = nn.simulated_quantize(%7, %in_scale5, %out_scale5, %clip_min5, %clip_max5, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %9 = nn.simulated_quantize(meta[relay.Constant][2] /* ty=Tensor[(32, 32, 3, 3), float32] */ /* ty=Tensor[(32, 32, 3, 3), float32] */, %in_scale6, %out_scale6, %clip_min6, %clip_max6, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 32, 3, 3), float32] */;
  %10 = nn.conv2d(%8, %9, padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %11 = nn.simulated_quantize(%10, %in_scale7, %out_scale7, %clip_min7, %clip_max7, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %12 = nn.simulated_quantize(meta[relay.Constant][3] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */, %in_scale8, %out_scale8, %clip_min8, %clip_max8, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1, 1), float32] */;
  %13 = add(%11, %12) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %14 = nn.simulated_quantize(%13, %in_scale9, %out_scale9, %clip_min9, %clip_max9, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %15 = nn.relu(%14) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %16 = nn.simulated_quantize(%15, %in_scale10, %out_scale10, %clip_min10, %clip_max10, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %17 = nn.simulated_quantize(meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */ /* ty=Tensor[(64, 32, 3, 3), float32] */, %in_scale11, %out_scale11, %clip_min11, %clip_max11, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 32, 3, 3), float32] */;
  %18 = nn.conv2d(%16, %17, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %19 = nn.simulated_quantize(%18, %in_scale12, %out_scale12, %clip_min12, %clip_max12, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %20 = nn.simulated_quantize(meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale13, %out_scale13, %clip_min13, %clip_max13, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %21 = add(%19, %20) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %22 = nn.simulated_quantize(%21, %in_scale14, %out_scale14, %clip_min14, %clip_max14, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %23 = nn.relu(%22) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %24 = nn.simulated_quantize(%23, %in_scale15, %out_scale15, %clip_min15, %clip_max15, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %25 = nn.max_pool2d(%24, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 64, 73, 73), float32] */;
  %26 = nn.simulated_quantize(%25, %in_scale16, %out_scale16, %clip_min16, %clip_max16, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 73, 73), float32] */;
  %27 = nn.simulated_quantize(meta[relay.Constant][6] /* ty=Tensor[(80, 64, 1, 1), float32] */ /* ty=Tensor[(80, 64, 1, 1), float32] */, %in_scale17, %out_scale17, %clip_min17, %clip_max17, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(80, 64, 1, 1), float32] */;
  %28 = nn.conv2d(%26, %27, padding=[0, 0, 0, 0], channels=80, kernel_size=[1, 1]) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %29 = nn.simulated_quantize(%28, %in_scale18, %out_scale18, %clip_min18, %clip_max18, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %30 = nn.simulated_quantize(meta[relay.Constant][7] /* ty=Tensor[(80, 1, 1), float32] */ /* ty=Tensor[(80, 1, 1), float32] */, %in_scale19, %out_scale19, %clip_min19, %clip_max19, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(80, 1, 1), float32] */;
  %31 = add(%29, %30) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %32 = nn.simulated_quantize(%31, %in_scale20, %out_scale20, %clip_min20, %clip_max20, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %34 = nn.simulated_quantize(%33, %in_scale21, %out_scale21, %clip_min21, %clip_max21, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %35 = nn.simulated_quantize(meta[relay.Constant][8] /* ty=Tensor[(192, 80, 3, 3), float32] */ /* ty=Tensor[(192, 80, 3, 3), float32] */, %in_scale22, %out_scale22, %clip_min22, %clip_max22, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 80, 3, 3), float32] */;
  %36 = nn.conv2d(%34, %35, padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %37 = nn.simulated_quantize(%36, %in_scale23, %out_scale23, %clip_min23, %clip_max23, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %38 = nn.simulated_quantize(meta[relay.Constant][9] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale24, %out_scale24, %clip_min24, %clip_max24, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %39 = add(%37, %38) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %40 = nn.simulated_quantize(%39, %in_scale25, %out_scale25, %clip_min25, %clip_max25, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %42 = nn.simulated_quantize(%41, %in_scale26, %out_scale26, %clip_min26, %clip_max26, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %43 = nn.max_pool2d(%42, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %44 = nn.simulated_quantize(%43, %in_scale27, %out_scale27, %clip_min27, %clip_max27, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %45 = nn.simulated_quantize(meta[relay.Constant][10] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, %in_scale28, %out_scale28, %clip_min28, %clip_max28, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 192, 1, 1), float32] */;
  %46 = nn.conv2d(%44, %45, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %47 = nn.simulated_quantize(%46, %in_scale29, %out_scale29, %clip_min29, %clip_max29, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %48 = nn.simulated_quantize(meta[relay.Constant][11] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale30, %out_scale30, %clip_min30, %clip_max30, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %49 = add(%47, %48) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %50 = nn.simulated_quantize(%49, %in_scale31, %out_scale31, %clip_min31, %clip_max31, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %51 = nn.relu(%50) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %52 = nn.simulated_quantize(%51, %in_scale63, %out_scale63, %clip_min63, %clip_max63, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %53 = nn.simulated_quantize(%43, %in_scale32, %out_scale32, %clip_min32, %clip_max32, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %54 = nn.simulated_quantize(meta[relay.Constant][12] /* ty=Tensor[(48, 192, 1, 1), float32] */ /* ty=Tensor[(48, 192, 1, 1), float32] */, %in_scale33, %out_scale33, %clip_min33, %clip_max33, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(48, 192, 1, 1), float32] */;
  %55 = nn.conv2d(%53, %54, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %56 = nn.simulated_quantize(%55, %in_scale34, %out_scale34, %clip_min34, %clip_max34, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %57 = nn.simulated_quantize(meta[relay.Constant][13] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */, %in_scale35, %out_scale35, %clip_min35, %clip_max35, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(48, 1, 1), float32] */;
  %58 = add(%56, %57) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %59 = nn.simulated_quantize(%58, %in_scale36, %out_scale36, %clip_min36, %clip_max36, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %60 = nn.relu(%59) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %61 = nn.simulated_quantize(%60, %in_scale37, %out_scale37, %clip_min37, %clip_max37, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %62 = nn.simulated_quantize(meta[relay.Constant][14] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, %in_scale38, %out_scale38, %clip_min38, %clip_max38, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 48, 5, 5), float32] */;
  %63 = nn.conv2d(%61, %62, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %64 = nn.simulated_quantize(%63, %in_scale39, %out_scale39, %clip_min39, %clip_max39, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %65 = nn.simulated_quantize(meta[relay.Constant][15] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale40, %out_scale40, %clip_min40, %clip_max40, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %66 = add(%64, %65) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %67 = nn.simulated_quantize(%66, %in_scale41, %out_scale41, %clip_min41, %clip_max41, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %68 = nn.relu(%67) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %69 = nn.simulated_quantize(%68, %in_scale64, %out_scale64, %clip_min64, %clip_max64, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %70 = nn.simulated_quantize(%43, %in_scale42, %out_scale42, %clip_min42, %clip_max42, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %71 = nn.simulated_quantize(meta[relay.Constant][16] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, %in_scale43, %out_scale43, %clip_min43, %clip_max43, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 192, 1, 1), float32] */;
  %72 = nn.conv2d(%70, %71, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %73 = nn.simulated_quantize(%72, %in_scale44, %out_scale44, %clip_min44, %clip_max44, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %74 = nn.simulated_quantize(meta[relay.Constant][17] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale45, %out_scale45, %clip_min45, %clip_max45, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %75 = add(%73, %74) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %76 = nn.simulated_quantize(%75, %in_scale46, %out_scale46, %clip_min46, %clip_max46, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %77 = nn.relu(%76) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %78 = nn.simulated_quantize(%77, %in_scale47, %out_scale47, %clip_min47, %clip_max47, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %79 = nn.simulated_quantize(meta[relay.Constant][18] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, %in_scale48, %out_scale48, %clip_min48, %clip_max48, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 64, 3, 3), float32] */;
  %80 = nn.conv2d(%78, %79, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %81 = nn.simulated_quantize(%80, %in_scale49, %out_scale49, %clip_min49, %clip_max49, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %82 = nn.simulated_quantize(meta[relay.Constant][19] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, %in_scale50, %out_scale50, %clip_min50, %clip_max50, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %83 = add(%81, %82) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %84 = nn.simulated_quantize(%83, %in_scale51, %out_scale51, %clip_min51, %clip_max51, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %85 = nn.relu(%84) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %86 = nn.simulated_quantize(%85, %in_scale52, %out_scale52, %clip_min52, %clip_max52, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %87 = nn.simulated_quantize(meta[relay.Constant][20] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, %in_scale53, %out_scale53, %clip_min53, %clip_max53, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 96, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %89 = nn.simulated_quantize(%88, %in_scale54, %out_scale54, %clip_min54, %clip_max54, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %90 = nn.simulated_quantize(meta[relay.Constant][21] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, %in_scale55, %out_scale55, %clip_min55, %clip_max55, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %91 = add(%89, %90) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %92 = nn.simulated_quantize(%91, %in_scale56, %out_scale56, %clip_min56, %clip_max56, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %93 = nn.relu(%92) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %94 = nn.simulated_quantize(%93, %in_scale65, %out_scale65, %clip_min65, %clip_max65, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %95 = nn.simulated_quantize(%43, %in_scale57, %out_scale57, %clip_min57, %clip_max57, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %96 = nn.avg_pool2d(%95, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %97 = nn.simulated_quantize(%96, %in_scale58, %out_scale58, %clip_min58, %clip_max58, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %98 = nn.simulated_quantize(meta[relay.Constant][22] /* ty=Tensor[(32, 192, 1, 1), float32] */ /* ty=Tensor[(32, 192, 1, 1), float32] */, %in_scale59, %out_scale59, %clip_min59, %clip_max59, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 1, 1), float32] */;
  %99 = nn.conv2d(%97, %98, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %100 = nn.simulated_quantize(%99, %in_scale60, %out_scale60, %clip_min60, %clip_max60, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %101 = nn.simulated_quantize(meta[relay.Constant][23] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */, %in_scale61, %out_scale61, %clip_min61, %clip_max61, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1, 1), float32] */;
  %102 = add(%100, %101) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %103 = nn.simulated_quantize(%102, %in_scale62, %out_scale62, %clip_min62, %clip_max62, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %104 = nn.relu(%103) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %105 = nn.simulated_quantize(%104, %in_scale66, %out_scale66, %clip_min66, %clip_max66, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %106 = (%52, %69, %94, %105);
  %107 = concatenate(%106, axis=1) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %108 = nn.simulated_quantize(%107, %in_scale67, %out_scale67, %clip_min67, %clip_max67, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %109 = nn.simulated_quantize(meta[relay.Constant][24] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, %in_scale68, %out_scale68, %clip_min68, %clip_max68, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %110 = nn.conv2d(%108, %109, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %111 = nn.simulated_quantize(%110, %in_scale69, %out_scale69, %clip_min69, %clip_max69, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %112 = nn.simulated_quantize(meta[relay.Constant][25] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale70, %out_scale70, %clip_min70, %clip_max70, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %113 = add(%111, %112) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %114 = nn.simulated_quantize(%113, %in_scale71, %out_scale71, %clip_min71, %clip_max71, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %115 = nn.relu(%114) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %116 = nn.simulated_quantize(%115, %in_scale103, %out_scale103, %clip_min103, %clip_max103, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %117 = nn.simulated_quantize(%107, %in_scale72, %out_scale72, %clip_min72, %clip_max72, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %118 = nn.simulated_quantize(meta[relay.Constant][26] /* ty=Tensor[(48, 256, 1, 1), float32] */ /* ty=Tensor[(48, 256, 1, 1), float32] */, %in_scale73, %out_scale73, %clip_min73, %clip_max73, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(48, 256, 1, 1), float32] */;
  %119 = nn.conv2d(%117, %118, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %120 = nn.simulated_quantize(%119, %in_scale74, %out_scale74, %clip_min74, %clip_max74, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %121 = nn.simulated_quantize(meta[relay.Constant][27] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */, %in_scale75, %out_scale75, %clip_min75, %clip_max75, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(48, 1, 1), float32] */;
  %122 = add(%120, %121) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %123 = nn.simulated_quantize(%122, %in_scale76, %out_scale76, %clip_min76, %clip_max76, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %125 = nn.simulated_quantize(%124, %in_scale77, %out_scale77, %clip_min77, %clip_max77, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %126 = nn.simulated_quantize(meta[relay.Constant][28] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, %in_scale78, %out_scale78, %clip_min78, %clip_max78, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 48, 5, 5), float32] */;
  %127 = nn.conv2d(%125, %126, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %128 = nn.simulated_quantize(%127, %in_scale79, %out_scale79, %clip_min79, %clip_max79, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %129 = nn.simulated_quantize(meta[relay.Constant][29] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale80, %out_scale80, %clip_min80, %clip_max80, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %130 = add(%128, %129) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %131 = nn.simulated_quantize(%130, %in_scale81, %out_scale81, %clip_min81, %clip_max81, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %132 = nn.relu(%131) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %133 = nn.simulated_quantize(%132, %in_scale104, %out_scale104, %clip_min104, %clip_max104, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %134 = nn.simulated_quantize(%107, %in_scale82, %out_scale82, %clip_min82, %clip_max82, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %135 = nn.simulated_quantize(meta[relay.Constant][30] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, %in_scale83, %out_scale83, %clip_min83, %clip_max83, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %136 = nn.conv2d(%134, %135, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %137 = nn.simulated_quantize(%136, %in_scale84, %out_scale84, %clip_min84, %clip_max84, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %138 = nn.simulated_quantize(meta[relay.Constant][31] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale85, %out_scale85, %clip_min85, %clip_max85, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %139 = add(%137, %138) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %140 = nn.simulated_quantize(%139, %in_scale86, %out_scale86, %clip_min86, %clip_max86, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %141 = nn.relu(%140) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %142 = nn.simulated_quantize(%141, %in_scale87, %out_scale87, %clip_min87, %clip_max87, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %143 = nn.simulated_quantize(meta[relay.Constant][32] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, %in_scale88, %out_scale88, %clip_min88, %clip_max88, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 64, 3, 3), float32] */;
  %144 = nn.conv2d(%142, %143, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %145 = nn.simulated_quantize(%144, %in_scale89, %out_scale89, %clip_min89, %clip_max89, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %146 = nn.simulated_quantize(meta[relay.Constant][33] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, %in_scale90, %out_scale90, %clip_min90, %clip_max90, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %147 = add(%145, %146) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %148 = nn.simulated_quantize(%147, %in_scale91, %out_scale91, %clip_min91, %clip_max91, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %149 = nn.relu(%148) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %150 = nn.simulated_quantize(%149, %in_scale92, %out_scale92, %clip_min92, %clip_max92, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %151 = nn.simulated_quantize(meta[relay.Constant][34] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, %in_scale93, %out_scale93, %clip_min93, %clip_max93, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 96, 3, 3), float32] */;
  %152 = nn.conv2d(%150, %151, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %153 = nn.simulated_quantize(%152, %in_scale94, %out_scale94, %clip_min94, %clip_max94, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %154 = nn.simulated_quantize(meta[relay.Constant][35] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, %in_scale95, %out_scale95, %clip_min95, %clip_max95, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %155 = add(%153, %154) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %156 = nn.simulated_quantize(%155, %in_scale96, %out_scale96, %clip_min96, %clip_max96, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %158 = nn.simulated_quantize(%157, %in_scale105, %out_scale105, %clip_min105, %clip_max105, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %159 = nn.simulated_quantize(%107, %in_scale97, %out_scale97, %clip_min97, %clip_max97, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %160 = nn.avg_pool2d(%159, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %161 = nn.simulated_quantize(%160, %in_scale98, %out_scale98, %clip_min98, %clip_max98, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %162 = nn.simulated_quantize(meta[relay.Constant][36] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, %in_scale99, %out_scale99, %clip_min99, %clip_max99, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %163 = nn.conv2d(%161, %162, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %164 = nn.simulated_quantize(%163, %in_scale100, %out_scale100, %clip_min100, %clip_max100, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %165 = nn.simulated_quantize(meta[relay.Constant][37] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale101, %out_scale101, %clip_min101, %clip_max101, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %166 = add(%164, %165) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %167 = nn.simulated_quantize(%166, %in_scale102, %out_scale102, %clip_min102, %clip_max102, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %168 = nn.relu(%167) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %169 = nn.simulated_quantize(%168, %in_scale106, %out_scale106, %clip_min106, %clip_max106, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %170 = (%116, %133, %158, %169);
  %171 = concatenate(%170, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %172 = nn.simulated_quantize(%171, %in_scale107, %out_scale107, %clip_min107, %clip_max107, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %173 = nn.simulated_quantize(meta[relay.Constant][38] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, %in_scale108, %out_scale108, %clip_min108, %clip_max108, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 288, 1, 1), float32] */;
  %174 = nn.conv2d(%172, %173, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %175 = nn.simulated_quantize(%174, %in_scale109, %out_scale109, %clip_min109, %clip_max109, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %176 = nn.simulated_quantize(meta[relay.Constant][39] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale110, %out_scale110, %clip_min110, %clip_max110, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %177 = add(%175, %176) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %178 = nn.simulated_quantize(%177, %in_scale111, %out_scale111, %clip_min111, %clip_max111, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %180 = nn.simulated_quantize(%179, %in_scale143, %out_scale143, %clip_min143, %clip_max143, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %181 = nn.simulated_quantize(%171, %in_scale112, %out_scale112, %clip_min112, %clip_max112, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %182 = nn.simulated_quantize(meta[relay.Constant][40] /* ty=Tensor[(48, 288, 1, 1), float32] */ /* ty=Tensor[(48, 288, 1, 1), float32] */, %in_scale113, %out_scale113, %clip_min113, %clip_max113, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(48, 288, 1, 1), float32] */;
  %183 = nn.conv2d(%181, %182, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %184 = nn.simulated_quantize(%183, %in_scale114, %out_scale114, %clip_min114, %clip_max114, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %185 = nn.simulated_quantize(meta[relay.Constant][41] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */, %in_scale115, %out_scale115, %clip_min115, %clip_max115, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(48, 1, 1), float32] */;
  %186 = add(%184, %185) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %187 = nn.simulated_quantize(%186, %in_scale116, %out_scale116, %clip_min116, %clip_max116, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %188 = nn.relu(%187) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %189 = nn.simulated_quantize(%188, %in_scale117, %out_scale117, %clip_min117, %clip_max117, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %190 = nn.simulated_quantize(meta[relay.Constant][42] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, %in_scale118, %out_scale118, %clip_min118, %clip_max118, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 48, 5, 5), float32] */;
  %191 = nn.conv2d(%189, %190, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %192 = nn.simulated_quantize(%191, %in_scale119, %out_scale119, %clip_min119, %clip_max119, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %193 = nn.simulated_quantize(meta[relay.Constant][43] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale120, %out_scale120, %clip_min120, %clip_max120, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %194 = add(%192, %193) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %195 = nn.simulated_quantize(%194, %in_scale121, %out_scale121, %clip_min121, %clip_max121, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %196 = nn.relu(%195) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %197 = nn.simulated_quantize(%196, %in_scale144, %out_scale144, %clip_min144, %clip_max144, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %198 = nn.simulated_quantize(%171, %in_scale122, %out_scale122, %clip_min122, %clip_max122, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %199 = nn.simulated_quantize(meta[relay.Constant][44] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, %in_scale123, %out_scale123, %clip_min123, %clip_max123, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 288, 1, 1), float32] */;
  %200 = nn.conv2d(%198, %199, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %201 = nn.simulated_quantize(%200, %in_scale124, %out_scale124, %clip_min124, %clip_max124, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %202 = nn.simulated_quantize(meta[relay.Constant][45] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale125, %out_scale125, %clip_min125, %clip_max125, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %203 = add(%201, %202) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %204 = nn.simulated_quantize(%203, %in_scale126, %out_scale126, %clip_min126, %clip_max126, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %206 = nn.simulated_quantize(%205, %in_scale127, %out_scale127, %clip_min127, %clip_max127, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %207 = nn.simulated_quantize(meta[relay.Constant][46] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, %in_scale128, %out_scale128, %clip_min128, %clip_max128, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 64, 3, 3), float32] */;
  %208 = nn.conv2d(%206, %207, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %209 = nn.simulated_quantize(%208, %in_scale129, %out_scale129, %clip_min129, %clip_max129, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %210 = nn.simulated_quantize(meta[relay.Constant][47] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, %in_scale130, %out_scale130, %clip_min130, %clip_max130, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %211 = add(%209, %210) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %212 = nn.simulated_quantize(%211, %in_scale131, %out_scale131, %clip_min131, %clip_max131, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %213 = nn.relu(%212) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %214 = nn.simulated_quantize(%213, %in_scale132, %out_scale132, %clip_min132, %clip_max132, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %215 = nn.simulated_quantize(meta[relay.Constant][48] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, %in_scale133, %out_scale133, %clip_min133, %clip_max133, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 96, 3, 3), float32] */;
  %216 = nn.conv2d(%214, %215, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %217 = nn.simulated_quantize(%216, %in_scale134, %out_scale134, %clip_min134, %clip_max134, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %218 = nn.simulated_quantize(meta[relay.Constant][49] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, %in_scale135, %out_scale135, %clip_min135, %clip_max135, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %219 = add(%217, %218) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %220 = nn.simulated_quantize(%219, %in_scale136, %out_scale136, %clip_min136, %clip_max136, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %221 = nn.relu(%220) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %222 = nn.simulated_quantize(%221, %in_scale145, %out_scale145, %clip_min145, %clip_max145, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %223 = nn.simulated_quantize(%171, %in_scale137, %out_scale137, %clip_min137, %clip_max137, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %224 = nn.avg_pool2d(%223, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %225 = nn.simulated_quantize(%224, %in_scale138, %out_scale138, %clip_min138, %clip_max138, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %226 = nn.simulated_quantize(meta[relay.Constant][50] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, %in_scale139, %out_scale139, %clip_min139, %clip_max139, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 288, 1, 1), float32] */;
  %227 = nn.conv2d(%225, %226, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %228 = nn.simulated_quantize(%227, %in_scale140, %out_scale140, %clip_min140, %clip_max140, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %229 = nn.simulated_quantize(meta[relay.Constant][51] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale141, %out_scale141, %clip_min141, %clip_max141, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %230 = add(%228, %229) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %231 = nn.simulated_quantize(%230, %in_scale142, %out_scale142, %clip_min142, %clip_max142, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %232 = nn.relu(%231) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %233 = nn.simulated_quantize(%232, %in_scale146, %out_scale146, %clip_min146, %clip_max146, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %234 = (%180, %197, %222, %233);
  %235 = concatenate(%234, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %236 = nn.simulated_quantize(%235, %in_scale147, %out_scale147, %clip_min147, %clip_max147, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %237 = nn.simulated_quantize(meta[relay.Constant][52] /* ty=Tensor[(384, 288, 3, 3), float32] */ /* ty=Tensor[(384, 288, 3, 3), float32] */, %in_scale148, %out_scale148, %clip_min148, %clip_max148, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 288, 3, 3), float32] */;
  %238 = nn.conv2d(%236, %237, strides=[2, 2], padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %239 = nn.simulated_quantize(%238, %in_scale149, %out_scale149, %clip_min149, %clip_max149, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %240 = nn.simulated_quantize(meta[relay.Constant][53] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale150, %out_scale150, %clip_min150, %clip_max150, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %241 = add(%239, %240) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %242 = nn.simulated_quantize(%241, %in_scale151, %out_scale151, %clip_min151, %clip_max151, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %243 = nn.relu(%242) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %244 = nn.simulated_quantize(%243, %in_scale168, %out_scale168, %clip_min168, %clip_max168, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %245 = nn.simulated_quantize(%235, %in_scale152, %out_scale152, %clip_min152, %clip_max152, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %246 = nn.simulated_quantize(meta[relay.Constant][54] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, %in_scale153, %out_scale153, %clip_min153, %clip_max153, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 288, 1, 1), float32] */;
  %247 = nn.conv2d(%245, %246, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %248 = nn.simulated_quantize(%247, %in_scale154, %out_scale154, %clip_min154, %clip_max154, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %249 = nn.simulated_quantize(meta[relay.Constant][55] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, %in_scale155, %out_scale155, %clip_min155, %clip_max155, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %250 = add(%248, %249) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %251 = nn.simulated_quantize(%250, %in_scale156, %out_scale156, %clip_min156, %clip_max156, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %252 = nn.relu(%251) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %253 = nn.simulated_quantize(%252, %in_scale157, %out_scale157, %clip_min157, %clip_max157, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %254 = nn.simulated_quantize(meta[relay.Constant][56] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, %in_scale158, %out_scale158, %clip_min158, %clip_max158, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 64, 3, 3), float32] */;
  %255 = nn.conv2d(%253, %254, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %256 = nn.simulated_quantize(%255, %in_scale159, %out_scale159, %clip_min159, %clip_max159, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %257 = nn.simulated_quantize(meta[relay.Constant][57] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, %in_scale160, %out_scale160, %clip_min160, %clip_max160, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %258 = add(%256, %257) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %259 = nn.simulated_quantize(%258, %in_scale161, %out_scale161, %clip_min161, %clip_max161, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %260 = nn.relu(%259) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %261 = nn.simulated_quantize(%260, %in_scale162, %out_scale162, %clip_min162, %clip_max162, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %262 = nn.simulated_quantize(meta[relay.Constant][58] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, %in_scale163, %out_scale163, %clip_min163, %clip_max163, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 96, 3, 3), float32] */;
  %263 = nn.conv2d(%261, %262, strides=[2, 2], padding=[0, 0, 0, 0], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %264 = nn.simulated_quantize(%263, %in_scale164, %out_scale164, %clip_min164, %clip_max164, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %265 = nn.simulated_quantize(meta[relay.Constant][59] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, %in_scale165, %out_scale165, %clip_min165, %clip_max165, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %266 = add(%264, %265) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %267 = nn.simulated_quantize(%266, %in_scale166, %out_scale166, %clip_min166, %clip_max166, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %268 = nn.relu(%267) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %269 = nn.simulated_quantize(%268, %in_scale169, %out_scale169, %clip_min169, %clip_max169, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %270 = nn.simulated_quantize(%235, %in_scale167, %out_scale167, %clip_min167, %clip_max167, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %271 = nn.max_pool2d(%270, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 288, 17, 17), float32] */;
  %272 = nn.simulated_quantize(%271, %in_scale170, %out_scale170, %clip_min170, %clip_max170, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 288, 17, 17), float32] */;
  %273 = (%244, %269, %272);
  %274 = concatenate(%273, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %275 = nn.simulated_quantize(%274, %in_scale171, %out_scale171, %clip_min171, %clip_max171, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %276 = nn.simulated_quantize(meta[relay.Constant][60] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale172, %out_scale172, %clip_min172, %clip_max172, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %277 = nn.conv2d(%275, %276, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %278 = nn.simulated_quantize(%277, %in_scale173, %out_scale173, %clip_min173, %clip_max173, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %279 = nn.simulated_quantize(meta[relay.Constant][61] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale174, %out_scale174, %clip_min174, %clip_max174, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %280 = add(%278, %279) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %281 = nn.simulated_quantize(%280, %in_scale175, %out_scale175, %clip_min175, %clip_max175, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %282 = nn.relu(%281) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %283 = nn.simulated_quantize(%282, %in_scale222, %out_scale222, %clip_min222, %clip_max222, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %284 = nn.simulated_quantize(%274, %in_scale176, %out_scale176, %clip_min176, %clip_max176, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %285 = nn.simulated_quantize(meta[relay.Constant][62] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, %in_scale177, %out_scale177, %clip_min177, %clip_max177, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 768, 1, 1), float32] */;
  %286 = nn.conv2d(%284, %285, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %287 = nn.simulated_quantize(%286, %in_scale178, %out_scale178, %clip_min178, %clip_max178, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %288 = nn.simulated_quantize(meta[relay.Constant][63] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale179, %out_scale179, %clip_min179, %clip_max179, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %289 = add(%287, %288) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %290 = nn.simulated_quantize(%289, %in_scale180, %out_scale180, %clip_min180, %clip_max180, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %291 = nn.relu(%290) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %292 = nn.simulated_quantize(%291, %in_scale181, %out_scale181, %clip_min181, %clip_max181, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %293 = nn.simulated_quantize(meta[relay.Constant][64] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, %in_scale182, %out_scale182, %clip_min182, %clip_max182, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 1, 7), float32] */;
  %294 = nn.conv2d(%292, %293, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %295 = nn.simulated_quantize(%294, %in_scale183, %out_scale183, %clip_min183, %clip_max183, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %296 = nn.simulated_quantize(meta[relay.Constant][65] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale184, %out_scale184, %clip_min184, %clip_max184, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %297 = add(%295, %296) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %298 = nn.simulated_quantize(%297, %in_scale185, %out_scale185, %clip_min185, %clip_max185, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %299 = nn.relu(%298) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %300 = nn.simulated_quantize(%299, %in_scale186, %out_scale186, %clip_min186, %clip_max186, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %301 = nn.simulated_quantize(meta[relay.Constant][66] /* ty=Tensor[(192, 128, 7, 1), float32] */ /* ty=Tensor[(192, 128, 7, 1), float32] */, %in_scale187, %out_scale187, %clip_min187, %clip_max187, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 128, 7, 1), float32] */;
  %302 = nn.conv2d(%300, %301, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %303 = nn.simulated_quantize(%302, %in_scale188, %out_scale188, %clip_min188, %clip_max188, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %304 = nn.simulated_quantize(meta[relay.Constant][67] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale189, %out_scale189, %clip_min189, %clip_max189, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %305 = add(%303, %304) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %306 = nn.simulated_quantize(%305, %in_scale190, %out_scale190, %clip_min190, %clip_max190, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %307 = nn.relu(%306) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %308 = nn.simulated_quantize(%307, %in_scale223, %out_scale223, %clip_min223, %clip_max223, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %309 = nn.simulated_quantize(%274, %in_scale191, %out_scale191, %clip_min191, %clip_max191, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %310 = nn.simulated_quantize(meta[relay.Constant][68] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, %in_scale192, %out_scale192, %clip_min192, %clip_max192, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 768, 1, 1), float32] */;
  %311 = nn.conv2d(%309, %310, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %312 = nn.simulated_quantize(%311, %in_scale193, %out_scale193, %clip_min193, %clip_max193, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %313 = nn.simulated_quantize(meta[relay.Constant][69] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale194, %out_scale194, %clip_min194, %clip_max194, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %314 = add(%312, %313) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %315 = nn.simulated_quantize(%314, %in_scale195, %out_scale195, %clip_min195, %clip_max195, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %316 = nn.relu(%315) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %317 = nn.simulated_quantize(%316, %in_scale196, %out_scale196, %clip_min196, %clip_max196, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %318 = nn.simulated_quantize(meta[relay.Constant][70] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, %in_scale197, %out_scale197, %clip_min197, %clip_max197, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 7, 1), float32] */;
  %319 = nn.conv2d(%317, %318, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %320 = nn.simulated_quantize(%319, %in_scale198, %out_scale198, %clip_min198, %clip_max198, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %321 = nn.simulated_quantize(meta[relay.Constant][71] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale199, %out_scale199, %clip_min199, %clip_max199, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %322 = add(%320, %321) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %323 = nn.simulated_quantize(%322, %in_scale200, %out_scale200, %clip_min200, %clip_max200, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %324 = nn.relu(%323) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %325 = nn.simulated_quantize(%324, %in_scale201, %out_scale201, %clip_min201, %clip_max201, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %326 = nn.simulated_quantize(meta[relay.Constant][72] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, %in_scale202, %out_scale202, %clip_min202, %clip_max202, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 1, 7), float32] */;
  %327 = nn.conv2d(%325, %326, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %328 = nn.simulated_quantize(%327, %in_scale203, %out_scale203, %clip_min203, %clip_max203, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %329 = nn.simulated_quantize(meta[relay.Constant][73] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale204, %out_scale204, %clip_min204, %clip_max204, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %330 = add(%328, %329) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %331 = nn.simulated_quantize(%330, %in_scale205, %out_scale205, %clip_min205, %clip_max205, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %332 = nn.relu(%331) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %333 = nn.simulated_quantize(%332, %in_scale206, %out_scale206, %clip_min206, %clip_max206, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %334 = nn.simulated_quantize(meta[relay.Constant][74] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, %in_scale207, %out_scale207, %clip_min207, %clip_max207, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 7, 1), float32] */;
  %335 = nn.conv2d(%333, %334, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %336 = nn.simulated_quantize(%335, %in_scale208, %out_scale208, %clip_min208, %clip_max208, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %337 = nn.simulated_quantize(meta[relay.Constant][75] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, %in_scale209, %out_scale209, %clip_min209, %clip_max209, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %338 = add(%336, %337) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %339 = nn.simulated_quantize(%338, %in_scale210, %out_scale210, %clip_min210, %clip_max210, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %340 = nn.relu(%339) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %341 = nn.simulated_quantize(%340, %in_scale211, %out_scale211, %clip_min211, %clip_max211, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %342 = nn.simulated_quantize(meta[relay.Constant][76] /* ty=Tensor[(192, 128, 1, 7), float32] */ /* ty=Tensor[(192, 128, 1, 7), float32] */, %in_scale212, %out_scale212, %clip_min212, %clip_max212, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 128, 1, 7), float32] */;
  %343 = nn.conv2d(%341, %342, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %344 = nn.simulated_quantize(%343, %in_scale213, %out_scale213, %clip_min213, %clip_max213, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %345 = nn.simulated_quantize(meta[relay.Constant][77] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale214, %out_scale214, %clip_min214, %clip_max214, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %346 = add(%344, %345) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %347 = nn.simulated_quantize(%346, %in_scale215, %out_scale215, %clip_min215, %clip_max215, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %348 = nn.relu(%347) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %349 = nn.simulated_quantize(%348, %in_scale224, %out_scale224, %clip_min224, %clip_max224, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %350 = nn.simulated_quantize(%274, %in_scale216, %out_scale216, %clip_min216, %clip_max216, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %351 = nn.avg_pool2d(%350, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %352 = nn.simulated_quantize(%351, %in_scale217, %out_scale217, %clip_min217, %clip_max217, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %353 = nn.simulated_quantize(meta[relay.Constant][78] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale218, %out_scale218, %clip_min218, %clip_max218, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %354 = nn.conv2d(%352, %353, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %355 = nn.simulated_quantize(%354, %in_scale219, %out_scale219, %clip_min219, %clip_max219, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %356 = nn.simulated_quantize(meta[relay.Constant][79] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale220, %out_scale220, %clip_min220, %clip_max220, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %357 = add(%355, %356) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %358 = nn.simulated_quantize(%357, %in_scale221, %out_scale221, %clip_min221, %clip_max221, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %359 = nn.relu(%358) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %360 = nn.simulated_quantize(%359, %in_scale225, %out_scale225, %clip_min225, %clip_max225, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %361 = (%283, %308, %349, %360);
  %362 = concatenate(%361, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %363 = nn.simulated_quantize(%362, %in_scale226, %out_scale226, %clip_min226, %clip_max226, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %364 = nn.simulated_quantize(meta[relay.Constant][80] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale227, %out_scale227, %clip_min227, %clip_max227, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %365 = nn.conv2d(%363, %364, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %366 = nn.simulated_quantize(%365, %in_scale228, %out_scale228, %clip_min228, %clip_max228, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %367 = nn.simulated_quantize(meta[relay.Constant][81] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale229, %out_scale229, %clip_min229, %clip_max229, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %368 = add(%366, %367) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %369 = nn.simulated_quantize(%368, %in_scale230, %out_scale230, %clip_min230, %clip_max230, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %370 = nn.relu(%369) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %371 = nn.simulated_quantize(%370, %in_scale277, %out_scale277, %clip_min277, %clip_max277, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %372 = nn.simulated_quantize(%362, %in_scale231, %out_scale231, %clip_min231, %clip_max231, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %373 = nn.simulated_quantize(meta[relay.Constant][82] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, %in_scale232, %out_scale232, %clip_min232, %clip_max232, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 768, 1, 1), float32] */;
  %374 = nn.conv2d(%372, %373, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %375 = nn.simulated_quantize(%374, %in_scale233, %out_scale233, %clip_min233, %clip_max233, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %376 = nn.simulated_quantize(meta[relay.Constant][83] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale234, %out_scale234, %clip_min234, %clip_max234, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %377 = add(%375, %376) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %378 = nn.simulated_quantize(%377, %in_scale235, %out_scale235, %clip_min235, %clip_max235, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %379 = nn.relu(%378) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %380 = nn.simulated_quantize(%379, %in_scale236, %out_scale236, %clip_min236, %clip_max236, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %381 = nn.simulated_quantize(meta[relay.Constant][84] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, %in_scale237, %out_scale237, %clip_min237, %clip_max237, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 1, 7), float32] */;
  %382 = nn.conv2d(%380, %381, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %383 = nn.simulated_quantize(%382, %in_scale238, %out_scale238, %clip_min238, %clip_max238, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %384 = nn.simulated_quantize(meta[relay.Constant][85] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale239, %out_scale239, %clip_min239, %clip_max239, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %385 = add(%383, %384) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %386 = nn.simulated_quantize(%385, %in_scale240, %out_scale240, %clip_min240, %clip_max240, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %387 = nn.relu(%386) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %388 = nn.simulated_quantize(%387, %in_scale241, %out_scale241, %clip_min241, %clip_max241, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %389 = nn.simulated_quantize(meta[relay.Constant][86] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, %in_scale242, %out_scale242, %clip_min242, %clip_max242, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 160, 7, 1), float32] */;
  %390 = nn.conv2d(%388, %389, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %391 = nn.simulated_quantize(%390, %in_scale243, %out_scale243, %clip_min243, %clip_max243, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %392 = nn.simulated_quantize(meta[relay.Constant][87] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale244, %out_scale244, %clip_min244, %clip_max244, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %393 = add(%391, %392) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %394 = nn.simulated_quantize(%393, %in_scale245, %out_scale245, %clip_min245, %clip_max245, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %395 = nn.relu(%394) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %396 = nn.simulated_quantize(%395, %in_scale278, %out_scale278, %clip_min278, %clip_max278, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %397 = nn.simulated_quantize(%362, %in_scale246, %out_scale246, %clip_min246, %clip_max246, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %398 = nn.simulated_quantize(meta[relay.Constant][88] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, %in_scale247, %out_scale247, %clip_min247, %clip_max247, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 768, 1, 1), float32] */;
  %399 = nn.conv2d(%397, %398, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %400 = nn.simulated_quantize(%399, %in_scale248, %out_scale248, %clip_min248, %clip_max248, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %401 = nn.simulated_quantize(meta[relay.Constant][89] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale249, %out_scale249, %clip_min249, %clip_max249, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %402 = add(%400, %401) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %403 = nn.simulated_quantize(%402, %in_scale250, %out_scale250, %clip_min250, %clip_max250, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %404 = nn.relu(%403) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %405 = nn.simulated_quantize(%404, %in_scale251, %out_scale251, %clip_min251, %clip_max251, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %406 = nn.simulated_quantize(meta[relay.Constant][90] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, %in_scale252, %out_scale252, %clip_min252, %clip_max252, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 7, 1), float32] */;
  %407 = nn.conv2d(%405, %406, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %408 = nn.simulated_quantize(%407, %in_scale253, %out_scale253, %clip_min253, %clip_max253, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %409 = nn.simulated_quantize(meta[relay.Constant][91] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale254, %out_scale254, %clip_min254, %clip_max254, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %410 = add(%408, %409) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %411 = nn.simulated_quantize(%410, %in_scale255, %out_scale255, %clip_min255, %clip_max255, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %412 = nn.relu(%411) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %413 = nn.simulated_quantize(%412, %in_scale256, %out_scale256, %clip_min256, %clip_max256, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %414 = nn.simulated_quantize(meta[relay.Constant][92] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, %in_scale257, %out_scale257, %clip_min257, %clip_max257, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 1, 7), float32] */;
  %415 = nn.conv2d(%413, %414, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %416 = nn.simulated_quantize(%415, %in_scale258, %out_scale258, %clip_min258, %clip_max258, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %417 = nn.simulated_quantize(meta[relay.Constant][93] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale259, %out_scale259, %clip_min259, %clip_max259, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %418 = add(%416, %417) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %419 = nn.simulated_quantize(%418, %in_scale260, %out_scale260, %clip_min260, %clip_max260, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %420 = nn.relu(%419) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %421 = nn.simulated_quantize(%420, %in_scale261, %out_scale261, %clip_min261, %clip_max261, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %422 = nn.simulated_quantize(meta[relay.Constant][94] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, %in_scale262, %out_scale262, %clip_min262, %clip_max262, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 7, 1), float32] */;
  %423 = nn.conv2d(%421, %422, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %424 = nn.simulated_quantize(%423, %in_scale263, %out_scale263, %clip_min263, %clip_max263, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %425 = nn.simulated_quantize(meta[relay.Constant][95] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale264, %out_scale264, %clip_min264, %clip_max264, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %426 = add(%424, %425) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %427 = nn.simulated_quantize(%426, %in_scale265, %out_scale265, %clip_min265, %clip_max265, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %428 = nn.relu(%427) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %429 = nn.simulated_quantize(%428, %in_scale266, %out_scale266, %clip_min266, %clip_max266, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %430 = nn.simulated_quantize(meta[relay.Constant][96] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, %in_scale267, %out_scale267, %clip_min267, %clip_max267, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 160, 1, 7), float32] */;
  %431 = nn.conv2d(%429, %430, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %432 = nn.simulated_quantize(%431, %in_scale268, %out_scale268, %clip_min268, %clip_max268, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %433 = nn.simulated_quantize(meta[relay.Constant][97] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale269, %out_scale269, %clip_min269, %clip_max269, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %434 = add(%432, %433) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %435 = nn.simulated_quantize(%434, %in_scale270, %out_scale270, %clip_min270, %clip_max270, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %436 = nn.relu(%435) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %437 = nn.simulated_quantize(%436, %in_scale279, %out_scale279, %clip_min279, %clip_max279, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %438 = nn.simulated_quantize(%362, %in_scale271, %out_scale271, %clip_min271, %clip_max271, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %439 = nn.avg_pool2d(%438, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %440 = nn.simulated_quantize(%439, %in_scale272, %out_scale272, %clip_min272, %clip_max272, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %441 = nn.simulated_quantize(meta[relay.Constant][98] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale273, %out_scale273, %clip_min273, %clip_max273, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %442 = nn.conv2d(%440, %441, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %443 = nn.simulated_quantize(%442, %in_scale274, %out_scale274, %clip_min274, %clip_max274, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %444 = nn.simulated_quantize(meta[relay.Constant][99] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale275, %out_scale275, %clip_min275, %clip_max275, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %445 = add(%443, %444) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %446 = nn.simulated_quantize(%445, %in_scale276, %out_scale276, %clip_min276, %clip_max276, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %447 = nn.relu(%446) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %448 = nn.simulated_quantize(%447, %in_scale280, %out_scale280, %clip_min280, %clip_max280, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %449 = (%371, %396, %437, %448);
  %450 = concatenate(%449, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %451 = nn.simulated_quantize(%450, %in_scale281, %out_scale281, %clip_min281, %clip_max281, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %452 = nn.simulated_quantize(meta[relay.Constant][100] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale282, %out_scale282, %clip_min282, %clip_max282, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %453 = nn.conv2d(%451, %452, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %454 = nn.simulated_quantize(%453, %in_scale283, %out_scale283, %clip_min283, %clip_max283, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %455 = nn.simulated_quantize(meta[relay.Constant][101] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale284, %out_scale284, %clip_min284, %clip_max284, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %456 = add(%454, %455) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %457 = nn.simulated_quantize(%456, %in_scale285, %out_scale285, %clip_min285, %clip_max285, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %458 = nn.relu(%457) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %459 = nn.simulated_quantize(%458, %in_scale332, %out_scale332, %clip_min332, %clip_max332, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %460 = nn.simulated_quantize(%450, %in_scale286, %out_scale286, %clip_min286, %clip_max286, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %461 = nn.simulated_quantize(meta[relay.Constant][102] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, %in_scale287, %out_scale287, %clip_min287, %clip_max287, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 768, 1, 1), float32] */;
  %462 = nn.conv2d(%460, %461, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %463 = nn.simulated_quantize(%462, %in_scale288, %out_scale288, %clip_min288, %clip_max288, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %464 = nn.simulated_quantize(meta[relay.Constant][103] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale289, %out_scale289, %clip_min289, %clip_max289, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %465 = add(%463, %464) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %466 = nn.simulated_quantize(%465, %in_scale290, %out_scale290, %clip_min290, %clip_max290, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %467 = nn.relu(%466) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %468 = nn.simulated_quantize(%467, %in_scale291, %out_scale291, %clip_min291, %clip_max291, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %469 = nn.simulated_quantize(meta[relay.Constant][104] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, %in_scale292, %out_scale292, %clip_min292, %clip_max292, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 1, 7), float32] */;
  %470 = nn.conv2d(%468, %469, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %471 = nn.simulated_quantize(%470, %in_scale293, %out_scale293, %clip_min293, %clip_max293, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %472 = nn.simulated_quantize(meta[relay.Constant][105] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale294, %out_scale294, %clip_min294, %clip_max294, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %473 = add(%471, %472) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %474 = nn.simulated_quantize(%473, %in_scale295, %out_scale295, %clip_min295, %clip_max295, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %475 = nn.relu(%474) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %476 = nn.simulated_quantize(%475, %in_scale296, %out_scale296, %clip_min296, %clip_max296, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %477 = nn.simulated_quantize(meta[relay.Constant][106] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, %in_scale297, %out_scale297, %clip_min297, %clip_max297, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 160, 7, 1), float32] */;
  %478 = nn.conv2d(%476, %477, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %479 = nn.simulated_quantize(%478, %in_scale298, %out_scale298, %clip_min298, %clip_max298, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %480 = nn.simulated_quantize(meta[relay.Constant][107] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale299, %out_scale299, %clip_min299, %clip_max299, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %481 = add(%479, %480) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %482 = nn.simulated_quantize(%481, %in_scale300, %out_scale300, %clip_min300, %clip_max300, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %483 = nn.relu(%482) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %484 = nn.simulated_quantize(%483, %in_scale333, %out_scale333, %clip_min333, %clip_max333, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %485 = nn.simulated_quantize(%450, %in_scale301, %out_scale301, %clip_min301, %clip_max301, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %486 = nn.simulated_quantize(meta[relay.Constant][108] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, %in_scale302, %out_scale302, %clip_min302, %clip_max302, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 768, 1, 1), float32] */;
  %487 = nn.conv2d(%485, %486, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %488 = nn.simulated_quantize(%487, %in_scale303, %out_scale303, %clip_min303, %clip_max303, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %489 = nn.simulated_quantize(meta[relay.Constant][109] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale304, %out_scale304, %clip_min304, %clip_max304, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %490 = add(%488, %489) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %491 = nn.simulated_quantize(%490, %in_scale305, %out_scale305, %clip_min305, %clip_max305, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %492 = nn.relu(%491) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %493 = nn.simulated_quantize(%492, %in_scale306, %out_scale306, %clip_min306, %clip_max306, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %494 = nn.simulated_quantize(meta[relay.Constant][110] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, %in_scale307, %out_scale307, %clip_min307, %clip_max307, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 7, 1), float32] */;
  %495 = nn.conv2d(%493, %494, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %496 = nn.simulated_quantize(%495, %in_scale308, %out_scale308, %clip_min308, %clip_max308, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %497 = nn.simulated_quantize(meta[relay.Constant][111] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale309, %out_scale309, %clip_min309, %clip_max309, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %498 = add(%496, %497) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %499 = nn.simulated_quantize(%498, %in_scale310, %out_scale310, %clip_min310, %clip_max310, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %500 = nn.relu(%499) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %501 = nn.simulated_quantize(%500, %in_scale311, %out_scale311, %clip_min311, %clip_max311, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %502 = nn.simulated_quantize(meta[relay.Constant][112] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, %in_scale312, %out_scale312, %clip_min312, %clip_max312, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 1, 7), float32] */;
  %503 = nn.conv2d(%501, %502, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %504 = nn.simulated_quantize(%503, %in_scale313, %out_scale313, %clip_min313, %clip_max313, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %505 = nn.simulated_quantize(meta[relay.Constant][113] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale314, %out_scale314, %clip_min314, %clip_max314, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %506 = add(%504, %505) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %507 = nn.simulated_quantize(%506, %in_scale315, %out_scale315, %clip_min315, %clip_max315, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %508 = nn.relu(%507) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %509 = nn.simulated_quantize(%508, %in_scale316, %out_scale316, %clip_min316, %clip_max316, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %510 = nn.simulated_quantize(meta[relay.Constant][114] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, %in_scale317, %out_scale317, %clip_min317, %clip_max317, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 7, 1), float32] */;
  %511 = nn.conv2d(%509, %510, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %512 = nn.simulated_quantize(%511, %in_scale318, %out_scale318, %clip_min318, %clip_max318, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %513 = nn.simulated_quantize(meta[relay.Constant][115] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, %in_scale319, %out_scale319, %clip_min319, %clip_max319, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %514 = add(%512, %513) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %515 = nn.simulated_quantize(%514, %in_scale320, %out_scale320, %clip_min320, %clip_max320, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %516 = nn.relu(%515) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %517 = nn.simulated_quantize(%516, %in_scale321, %out_scale321, %clip_min321, %clip_max321, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %518 = nn.simulated_quantize(meta[relay.Constant][116] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, %in_scale322, %out_scale322, %clip_min322, %clip_max322, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 160, 1, 7), float32] */;
  %519 = nn.conv2d(%517, %518, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %520 = nn.simulated_quantize(%519, %in_scale323, %out_scale323, %clip_min323, %clip_max323, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %521 = nn.simulated_quantize(meta[relay.Constant][117] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale324, %out_scale324, %clip_min324, %clip_max324, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %522 = add(%520, %521) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %523 = nn.simulated_quantize(%522, %in_scale325, %out_scale325, %clip_min325, %clip_max325, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %524 = nn.relu(%523) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %525 = nn.simulated_quantize(%524, %in_scale334, %out_scale334, %clip_min334, %clip_max334, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %526 = nn.simulated_quantize(%450, %in_scale326, %out_scale326, %clip_min326, %clip_max326, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %527 = nn.avg_pool2d(%526, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %528 = nn.simulated_quantize(%527, %in_scale327, %out_scale327, %clip_min327, %clip_max327, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %529 = nn.simulated_quantize(meta[relay.Constant][118] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale328, %out_scale328, %clip_min328, %clip_max328, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %530 = nn.conv2d(%528, %529, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %531 = nn.simulated_quantize(%530, %in_scale329, %out_scale329, %clip_min329, %clip_max329, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %532 = nn.simulated_quantize(meta[relay.Constant][119] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale330, %out_scale330, %clip_min330, %clip_max330, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %533 = add(%531, %532) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %534 = nn.simulated_quantize(%533, %in_scale331, %out_scale331, %clip_min331, %clip_max331, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %535 = nn.relu(%534) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %536 = nn.simulated_quantize(%535, %in_scale335, %out_scale335, %clip_min335, %clip_max335, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %537 = (%459, %484, %525, %536);
  %538 = concatenate(%537, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %539 = nn.simulated_quantize(%538, %in_scale336, %out_scale336, %clip_min336, %clip_max336, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %540 = nn.simulated_quantize(meta[relay.Constant][120] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale337, %out_scale337, %clip_min337, %clip_max337, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %541 = nn.conv2d(%539, %540, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %542 = nn.simulated_quantize(%541, %in_scale338, %out_scale338, %clip_min338, %clip_max338, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %543 = nn.simulated_quantize(meta[relay.Constant][121] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale339, %out_scale339, %clip_min339, %clip_max339, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %544 = add(%542, %543) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %545 = nn.simulated_quantize(%544, %in_scale340, %out_scale340, %clip_min340, %clip_max340, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %546 = nn.relu(%545) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %547 = nn.simulated_quantize(%546, %in_scale387, %out_scale387, %clip_min387, %clip_max387, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %548 = nn.simulated_quantize(%538, %in_scale341, %out_scale341, %clip_min341, %clip_max341, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %549 = nn.simulated_quantize(meta[relay.Constant][122] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale342, %out_scale342, %clip_min342, %clip_max342, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %550 = nn.conv2d(%548, %549, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %551 = nn.simulated_quantize(%550, %in_scale343, %out_scale343, %clip_min343, %clip_max343, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %552 = nn.simulated_quantize(meta[relay.Constant][123] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale344, %out_scale344, %clip_min344, %clip_max344, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %553 = add(%551, %552) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %554 = nn.simulated_quantize(%553, %in_scale345, %out_scale345, %clip_min345, %clip_max345, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %555 = nn.relu(%554) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %556 = nn.simulated_quantize(%555, %in_scale346, %out_scale346, %clip_min346, %clip_max346, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %557 = nn.simulated_quantize(meta[relay.Constant][124] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, %in_scale347, %out_scale347, %clip_min347, %clip_max347, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 1, 7), float32] */;
  %558 = nn.conv2d(%556, %557, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %559 = nn.simulated_quantize(%558, %in_scale348, %out_scale348, %clip_min348, %clip_max348, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %560 = nn.simulated_quantize(meta[relay.Constant][125] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale349, %out_scale349, %clip_min349, %clip_max349, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %561 = add(%559, %560) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %562 = nn.simulated_quantize(%561, %in_scale350, %out_scale350, %clip_min350, %clip_max350, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %563 = nn.relu(%562) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %564 = nn.simulated_quantize(%563, %in_scale351, %out_scale351, %clip_min351, %clip_max351, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %565 = nn.simulated_quantize(meta[relay.Constant][126] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, %in_scale352, %out_scale352, %clip_min352, %clip_max352, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 7, 1), float32] */;
  %566 = nn.conv2d(%564, %565, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %567 = nn.simulated_quantize(%566, %in_scale353, %out_scale353, %clip_min353, %clip_max353, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %568 = nn.simulated_quantize(meta[relay.Constant][127] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale354, %out_scale354, %clip_min354, %clip_max354, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %569 = add(%567, %568) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %570 = nn.simulated_quantize(%569, %in_scale355, %out_scale355, %clip_min355, %clip_max355, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %571 = nn.relu(%570) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %572 = nn.simulated_quantize(%571, %in_scale388, %out_scale388, %clip_min388, %clip_max388, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %573 = nn.simulated_quantize(%538, %in_scale356, %out_scale356, %clip_min356, %clip_max356, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %574 = nn.simulated_quantize(meta[relay.Constant][128] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale357, %out_scale357, %clip_min357, %clip_max357, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %575 = nn.conv2d(%573, %574, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %576 = nn.simulated_quantize(%575, %in_scale358, %out_scale358, %clip_min358, %clip_max358, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %577 = nn.simulated_quantize(meta[relay.Constant][129] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale359, %out_scale359, %clip_min359, %clip_max359, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %578 = add(%576, %577) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %579 = nn.simulated_quantize(%578, %in_scale360, %out_scale360, %clip_min360, %clip_max360, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %580 = nn.relu(%579) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %581 = nn.simulated_quantize(%580, %in_scale361, %out_scale361, %clip_min361, %clip_max361, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %582 = nn.simulated_quantize(meta[relay.Constant][130] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, %in_scale362, %out_scale362, %clip_min362, %clip_max362, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 7, 1), float32] */;
  %583 = nn.conv2d(%581, %582, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %584 = nn.simulated_quantize(%583, %in_scale363, %out_scale363, %clip_min363, %clip_max363, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %585 = nn.simulated_quantize(meta[relay.Constant][131] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale364, %out_scale364, %clip_min364, %clip_max364, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %586 = add(%584, %585) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %587 = nn.simulated_quantize(%586, %in_scale365, %out_scale365, %clip_min365, %clip_max365, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %588 = nn.relu(%587) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %589 = nn.simulated_quantize(%588, %in_scale366, %out_scale366, %clip_min366, %clip_max366, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %590 = nn.simulated_quantize(meta[relay.Constant][132] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, %in_scale367, %out_scale367, %clip_min367, %clip_max367, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 1, 7), float32] */;
  %591 = nn.conv2d(%589, %590, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %592 = nn.simulated_quantize(%591, %in_scale368, %out_scale368, %clip_min368, %clip_max368, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %593 = nn.simulated_quantize(meta[relay.Constant][133] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale369, %out_scale369, %clip_min369, %clip_max369, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %594 = add(%592, %593) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %595 = nn.simulated_quantize(%594, %in_scale370, %out_scale370, %clip_min370, %clip_max370, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %596 = nn.relu(%595) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %597 = nn.simulated_quantize(%596, %in_scale371, %out_scale371, %clip_min371, %clip_max371, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %598 = nn.simulated_quantize(meta[relay.Constant][134] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, %in_scale372, %out_scale372, %clip_min372, %clip_max372, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 7, 1), float32] */;
  %599 = nn.conv2d(%597, %598, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %600 = nn.simulated_quantize(%599, %in_scale373, %out_scale373, %clip_min373, %clip_max373, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %601 = nn.simulated_quantize(meta[relay.Constant][135] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale374, %out_scale374, %clip_min374, %clip_max374, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %602 = add(%600, %601) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %603 = nn.simulated_quantize(%602, %in_scale375, %out_scale375, %clip_min375, %clip_max375, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %604 = nn.relu(%603) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %605 = nn.simulated_quantize(%604, %in_scale376, %out_scale376, %clip_min376, %clip_max376, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %606 = nn.simulated_quantize(meta[relay.Constant][136] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, %in_scale377, %out_scale377, %clip_min377, %clip_max377, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 1, 7), float32] */;
  %607 = nn.conv2d(%605, %606, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %608 = nn.simulated_quantize(%607, %in_scale378, %out_scale378, %clip_min378, %clip_max378, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %609 = nn.simulated_quantize(meta[relay.Constant][137] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale379, %out_scale379, %clip_min379, %clip_max379, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %610 = add(%608, %609) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %611 = nn.simulated_quantize(%610, %in_scale380, %out_scale380, %clip_min380, %clip_max380, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %612 = nn.relu(%611) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %613 = nn.simulated_quantize(%612, %in_scale389, %out_scale389, %clip_min389, %clip_max389, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %614 = nn.simulated_quantize(%538, %in_scale381, %out_scale381, %clip_min381, %clip_max381, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %615 = nn.avg_pool2d(%614, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %616 = nn.simulated_quantize(%615, %in_scale382, %out_scale382, %clip_min382, %clip_max382, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %617 = nn.simulated_quantize(meta[relay.Constant][138] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale383, %out_scale383, %clip_min383, %clip_max383, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %618 = nn.conv2d(%616, %617, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %619 = nn.simulated_quantize(%618, %in_scale384, %out_scale384, %clip_min384, %clip_max384, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %620 = nn.simulated_quantize(meta[relay.Constant][139] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale385, %out_scale385, %clip_min385, %clip_max385, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %621 = add(%619, %620) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %622 = nn.simulated_quantize(%621, %in_scale386, %out_scale386, %clip_min386, %clip_max386, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %623 = nn.relu(%622) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %624 = nn.simulated_quantize(%623, %in_scale390, %out_scale390, %clip_min390, %clip_max390, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %625 = (%547, %572, %613, %624);
  %626 = concatenate(%625, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %627 = nn.simulated_quantize(%626, %in_scale391, %out_scale391, %clip_min391, %clip_max391, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %628 = nn.simulated_quantize(meta[relay.Constant][140] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale392, %out_scale392, %clip_min392, %clip_max392, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %629 = nn.conv2d(%627, %628, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %630 = nn.simulated_quantize(%629, %in_scale393, %out_scale393, %clip_min393, %clip_max393, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %631 = nn.simulated_quantize(meta[relay.Constant][141] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale394, %out_scale394, %clip_min394, %clip_max394, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %632 = add(%630, %631) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %633 = nn.simulated_quantize(%632, %in_scale395, %out_scale395, %clip_min395, %clip_max395, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %634 = nn.relu(%633) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %635 = nn.simulated_quantize(%634, %in_scale396, %out_scale396, %clip_min396, %clip_max396, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %636 = nn.simulated_quantize(meta[relay.Constant][142] /* ty=Tensor[(320, 192, 3, 3), float32] */ /* ty=Tensor[(320, 192, 3, 3), float32] */, %in_scale397, %out_scale397, %clip_min397, %clip_max397, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(320, 192, 3, 3), float32] */;
  %637 = nn.conv2d(%635, %636, strides=[2, 2], padding=[0, 0, 0, 0], channels=320, kernel_size=[3, 3]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %638 = nn.simulated_quantize(%637, %in_scale398, %out_scale398, %clip_min398, %clip_max398, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %639 = nn.simulated_quantize(meta[relay.Constant][143] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */, %in_scale399, %out_scale399, %clip_min399, %clip_max399, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(320, 1, 1), float32] */;
  %640 = add(%638, %639) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %641 = nn.simulated_quantize(%640, %in_scale400, %out_scale400, %clip_min400, %clip_max400, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %642 = nn.relu(%641) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %643 = nn.simulated_quantize(%642, %in_scale422, %out_scale422, %clip_min422, %clip_max422, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %644 = nn.simulated_quantize(%626, %in_scale401, %out_scale401, %clip_min401, %clip_max401, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %645 = nn.simulated_quantize(meta[relay.Constant][144] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, %in_scale402, %out_scale402, %clip_min402, %clip_max402, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %646 = nn.conv2d(%644, %645, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %647 = nn.simulated_quantize(%646, %in_scale403, %out_scale403, %clip_min403, %clip_max403, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %648 = nn.simulated_quantize(meta[relay.Constant][145] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale404, %out_scale404, %clip_min404, %clip_max404, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %649 = add(%647, %648) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %650 = nn.simulated_quantize(%649, %in_scale405, %out_scale405, %clip_min405, %clip_max405, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %651 = nn.relu(%650) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %652 = nn.simulated_quantize(%651, %in_scale406, %out_scale406, %clip_min406, %clip_max406, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %653 = nn.simulated_quantize(meta[relay.Constant][146] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, %in_scale407, %out_scale407, %clip_min407, %clip_max407, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 1, 7), float32] */;
  %654 = nn.conv2d(%652, %653, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %655 = nn.simulated_quantize(%654, %in_scale408, %out_scale408, %clip_min408, %clip_max408, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %656 = nn.simulated_quantize(meta[relay.Constant][147] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale409, %out_scale409, %clip_min409, %clip_max409, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %657 = add(%655, %656) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %658 = nn.simulated_quantize(%657, %in_scale410, %out_scale410, %clip_min410, %clip_max410, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %659 = nn.relu(%658) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %660 = nn.simulated_quantize(%659, %in_scale411, %out_scale411, %clip_min411, %clip_max411, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %661 = nn.simulated_quantize(meta[relay.Constant][148] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, %in_scale412, %out_scale412, %clip_min412, %clip_max412, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 7, 1), float32] */;
  %662 = nn.conv2d(%660, %661, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %663 = nn.simulated_quantize(%662, %in_scale413, %out_scale413, %clip_min413, %clip_max413, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %664 = nn.simulated_quantize(meta[relay.Constant][149] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale414, %out_scale414, %clip_min414, %clip_max414, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %665 = add(%663, %664) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %666 = nn.simulated_quantize(%665, %in_scale415, %out_scale415, %clip_min415, %clip_max415, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %667 = nn.relu(%666) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %668 = nn.simulated_quantize(%667, %in_scale416, %out_scale416, %clip_min416, %clip_max416, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %669 = nn.simulated_quantize(meta[relay.Constant][150] /* ty=Tensor[(192, 192, 3, 3), float32] */ /* ty=Tensor[(192, 192, 3, 3), float32] */, %in_scale417, %out_scale417, %clip_min417, %clip_max417, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 3, 3), float32] */;
  %670 = nn.conv2d(%668, %669, strides=[2, 2], padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %671 = nn.simulated_quantize(%670, %in_scale418, %out_scale418, %clip_min418, %clip_max418, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %672 = nn.simulated_quantize(meta[relay.Constant][151] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale419, %out_scale419, %clip_min419, %clip_max419, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %673 = add(%671, %672) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %674 = nn.simulated_quantize(%673, %in_scale420, %out_scale420, %clip_min420, %clip_max420, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %675 = nn.relu(%674) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %676 = nn.simulated_quantize(%675, %in_scale423, %out_scale423, %clip_min423, %clip_max423, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %677 = nn.simulated_quantize(%626, %in_scale421, %out_scale421, %clip_min421, %clip_max421, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %678 = nn.max_pool2d(%677, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %679 = nn.simulated_quantize(%678, %in_scale424, %out_scale424, %clip_min424, %clip_max424, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %680 = (%643, %676, %679);
  %681 = concatenate(%680, axis=1) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %682 = nn.simulated_quantize(%681, %in_scale425, %out_scale425, %clip_min425, %clip_max425, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %683 = nn.simulated_quantize(meta[relay.Constant][152] /* ty=Tensor[(320, 1280, 1, 1), float32] */ /* ty=Tensor[(320, 1280, 1, 1), float32] */, %in_scale426, %out_scale426, %clip_min426, %clip_max426, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(320, 1280, 1, 1), float32] */;
  %684 = nn.conv2d(%682, %683, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %685 = nn.simulated_quantize(%684, %in_scale427, %out_scale427, %clip_min427, %clip_max427, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %686 = nn.simulated_quantize(meta[relay.Constant][153] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */, %in_scale428, %out_scale428, %clip_min428, %clip_max428, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(320, 1, 1), float32] */;
  %687 = add(%685, %686) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %688 = nn.simulated_quantize(%687, %in_scale429, %out_scale429, %clip_min429, %clip_max429, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %689 = nn.relu(%688) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %690 = nn.simulated_quantize(%689, %in_scale475, %out_scale475, %clip_min475, %clip_max475, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %691 = nn.simulated_quantize(%681, %in_scale430, %out_scale430, %clip_min430, %clip_max430, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %692 = nn.simulated_quantize(meta[relay.Constant][154] /* ty=Tensor[(384, 1280, 1, 1), float32] */ /* ty=Tensor[(384, 1280, 1, 1), float32] */, %in_scale431, %out_scale431, %clip_min431, %clip_max431, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 1280, 1, 1), float32] */;
  %693 = nn.conv2d(%691, %692, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %694 = nn.simulated_quantize(%693, %in_scale432, %out_scale432, %clip_min432, %clip_max432, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %695 = nn.simulated_quantize(meta[relay.Constant][155] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale433, %out_scale433, %clip_min433, %clip_max433, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %696 = add(%694, %695) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %697 = nn.simulated_quantize(%696, %in_scale434, %out_scale434, %clip_min434, %clip_max434, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %698 = nn.relu(%697) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %699 = nn.simulated_quantize(%698, %in_scale435, %out_scale435, %clip_min435, %clip_max435, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %700 = nn.simulated_quantize(meta[relay.Constant][156] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, %in_scale436, %out_scale436, %clip_min436, %clip_max436, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 1, 3), float32] */;
  %701 = nn.conv2d(%699, %700, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %702 = nn.simulated_quantize(%701, %in_scale437, %out_scale437, %clip_min437, %clip_max437, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %703 = nn.simulated_quantize(meta[relay.Constant][157] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale438, %out_scale438, %clip_min438, %clip_max438, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %704 = add(%702, %703) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %705 = nn.simulated_quantize(%704, %in_scale439, %out_scale439, %clip_min439, %clip_max439, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %706 = nn.relu(%705) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %707 = nn.simulated_quantize(%706, %in_scale445, %out_scale445, %clip_min445, %clip_max445, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %708 = nn.simulated_quantize(%698, %in_scale440, %out_scale440, %clip_min440, %clip_max440, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %709 = nn.simulated_quantize(meta[relay.Constant][158] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, %in_scale441, %out_scale441, %clip_min441, %clip_max441, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 3, 1), float32] */;
  %710 = nn.conv2d(%708, %709, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %711 = nn.simulated_quantize(%710, %in_scale442, %out_scale442, %clip_min442, %clip_max442, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %712 = nn.simulated_quantize(meta[relay.Constant][159] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale443, %out_scale443, %clip_min443, %clip_max443, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %713 = add(%711, %712) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %714 = nn.simulated_quantize(%713, %in_scale444, %out_scale444, %clip_min444, %clip_max444, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %715 = nn.relu(%714) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %716 = nn.simulated_quantize(%715, %in_scale446, %out_scale446, %clip_min446, %clip_max446, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %717 = (%707, %716);
  %718 = concatenate(%717, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %719 = nn.simulated_quantize(%718, %in_scale476, %out_scale476, %clip_min476, %clip_max476, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %720 = nn.simulated_quantize(%681, %in_scale447, %out_scale447, %clip_min447, %clip_max447, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %721 = nn.simulated_quantize(meta[relay.Constant][160] /* ty=Tensor[(448, 1280, 1, 1), float32] */ /* ty=Tensor[(448, 1280, 1, 1), float32] */, %in_scale448, %out_scale448, %clip_min448, %clip_max448, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(448, 1280, 1, 1), float32] */;
  %722 = nn.conv2d(%720, %721, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %723 = nn.simulated_quantize(%722, %in_scale449, %out_scale449, %clip_min449, %clip_max449, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %724 = nn.simulated_quantize(meta[relay.Constant][161] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */, %in_scale450, %out_scale450, %clip_min450, %clip_max450, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(448, 1, 1), float32] */;
  %725 = add(%723, %724) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %726 = nn.simulated_quantize(%725, %in_scale451, %out_scale451, %clip_min451, %clip_max451, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %727 = nn.relu(%726) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %728 = nn.simulated_quantize(%727, %in_scale452, %out_scale452, %clip_min452, %clip_max452, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %729 = nn.simulated_quantize(meta[relay.Constant][162] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, %in_scale453, %out_scale453, %clip_min453, %clip_max453, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 448, 3, 3), float32] */;
  %730 = nn.conv2d(%728, %729, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %731 = nn.simulated_quantize(%730, %in_scale454, %out_scale454, %clip_min454, %clip_max454, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %732 = nn.simulated_quantize(meta[relay.Constant][163] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale455, %out_scale455, %clip_min455, %clip_max455, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %733 = add(%731, %732) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %734 = nn.simulated_quantize(%733, %in_scale456, %out_scale456, %clip_min456, %clip_max456, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %735 = nn.relu(%734) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %736 = nn.simulated_quantize(%735, %in_scale457, %out_scale457, %clip_min457, %clip_max457, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %737 = nn.simulated_quantize(meta[relay.Constant][164] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, %in_scale458, %out_scale458, %clip_min458, %clip_max458, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 1, 3), float32] */;
  %738 = nn.conv2d(%736, %737, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %739 = nn.simulated_quantize(%738, %in_scale459, %out_scale459, %clip_min459, %clip_max459, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %740 = nn.simulated_quantize(meta[relay.Constant][165] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale460, %out_scale460, %clip_min460, %clip_max460, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %741 = add(%739, %740) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %742 = nn.simulated_quantize(%741, %in_scale461, %out_scale461, %clip_min461, %clip_max461, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %743 = nn.relu(%742) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %744 = nn.simulated_quantize(%743, %in_scale467, %out_scale467, %clip_min467, %clip_max467, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %745 = nn.simulated_quantize(%735, %in_scale462, %out_scale462, %clip_min462, %clip_max462, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %746 = nn.simulated_quantize(meta[relay.Constant][166] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, %in_scale463, %out_scale463, %clip_min463, %clip_max463, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 3, 1), float32] */;
  %747 = nn.conv2d(%745, %746, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %748 = nn.simulated_quantize(%747, %in_scale464, %out_scale464, %clip_min464, %clip_max464, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %749 = nn.simulated_quantize(meta[relay.Constant][167] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale465, %out_scale465, %clip_min465, %clip_max465, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %750 = add(%748, %749) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %751 = nn.simulated_quantize(%750, %in_scale466, %out_scale466, %clip_min466, %clip_max466, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %752 = nn.relu(%751) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %753 = nn.simulated_quantize(%752, %in_scale468, %out_scale468, %clip_min468, %clip_max468, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %754 = (%744, %753);
  %755 = concatenate(%754, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %756 = nn.simulated_quantize(%755, %in_scale477, %out_scale477, %clip_min477, %clip_max477, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %757 = nn.simulated_quantize(%681, %in_scale469, %out_scale469, %clip_min469, %clip_max469, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %758 = nn.avg_pool2d(%757, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %759 = nn.simulated_quantize(%758, %in_scale470, %out_scale470, %clip_min470, %clip_max470, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %760 = nn.simulated_quantize(meta[relay.Constant][168] /* ty=Tensor[(192, 1280, 1, 1), float32] */ /* ty=Tensor[(192, 1280, 1, 1), float32] */, %in_scale471, %out_scale471, %clip_min471, %clip_max471, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 1280, 1, 1), float32] */;
  %761 = nn.conv2d(%759, %760, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %762 = nn.simulated_quantize(%761, %in_scale472, %out_scale472, %clip_min472, %clip_max472, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %763 = nn.simulated_quantize(meta[relay.Constant][169] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale473, %out_scale473, %clip_min473, %clip_max473, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %764 = add(%762, %763) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %765 = nn.simulated_quantize(%764, %in_scale474, %out_scale474, %clip_min474, %clip_max474, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %766 = nn.relu(%765) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %767 = nn.simulated_quantize(%766, %in_scale478, %out_scale478, %clip_min478, %clip_max478, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %768 = (%690, %719, %756, %767);
  %769 = concatenate(%768, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %770 = nn.simulated_quantize(%769, %in_scale479, %out_scale479, %clip_min479, %clip_max479, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %771 = nn.simulated_quantize(meta[relay.Constant][170] /* ty=Tensor[(320, 2048, 1, 1), float32] */ /* ty=Tensor[(320, 2048, 1, 1), float32] */, %in_scale480, %out_scale480, %clip_min480, %clip_max480, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(320, 2048, 1, 1), float32] */;
  %772 = nn.conv2d(%770, %771, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %773 = nn.simulated_quantize(%772, %in_scale481, %out_scale481, %clip_min481, %clip_max481, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %774 = nn.simulated_quantize(meta[relay.Constant][171] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */, %in_scale482, %out_scale482, %clip_min482, %clip_max482, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(320, 1, 1), float32] */;
  %775 = add(%773, %774) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %776 = nn.simulated_quantize(%775, %in_scale483, %out_scale483, %clip_min483, %clip_max483, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %777 = nn.relu(%776) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %778 = nn.simulated_quantize(%777, %in_scale529, %out_scale529, %clip_min529, %clip_max529, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %779 = nn.simulated_quantize(%769, %in_scale484, %out_scale484, %clip_min484, %clip_max484, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %780 = nn.simulated_quantize(meta[relay.Constant][172] /* ty=Tensor[(384, 2048, 1, 1), float32] */ /* ty=Tensor[(384, 2048, 1, 1), float32] */, %in_scale485, %out_scale485, %clip_min485, %clip_max485, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 2048, 1, 1), float32] */;
  %781 = nn.conv2d(%779, %780, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %782 = nn.simulated_quantize(%781, %in_scale486, %out_scale486, %clip_min486, %clip_max486, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %783 = nn.simulated_quantize(meta[relay.Constant][173] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale487, %out_scale487, %clip_min487, %clip_max487, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %784 = add(%782, %783) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %785 = nn.simulated_quantize(%784, %in_scale488, %out_scale488, %clip_min488, %clip_max488, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %786 = nn.relu(%785) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %787 = nn.simulated_quantize(%786, %in_scale489, %out_scale489, %clip_min489, %clip_max489, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %788 = nn.simulated_quantize(meta[relay.Constant][174] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, %in_scale490, %out_scale490, %clip_min490, %clip_max490, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 1, 3), float32] */;
  %789 = nn.conv2d(%787, %788, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %790 = nn.simulated_quantize(%789, %in_scale491, %out_scale491, %clip_min491, %clip_max491, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %791 = nn.simulated_quantize(meta[relay.Constant][175] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale492, %out_scale492, %clip_min492, %clip_max492, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %792 = add(%790, %791) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %793 = nn.simulated_quantize(%792, %in_scale493, %out_scale493, %clip_min493, %clip_max493, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %794 = nn.relu(%793) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %795 = nn.simulated_quantize(%794, %in_scale499, %out_scale499, %clip_min499, %clip_max499, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %796 = nn.simulated_quantize(%786, %in_scale494, %out_scale494, %clip_min494, %clip_max494, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %797 = nn.simulated_quantize(meta[relay.Constant][176] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, %in_scale495, %out_scale495, %clip_min495, %clip_max495, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 3, 1), float32] */;
  %798 = nn.conv2d(%796, %797, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %799 = nn.simulated_quantize(%798, %in_scale496, %out_scale496, %clip_min496, %clip_max496, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %800 = nn.simulated_quantize(meta[relay.Constant][177] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale497, %out_scale497, %clip_min497, %clip_max497, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %801 = add(%799, %800) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %802 = nn.simulated_quantize(%801, %in_scale498, %out_scale498, %clip_min498, %clip_max498, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %803 = nn.relu(%802) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %804 = nn.simulated_quantize(%803, %in_scale500, %out_scale500, %clip_min500, %clip_max500, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %805 = (%795, %804);
  %806 = concatenate(%805, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %807 = nn.simulated_quantize(%806, %in_scale530, %out_scale530, %clip_min530, %clip_max530, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %808 = nn.simulated_quantize(%769, %in_scale501, %out_scale501, %clip_min501, %clip_max501, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %809 = nn.simulated_quantize(meta[relay.Constant][178] /* ty=Tensor[(448, 2048, 1, 1), float32] */ /* ty=Tensor[(448, 2048, 1, 1), float32] */, %in_scale502, %out_scale502, %clip_min502, %clip_max502, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(448, 2048, 1, 1), float32] */;
  %810 = nn.conv2d(%808, %809, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %811 = nn.simulated_quantize(%810, %in_scale503, %out_scale503, %clip_min503, %clip_max503, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %812 = nn.simulated_quantize(meta[relay.Constant][179] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */, %in_scale504, %out_scale504, %clip_min504, %clip_max504, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(448, 1, 1), float32] */;
  %813 = add(%811, %812) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %814 = nn.simulated_quantize(%813, %in_scale505, %out_scale505, %clip_min505, %clip_max505, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %815 = nn.relu(%814) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %816 = nn.simulated_quantize(%815, %in_scale506, %out_scale506, %clip_min506, %clip_max506, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %817 = nn.simulated_quantize(meta[relay.Constant][180] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, %in_scale507, %out_scale507, %clip_min507, %clip_max507, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 448, 3, 3), float32] */;
  %818 = nn.conv2d(%816, %817, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %819 = nn.simulated_quantize(%818, %in_scale508, %out_scale508, %clip_min508, %clip_max508, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %820 = nn.simulated_quantize(meta[relay.Constant][181] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale509, %out_scale509, %clip_min509, %clip_max509, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %821 = add(%819, %820) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %822 = nn.simulated_quantize(%821, %in_scale510, %out_scale510, %clip_min510, %clip_max510, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %823 = nn.relu(%822) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %824 = nn.simulated_quantize(%823, %in_scale511, %out_scale511, %clip_min511, %clip_max511, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %825 = nn.simulated_quantize(meta[relay.Constant][182] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, %in_scale512, %out_scale512, %clip_min512, %clip_max512, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 1, 3), float32] */;
  %826 = nn.conv2d(%824, %825, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %827 = nn.simulated_quantize(%826, %in_scale513, %out_scale513, %clip_min513, %clip_max513, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %828 = nn.simulated_quantize(meta[relay.Constant][183] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale514, %out_scale514, %clip_min514, %clip_max514, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %829 = add(%827, %828) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %830 = nn.simulated_quantize(%829, %in_scale515, %out_scale515, %clip_min515, %clip_max515, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %831 = nn.relu(%830) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %832 = nn.simulated_quantize(%831, %in_scale521, %out_scale521, %clip_min521, %clip_max521, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %833 = nn.simulated_quantize(%823, %in_scale516, %out_scale516, %clip_min516, %clip_max516, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %834 = nn.simulated_quantize(meta[relay.Constant][184] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, %in_scale517, %out_scale517, %clip_min517, %clip_max517, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 3, 1), float32] */;
  %835 = nn.conv2d(%833, %834, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %836 = nn.simulated_quantize(%835, %in_scale518, %out_scale518, %clip_min518, %clip_max518, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %837 = nn.simulated_quantize(meta[relay.Constant][185] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, %in_scale519, %out_scale519, %clip_min519, %clip_max519, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %838 = add(%836, %837) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %839 = nn.simulated_quantize(%838, %in_scale520, %out_scale520, %clip_min520, %clip_max520, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %840 = nn.relu(%839) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %841 = nn.simulated_quantize(%840, %in_scale522, %out_scale522, %clip_min522, %clip_max522, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %842 = (%832, %841);
  %843 = concatenate(%842, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %844 = nn.simulated_quantize(%843, %in_scale531, %out_scale531, %clip_min531, %clip_max531, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %845 = nn.simulated_quantize(%769, %in_scale523, %out_scale523, %clip_min523, %clip_max523, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %846 = nn.avg_pool2d(%845, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %847 = nn.simulated_quantize(%846, %in_scale524, %out_scale524, %clip_min524, %clip_max524, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %848 = nn.simulated_quantize(meta[relay.Constant][186] /* ty=Tensor[(192, 2048, 1, 1), float32] */ /* ty=Tensor[(192, 2048, 1, 1), float32] */, %in_scale525, %out_scale525, %clip_min525, %clip_max525, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 2048, 1, 1), float32] */;
  %849 = nn.conv2d(%847, %848, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %850 = nn.simulated_quantize(%849, %in_scale526, %out_scale526, %clip_min526, %clip_max526, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %851 = nn.simulated_quantize(meta[relay.Constant][187] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, %in_scale527, %out_scale527, %clip_min527, %clip_max527, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %852 = add(%850, %851) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %853 = nn.simulated_quantize(%852, %in_scale528, %out_scale528, %clip_min528, %clip_max528, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %854 = nn.relu(%853) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %855 = nn.simulated_quantize(%854, %in_scale532, %out_scale532, %clip_min532, %clip_max532, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %856 = (%778, %807, %844, %855);
  %857 = concatenate(%856, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %858 = nn.simulated_quantize(%857, %in_scale533, %out_scale533, %clip_min533, %clip_max533, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %859 = nn.avg_pool2d(%858, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %860 = nn.simulated_quantize(%859, %in_scale534, %out_scale534, %clip_min534, %clip_max534, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %861 = nn.batch_flatten(%860) /* ty=Tensor[(32, 2048), float32] */;
  %862 = nn.simulated_quantize(%861, %in_scale535, %out_scale535, %clip_min535, %clip_max535, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048), float32] */;
  %863 = nn.simulated_quantize(meta[relay.Constant][188] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, %in_scale536, %out_scale536, %clip_min536, %clip_max536, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000, 2048), float32] */;
  %864 = nn.dense(%862, %863, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  %865 = nn.simulated_quantize(%864, %in_scale537, %out_scale537, %clip_min537, %clip_max537, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */;
  %866 = nn.simulated_quantize(meta[relay.Constant][189] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */, %in_scale538, %out_scale538, %clip_min538, %clip_max538, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000), float32] */;
  %867 = add(%865, %866) /* ty=Tensor[(32, 1000), float32] */;
  nn.simulated_quantize(%867, %in_scale539, %out_scale539, %clip_min539, %clip_max539, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
calculating threshold...
threshold method:
avg_range
thresholds: [2.6328714, 2.3602352, 12.065824, 4.038895, 13.020016, 13.020016, 1.962364, 16.575394, 4.9887137, 16.222195, 16.222195, 1.279021, 18.2013, 4.041763, 15.711964, 13.954992, 13.954992, 1.31267, 16.29665, 7.094276, 15.009352, 15.009352, 0.7347288, 12.659695, 9.251598, 12.553303, 12.553303, 12.553303, 0.55539376, 10.610788, 6.458848, 10.835763, 10.835763, 0.51406825, 10.755268, 3.0780153, 9.361836, 9.361836, 0.66891605, 9.37106, 4.6133475, 9.977282, 9.977282, 0.9048812, 13.44719, 7.991889, 10.989873, 10.989873, 0.77211726, 12.104381, 6.1439104, 13.117057, 13.117057, 0.74781567, 19.330097, 3.0267882, 18.472818, 18.472818, 7.913826, 1.4154208, 11.809603, 6.6556735, 7.3291736, 7.051488, 18.472818, 0.74861073, 14.490365, 4.264531, 14.008376, 14.008376, 0.5955357, 14.672113, 6.9303293, 13.359471, 13.359471, 0.5041235, 11.800966, 4.1340013, 10.409194, 10.409194, 0.6865485, 12.078745, 5.23171, 8.374466, 8.374466, 0.7145971, 10.136138, 4.6464834, 9.445021, 9.445021, 1.1885365, 17.309582, 2.2896428, 16.920353, 16.920353, 11.094852, 1.1952425, 15.430565, 4.416874, 13.282968, 10.640891, 16.920353, 1.0737723, 16.343807, 4.300724, 13.9632015, 10.449177, 0.6418416, 11.628702, 2.934396, 10.764244, 10.764244, 0.36714998, 9.588831, 2.7083352, 9.773143, 9.342621, 0.8906436, 14.058994, 5.0458527, 12.821347, 12.821347, 0.6013656, 11.428003, 2.114786, 11.316947, 11.316947, 0.6002355, 8.868477, 1.952292, 8.879394, 8.879394, 9.80882, 1.4492332, 11.832209, 4.774985, 12.317003, 6.1451626, 10.595314, 0.5099997, 10.54315, 4.899191, 8.520537, 8.520537, 0.6652739, 12.044025, 3.554875, 10.223427, 10.223427, 0.5947379, 10.728868, 3.038629, 10.034648, 10.034648, 0.5644395, 12.614764, 1.423963, 12.267465, 12.267465, 10.595314, 12.267465, 0.6881513, 10.497004, 3.3307908, 9.870791, 9.870791, 0.78116906, 11.282829, 2.5749738, 10.193617, 10.193617, 0.9599224, 10.883995, 1.9074258, 10.738835, 10.738835, 0.7903328, 10.384567, 2.165031, 9.735281, 9.735281, 0.60052913, 10.082382, 2.6436954, 8.269901, 8.269901, 0.6314415, 8.432455, 2.7555428, 7.066684, 7.066684, 0.765009, 8.602791, 2.2371757, 8.079626, 8.079626, 0.75104743, 12.54934, 2.471313, 12.332055, 12.332055, 0.51724476, 19.154474, 2.7970874, 19.085238, 19.085238, 7.356802, 1.4476806, 9.830336, 4.6358514, 8.537125, 6.1152954, 19.085238, 0.6792003, 12.262494, 2.193478, 11.482174, 10.633267, 0.9598259, 13.312948, 1.8335823, 13.235676, 13.235676, 0.6195499, 9.484833, 1.5446676, 9.890749, 9.890749, 0.7080584, 9.732739, 1.8019606, 8.9589615, 8.9589615, 0.884484, 11.355179, 3.3123145, 10.339099, 10.339099, 0.51855326, 11.518238, 1.4913455, 11.444398, 11.444398, 0.7107782, 15.455397, 2.3764243, 14.238655, 14.238655, 0.63944393, 21.546165, 2.3433688, 20.109032, 20.109032, 0.66751224, 22.700808, 3.3206854, 22.338528, 22.338528, 5.7801733, 0.9621532, 8.215586, 2.7727678, 8.354459, 6.464248, 22.338528, 1.1758604, 13.509401, 1.8875171, 13.829502, 13.829502, 0.5845544, 14.810645, 2.8225396, 12.803604, 12.803604, 1.1270542, 15.715628, 1.5113342, 15.453755, 15.453755, 0.6649887, 13.249524, 2.8508985, 13.1026325, 13.1026325, 0.84865415, 17.00101, 2.2719626, 15.889892, 15.889892, 0.94345284, 16.411385, 1.8475466, 16.727106, 16.727106, 0.62848896, 11.574715, 2.0777895, 11.313255, 11.313255, 0.9571489, 15.55384, 1.3082336, 16.044205, 11.781048, 0.9496047, 11.448919, 2.8602483, 10.766972, 10.534952, 6.1676292, 1.7909923, 8.292303, 1.1869276, 7.9545326, 5.8451796, 13.933533, 0.98677474, 15.45469, 0.80150926, 15.36489, 7.524719, 0.9906912, 12.012684, 1.1678783, 11.638571, 11.638571, 1.0864348, 13.870327, 1.4344316, 14.039324, 14.039324, 0.33959627, 9.884438, 2.9625235, 10.097863, 8.625576, 0.6093746, 13.462838, 2.3437815, 12.597473, 12.597473, 0.55741876, 12.285763, 1.5553131, 12.173564, 12.173564, 0.65304595, 13.004351, 1.9077833, 13.047298, 13.047298, 0.40914342, 13.369569, 2.121026, 13.940378, 13.940378, 0.3439411, 8.712378, 1.6823571, 8.996961, 8.264077, 6.527837, 0.9189665, 7.248463, 0.95216954, 7.169874, 4.9490304, 8.906408, 0.8908913, 14.494164, 2.821383, 14.856981, 11.869261, 0.6321445, 16.669294, 3.43237, 16.5741, 16.5741, 1.2494586, 14.51338, 2.9126673, 14.695057, 12.045427, 0.49153587, 13.483884, 2.605875, 13.627424, 13.627424, 0.4021762, 10.113058, 3.6154826, 9.992454, 9.711899, 0.95714265, 10.869899, 2.029177, 10.032608, 9.048611, 8.906408, 16.5741, 0.9461251, 9.953763, 1.7416651, 10.77752, 6.1388764, 0.78198653, 10.149608, 1.3491286, 9.828022, 9.828022, 0.6497981, 10.310915, 1.2121844, 10.601075, 8.434524, 1.0432422, 18.792377, 0.8334571, 18.286448, 18.286448, 18.286448, 0.73205537, 9.13486, 3.1731215, 9.873434, 7.4345064, 0.32545602, 8.348713, 2.3684962, 8.034111, 6.9484386, 0.6720498, 9.14498, 3.249198, 9.111607, 8.678696, 0.46124947, 9.487292, 3.512743, 8.938843, 6.666424, 8.678696, 8.041417, 1.1557106, 7.9648037, 1.077241, 8.030485, 3.7890759, 18.803204, 2.4350264, 12.920756, 0.35487694, 12.860561, 12.860561, 1.0867004, 15.692598, 1.4524096, 15.603008, 15.603008, 1.9439151, 34.635235, 0.5548153, 34.27431, 34.27431, 2.057164, 34.920853, 0.54182637, 34.572124, 34.572124, 39.721336, 0.8023834, 12.622428, 1.977705, 12.8295, 10.481781, 0.64310175, 18.393398, 2.9031682, 18.597048, 18.597048, 0.915376, 16.845688, 0.6826771, 16.534765, 16.534765, 0.9398305, 14.640697, 0.5877044, 14.328748, 14.328748, 16.940733, 9.98681, 2.8595436, 8.662158, 0.7978425, 8.956129, 8.956129, 39.721336, 6.822217, 6.822217, 0.23357357, 19.511122, 1.2305, 19.599134]

calculate parameters
---------
data[%0] -> nn.conv2d[%2]
  bit=8, threshold=2.632871389389038
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.02056930772960186, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%1] -> nn.conv2d[%2]
  bit=8, threshold=2.3602352142333984
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.018439337611198425, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%2] -> add[%4]
  bit=32, threshold=4.038895130157471
  SimulatedQuantizeParams(in_scale=0.0003792844, out_scale=5.618586929045932e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%3] -> add[%4]
  bit=32, threshold=4.038895130157471
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.618586929045932e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%4] -> nn.relu[%5]
  bit=32, threshold=13.020015716552734
  SimulatedQuantizeParams(in_scale=5.618587e-09, out_scale=5.618586929045932e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%5] -> nn.conv2d[%7]
  bit=8, threshold=13.020015716552734
  SimulatedQuantizeParams(in_scale=5.618587e-09, out_scale=0.10171887278556824, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%6] -> nn.conv2d[%7]
  bit=8, threshold=1.9623639583587646
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.015330968424677849, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%7] -> add[%9]
  bit=32, threshold=4.98871374130249
  SimulatedQuantizeParams(in_scale=0.0015594488, out_scale=7.718519157151604e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%8] -> add[%9]
  bit=32, threshold=4.98871374130249
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.718519157151604e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%9] -> nn.relu[%10]
  bit=32, threshold=16.22219467163086
  SimulatedQuantizeParams(in_scale=7.718519e-09, out_scale=7.718519157151604e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%10] -> nn.conv2d[%12]
  bit=8, threshold=16.22219467163086
  SimulatedQuantizeParams(in_scale=7.718519e-09, out_scale=0.1267358958721161, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%11] -> nn.conv2d[%12]
  bit=8, threshold=1.2790210247039795
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00999235175549984, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%12] -> add[%14]
  bit=32, threshold=4.041762828826904
  SimulatedQuantizeParams(in_scale=0.0012663896, out_scale=8.475640633776038e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%13] -> add[%14]
  bit=32, threshold=4.041762828826904
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.475640633776038e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%14] -> nn.relu[%15]
  bit=32, threshold=15.711963653564453
  SimulatedQuantizeParams(in_scale=8.475641e-09, out_scale=8.475640633776038e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%15] -> nn.max_pool2d[%16]
  bit=32, threshold=13.954992294311523
  SimulatedQuantizeParams(in_scale=8.475641e-09, out_scale=6.498299676138686e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%16] -> nn.conv2d[%18]
  bit=8, threshold=13.954992294311523
  SimulatedQuantizeParams(in_scale=6.4982997e-09, out_scale=0.10902337729930878, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%17] -> nn.conv2d[%18]
  bit=8, threshold=1.3126699924468994
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.010255234315991402, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%18] -> add[%20]
  bit=32, threshold=7.094275951385498
  SimulatedQuantizeParams(in_scale=0.0011180603, out_scale=7.588718986539789e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%19] -> add[%20]
  bit=32, threshold=7.094275951385498
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.588718986539789e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%20] -> nn.relu[%21]
  bit=32, threshold=15.00935173034668
  SimulatedQuantizeParams(in_scale=7.588719e-09, out_scale=7.588718986539789e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%21] -> nn.conv2d[%23]
  bit=8, threshold=15.00935173034668
  SimulatedQuantizeParams(in_scale=7.588719e-09, out_scale=0.11726056039333344, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%22] -> nn.conv2d[%23]
  bit=8, threshold=0.7347288131713867
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005740068852901459, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%23] -> add[%25]
  bit=32, threshold=9.251598358154297
  SimulatedQuantizeParams(in_scale=0.0006730837, out_scale=5.895129717714553e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%24] -> add[%25]
  bit=32, threshold=9.251598358154297
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.895129717714553e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%25] -> nn.relu[%26]
  bit=32, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.8951297e-09, out_scale=5.895129717714553e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%26] -> nn.max_pool2d[%27]
  bit=32, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.8951297e-09, out_scale=5.8455871254636804e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%27] -> nn.conv2d[%29]
  bit=8, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.845587e-09, out_scale=0.09807267785072327, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%28] -> nn.conv2d[%29]
  bit=8, threshold=0.5553937554359436
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004339013714343309, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%29] -> add[%31]
  bit=32, threshold=6.458847999572754
  SimulatedQuantizeParams(in_scale=0.0004255387, out_scale=4.941033360239544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%30] -> add[%31]
  bit=32, threshold=6.458847999572754
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.941033360239544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%31] -> nn.relu[%32]
  bit=32, threshold=10.835762977600098
  SimulatedQuantizeParams(in_scale=4.9410334e-09, out_scale=4.941033360239544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%27] -> nn.conv2d[%34]
  bit=8, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.845587e-09, out_scale=0.09807267785072327, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%33] -> nn.conv2d[%34]
  bit=8, threshold=0.5140682458877563
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0040161581709980965, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%34] -> add[%36]
  bit=32, threshold=3.0780153274536133
  SimulatedQuantizeParams(in_scale=0.00039387538, out_scale=5.008311987353409e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%35] -> add[%36]
  bit=32, threshold=3.0780153274536133
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.008311987353409e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%36] -> nn.relu[%37]
  bit=32, threshold=9.361836433410645
  SimulatedQuantizeParams(in_scale=5.008312e-09, out_scale=5.008311987353409e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%37] -> nn.conv2d[%39]
  bit=8, threshold=9.361836433410645
  SimulatedQuantizeParams(in_scale=5.008312e-09, out_scale=0.07313934713602066, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%38] -> nn.conv2d[%39]
  bit=8, threshold=0.6689160466194153
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005225906614214182, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%39] -> add[%41]
  bit=32, threshold=4.61334753036499
  SimulatedQuantizeParams(in_scale=0.0003822194, out_scale=4.363740035984165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%40] -> add[%41]
  bit=32, threshold=4.61334753036499
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.363740035984165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%41] -> nn.relu[%42]
  bit=32, threshold=9.97728157043457
  SimulatedQuantizeParams(in_scale=4.36374e-09, out_scale=4.363740035984165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%27] -> nn.conv2d[%44]
  bit=8, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.845587e-09, out_scale=0.09807267785072327, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%43] -> nn.conv2d[%44]
  bit=8, threshold=0.9048811793327332
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007069384213536978, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%44] -> add[%46]
  bit=32, threshold=7.991888999938965
  SimulatedQuantizeParams(in_scale=0.0006933134, out_scale=6.261835938659033e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%45] -> add[%46]
  bit=32, threshold=7.991888999938965
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.261835938659033e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%46] -> nn.relu[%47]
  bit=32, threshold=10.989872932434082
  SimulatedQuantizeParams(in_scale=6.261836e-09, out_scale=6.261835938659033e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%47] -> nn.conv2d[%49]
  bit=8, threshold=10.989872932434082
  SimulatedQuantizeParams(in_scale=6.261836e-09, out_scale=0.08585838228464127, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%48] -> nn.conv2d[%49]
  bit=8, threshold=0.7721172571182251
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006032166071236134, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%49] -> add[%51]
  bit=32, threshold=6.1439104080200195
  SimulatedQuantizeParams(in_scale=0.00051791203, out_scale=5.63654145580017e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%50] -> add[%51]
  bit=32, threshold=6.1439104080200195
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.63654145580017e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%51] -> nn.relu[%52]
  bit=32, threshold=13.117056846618652
  SimulatedQuantizeParams(in_scale=5.6365415e-09, out_scale=5.63654145580017e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%52] -> nn.conv2d[%54]
  bit=8, threshold=13.117056846618652
  SimulatedQuantizeParams(in_scale=5.6365415e-09, out_scale=0.10247700661420822, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%53] -> nn.conv2d[%54]
  bit=8, threshold=0.7478156685829163
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005842309910804033, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%54] -> add[%56]
  bit=32, threshold=3.0267882347106934
  SimulatedQuantizeParams(in_scale=0.0005987024, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%55] -> add[%56]
  bit=32, threshold=3.0267882347106934
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%56] -> nn.relu[%57]
  bit=32, threshold=18.47281837463379
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%27] -> nn.avg_pool2d[%58]
  not quantized
  SimulatedQuantizeParams(in_scale=5.845587e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%58] -> nn.conv2d[%60]
  bit=8, threshold=7.913825988769531
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.06182676553726196, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%59] -> nn.conv2d[%60]
  bit=8, threshold=1.4154207706451416
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.011057974770665169, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%60] -> add[%62]
  bit=32, threshold=6.655673503875732
  SimulatedQuantizeParams(in_scale=0.00068367884, out_scale=5.4992748133031455e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%61] -> add[%62]
  bit=32, threshold=6.655673503875732
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.4992748133031455e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%62] -> nn.relu[%63]
  bit=32, threshold=7.329173564910889
  SimulatedQuantizeParams(in_scale=5.499275e-09, out_scale=5.4992748133031455e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%32] -> concatenate[%64]
  bit=32, threshold=7.051487922668457
  SimulatedQuantizeParams(in_scale=4.9410334e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%42] -> concatenate[%64]
  bit=32, threshold=7.051487922668457
  SimulatedQuantizeParams(in_scale=4.36374e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%57] -> concatenate[%64]
  bit=32, threshold=7.051487922668457
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%63] -> concatenate[%64]
  bit=32, threshold=7.051487922668457
  SimulatedQuantizeParams(in_scale=5.499275e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%64] -> nn.conv2d[%66]
  bit=8, threshold=18.47281837463379
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=0.14431889355182648, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%65] -> nn.conv2d[%66]
  bit=8, threshold=0.7486107349395752
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005848521366715431, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%66] -> add[%68]
  bit=32, threshold=4.264531135559082
  SimulatedQuantizeParams(in_scale=0.0008440521, out_scale=6.747602032675104e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%67] -> add[%68]
  bit=32, threshold=4.264531135559082
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.747602032675104e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%68] -> nn.relu[%69]
  bit=32, threshold=14.008376121520996
  SimulatedQuantizeParams(in_scale=6.747602e-09, out_scale=6.747602032675104e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%64] -> nn.conv2d[%71]
  bit=8, threshold=18.47281837463379
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=0.14431889355182648, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%70] -> nn.conv2d[%71]
  bit=8, threshold=0.5955356955528259
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0046526226215064526, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%71] -> add[%73]
  bit=32, threshold=6.930329322814941
  SimulatedQuantizeParams(in_scale=0.00067146134, out_scale=6.8322352220206994e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%72] -> add[%73]
  bit=32, threshold=6.930329322814941
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.8322352220206994e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%73] -> nn.relu[%74]
  bit=32, threshold=13.359471321105957
  SimulatedQuantizeParams(in_scale=6.832235e-09, out_scale=6.8322352220206994e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%74] -> nn.conv2d[%76]
  bit=8, threshold=13.359471321105957
  SimulatedQuantizeParams(in_scale=6.832235e-09, out_scale=0.10437086969614029, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%75] -> nn.conv2d[%76]
  bit=8, threshold=0.5041235089302063
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003938464913517237, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%76] -> add[%78]
  bit=32, threshold=4.1340012550354
  SimulatedQuantizeParams(in_scale=0.00041106102, out_scale=5.495253141418743e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%77] -> add[%78]
  bit=32, threshold=4.1340012550354
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.495253141418743e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%78] -> nn.relu[%79]
  bit=32, threshold=10.409193992614746
  SimulatedQuantizeParams(in_scale=5.495253e-09, out_scale=5.495253141418743e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%64] -> nn.conv2d[%81]
  bit=8, threshold=18.47281837463379
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=0.14431889355182648, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%80] -> nn.conv2d[%81]
  bit=8, threshold=0.6865484714508057
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005363659933209419, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%81] -> add[%83]
  bit=32, threshold=5.231709957122803
  SimulatedQuantizeParams(in_scale=0.0007740775, out_scale=5.624603893750191e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%82] -> add[%83]
  bit=32, threshold=5.231709957122803
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.624603893750191e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%83] -> nn.relu[%84]
  bit=32, threshold=8.374465942382812
  SimulatedQuantizeParams(in_scale=5.624604e-09, out_scale=5.624603893750191e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%84] -> nn.conv2d[%86]
  bit=8, threshold=8.374465942382812
  SimulatedQuantizeParams(in_scale=5.624604e-09, out_scale=0.06542551517486572, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%85] -> nn.conv2d[%86]
  bit=8, threshold=0.7145971059799194
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005582789890468121, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%86] -> add[%88]
  bit=32, threshold=4.646483421325684
  SimulatedQuantizeParams(in_scale=0.0003652569, out_scale=4.720007051872699e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%87] -> add[%88]
  bit=32, threshold=4.646483421325684
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.720007051872699e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%88] -> nn.relu[%89]
  bit=32, threshold=9.44502067565918
  SimulatedQuantizeParams(in_scale=4.720007e-09, out_scale=4.720007051872699e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%89] -> nn.conv2d[%91]
  bit=8, threshold=9.44502067565918
  SimulatedQuantizeParams(in_scale=4.720007e-09, out_scale=0.07378922402858734, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%90] -> nn.conv2d[%91]
  bit=8, threshold=1.188536524772644
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.009285441599786282, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%91] -> add[%93]
  bit=32, threshold=2.289642810821533
  SimulatedQuantizeParams(in_scale=0.0006851655, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%92] -> add[%93]
  bit=32, threshold=2.289642810821533
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%93] -> nn.relu[%94]
  bit=32, threshold=16.920352935791016
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%64] -> nn.avg_pool2d[%95]
  not quantized
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%95] -> nn.conv2d[%97]
  bit=8, threshold=11.094852447509766
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.08667853474617004, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%96] -> nn.conv2d[%97]
  bit=8, threshold=1.1952425241470337
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0093378322198987, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%97] -> add[%99]
  bit=32, threshold=4.416873931884766
  SimulatedQuantizeParams(in_scale=0.00080938963, out_scale=7.185416706079195e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%98] -> add[%99]
  bit=32, threshold=4.416873931884766
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.185416706079195e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%99] -> nn.relu[%100]
  bit=32, threshold=13.282967567443848
  SimulatedQuantizeParams(in_scale=7.1854167e-09, out_scale=7.185416706079195e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%69] -> concatenate[%101]
  bit=32, threshold=10.640891075134277
  SimulatedQuantizeParams(in_scale=6.747602e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%79] -> concatenate[%101]
  bit=32, threshold=10.640891075134277
  SimulatedQuantizeParams(in_scale=5.495253e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%94] -> concatenate[%101]
  bit=32, threshold=10.640891075134277
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%100] -> concatenate[%101]
  bit=32, threshold=10.640891075134277
  SimulatedQuantizeParams(in_scale=7.1854167e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%101] -> nn.conv2d[%103]
  bit=8, threshold=16.920352935791016
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=0.1321902573108673, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%102] -> nn.conv2d[%103]
  bit=8, threshold=1.0737723112106323
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008388846181333065, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%103] -> add[%105]
  bit=32, threshold=4.300724029541016
  SimulatedQuantizeParams(in_scale=0.0011089237, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%104] -> add[%105]
  bit=32, threshold=4.300724029541016
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%105] -> nn.relu[%106]
  bit=32, threshold=13.963201522827148
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%101] -> nn.conv2d[%108]
  bit=8, threshold=16.920352935791016
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=0.1321902573108673, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%107] -> nn.conv2d[%108]
  bit=8, threshold=0.6418415904045105
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005014387425035238, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%108] -> add[%110]
  bit=32, threshold=2.9343960285186768
  SimulatedQuantizeParams(in_scale=0.0006628532, out_scale=5.415036419265107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%109] -> add[%110]
  bit=32, threshold=2.9343960285186768
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.415036419265107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%110] -> nn.relu[%111]
  bit=32, threshold=10.764244079589844
  SimulatedQuantizeParams(in_scale=5.4150364e-09, out_scale=5.415036419265107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%111] -> nn.conv2d[%113]
  bit=8, threshold=10.764244079589844
  SimulatedQuantizeParams(in_scale=5.4150364e-09, out_scale=0.08409565687179565, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%112] -> nn.conv2d[%113]
  bit=8, threshold=0.3671499788761139
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0028683592099696398, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%113] -> add[%115]
  bit=32, threshold=2.7083351612091064
  SimulatedQuantizeParams(in_scale=0.00024121655, out_scale=4.465147362964217e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%114] -> add[%115]
  bit=32, threshold=2.7083351612091064
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.465147362964217e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%115] -> nn.relu[%116]
  bit=32, threshold=9.77314281463623
  SimulatedQuantizeParams(in_scale=4.4651474e-09, out_scale=4.465147362964217e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%101] -> nn.conv2d[%118]
  bit=8, threshold=16.920352935791016
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=0.1321902573108673, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%117] -> nn.conv2d[%118]
  bit=8, threshold=0.8906435966491699
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00695815309882164, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%118] -> add[%120]
  bit=32, threshold=5.0458526611328125
  SimulatedQuantizeParams(in_scale=0.0009198001, out_scale=6.5467293808296745e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%119] -> add[%120]
  bit=32, threshold=5.0458526611328125
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.5467293808296745e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%120] -> nn.relu[%121]
  bit=32, threshold=12.8213472366333
  SimulatedQuantizeParams(in_scale=6.5467294e-09, out_scale=6.5467293808296745e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%121] -> nn.conv2d[%123]
  bit=8, threshold=12.8213472366333
  SimulatedQuantizeParams(in_scale=6.5467294e-09, out_scale=0.10016677528619766, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%122] -> nn.conv2d[%123]
  bit=8, threshold=0.6013656258583069
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0046981689520180225, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%123] -> add[%125]
  bit=32, threshold=2.11478590965271
  SimulatedQuantizeParams(in_scale=0.00047060044, out_scale=5.321578733230581e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%124] -> add[%125]
  bit=32, threshold=2.11478590965271
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.321578733230581e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%125] -> nn.relu[%126]
  bit=32, threshold=11.316946983337402
  SimulatedQuantizeParams(in_scale=5.3215787e-09, out_scale=5.321578733230581e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%126] -> nn.conv2d[%128]
  bit=8, threshold=11.316946983337402
  SimulatedQuantizeParams(in_scale=5.3215787e-09, out_scale=0.08841364830732346, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%127] -> nn.conv2d[%128]
  bit=8, threshold=0.6002355217933655
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004689340014010668, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%128] -> add[%130]
  bit=32, threshold=1.9522919654846191
  SimulatedQuantizeParams(in_scale=0.00041460167, out_scale=4.1297063546608115e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%129] -> add[%130]
  bit=32, threshold=1.9522919654846191
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.1297063546608115e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%130] -> nn.relu[%131]
  bit=32, threshold=8.879393577575684
  SimulatedQuantizeParams(in_scale=4.1297064e-09, out_scale=4.1297063546608115e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%101] -> nn.avg_pool2d[%132]
  not quantized
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%132] -> nn.conv2d[%134]
  bit=8, threshold=9.808819770812988
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.07663140445947647, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%133] -> nn.conv2d[%134]
  bit=8, threshold=1.4492331743240356
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.011322134174406528, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%134] -> add[%136]
  bit=32, threshold=4.774984836578369
  SimulatedQuantizeParams(in_scale=0.00086763105, out_scale=5.509801503933431e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%135] -> add[%136]
  bit=32, threshold=4.774984836578369
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.509801503933431e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%136] -> nn.relu[%137]
  bit=32, threshold=12.31700325012207
  SimulatedQuantizeParams(in_scale=5.5098015e-09, out_scale=5.509801503933431e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%106] -> concatenate[%138]
  bit=32, threshold=6.145162582397461
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%116] -> concatenate[%138]
  bit=32, threshold=6.145162582397461
  SimulatedQuantizeParams(in_scale=4.4651474e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%131] -> concatenate[%138]
  bit=32, threshold=6.145162582397461
  SimulatedQuantizeParams(in_scale=4.1297064e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%137] -> concatenate[%138]
  bit=32, threshold=6.145162582397461
  SimulatedQuantizeParams(in_scale=5.5098015e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%138] -> nn.conv2d[%140]
  bit=8, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=0.08277589082717896, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%139] -> nn.conv2d[%140]
  bit=8, threshold=0.509999692440033
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0039843725971877575, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%140] -> add[%142]
  bit=32, threshold=4.899190902709961
  SimulatedQuantizeParams(in_scale=0.00032981, out_scale=4.909536777120138e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%141] -> add[%142]
  bit=32, threshold=4.899190902709961
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.909536777120138e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%142] -> nn.relu[%143]
  bit=32, threshold=8.520537376403809
  SimulatedQuantizeParams(in_scale=4.9095368e-09, out_scale=4.909536777120138e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%138] -> nn.conv2d[%145]
  bit=8, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=0.08277589082717896, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%144] -> nn.conv2d[%145]
  bit=8, threshold=0.665273904800415
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0051974523812532425, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%145] -> add[%147]
  bit=32, threshold=3.554874897003174
  SimulatedQuantizeParams(in_scale=0.00043022376, out_scale=5.608436381976389e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%146] -> add[%147]
  bit=32, threshold=3.554874897003174
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.608436381976389e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%147] -> nn.relu[%148]
  bit=32, threshold=10.223426818847656
  SimulatedQuantizeParams(in_scale=5.6084364e-09, out_scale=5.608436381976389e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%148] -> nn.conv2d[%150]
  bit=8, threshold=10.223426818847656
  SimulatedQuantizeParams(in_scale=5.6084364e-09, out_scale=0.07987052202224731, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%149] -> nn.conv2d[%150]
  bit=8, threshold=0.5947378873825073
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0046463897451758385, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%150] -> add[%152]
  bit=32, threshold=3.0386290550231934
  SimulatedQuantizeParams(in_scale=0.00037110958, out_scale=4.9960182657571295e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%151] -> add[%152]
  bit=32, threshold=3.0386290550231934
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.9960182657571295e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%152] -> nn.relu[%153]
  bit=32, threshold=10.034647941589355
  SimulatedQuantizeParams(in_scale=4.9960183e-09, out_scale=4.9960182657571295e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%153] -> nn.conv2d[%155]
  bit=8, threshold=10.034647941589355
  SimulatedQuantizeParams(in_scale=4.9960183e-09, out_scale=0.07839568704366684, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%154] -> nn.conv2d[%155]
  bit=8, threshold=0.5644394755363464
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0044096834026277065, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%155] -> add[%157]
  bit=32, threshold=1.423962950706482
  SimulatedQuantizeParams(in_scale=0.00034570016, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%156] -> add[%157]
  bit=32, threshold=1.423962950706482
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%157] -> nn.relu[%158]
  bit=32, threshold=12.267464637756348
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%138] -> nn.max_pool2d[%159]
  bit=32, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=4.933827568720517e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%143] -> concatenate[%160]
  bit=32, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=4.9095368e-09, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%158] -> concatenate[%160]
  bit=32, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%159] -> concatenate[%160]
  bit=32, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=4.9338276e-09, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%160] -> nn.conv2d[%162]
  bit=8, threshold=12.267464637756348
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=0.09583956748247147, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%161] -> nn.conv2d[%162]
  bit=8, threshold=0.6881512999534607
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005376182030886412, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%162] -> add[%164]
  bit=32, threshold=3.3307907581329346
  SimulatedQuantizeParams(in_scale=0.00051525095, out_scale=4.888048188433913e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%163] -> add[%164]
  bit=32, threshold=3.3307907581329346
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.888048188433913e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%164] -> nn.relu[%165]
  bit=32, threshold=9.8707914352417
  SimulatedQuantizeParams(in_scale=4.888048e-09, out_scale=4.888048188433913e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%160] -> nn.conv2d[%167]
  bit=8, threshold=12.267464637756348
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=0.09583956748247147, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%166] -> nn.conv2d[%167]
  bit=8, threshold=0.781169056892395
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006102883256971836, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%167] -> add[%169]
  bit=32, threshold=2.5749738216400146
  SimulatedQuantizeParams(in_scale=0.0005848977, out_scale=5.253976809171945e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%168] -> add[%169]
  bit=32, threshold=2.5749738216400146
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.253976809171945e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%169] -> nn.relu[%170]
  bit=32, threshold=10.19361686706543
  SimulatedQuantizeParams(in_scale=5.253977e-09, out_scale=5.253976809171945e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%170] -> nn.conv2d[%172]
  bit=8, threshold=10.19361686706543
  SimulatedQuantizeParams(in_scale=5.253977e-09, out_scale=0.07963763177394867, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%171] -> nn.conv2d[%172]
  bit=8, threshold=0.9599223732948303
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007499393541365862, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%172] -> add[%174]
  bit=32, threshold=1.9074257612228394
  SimulatedQuantizeParams(in_scale=0.00059723394, out_scale=5.06825514889897e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%173] -> add[%174]
  bit=32, threshold=1.9074257612228394
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.06825514889897e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%174] -> nn.relu[%175]
  bit=32, threshold=10.738835334777832
  SimulatedQuantizeParams(in_scale=5.068255e-09, out_scale=5.06825514889897e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%175] -> nn.conv2d[%177]
  bit=8, threshold=10.738835334777832
  SimulatedQuantizeParams(in_scale=5.068255e-09, out_scale=0.08389715105295181, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%176] -> nn.conv2d[%177]
  bit=8, threshold=0.7903327941894531
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0061744749546051025, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%177] -> add[%179]
  bit=32, threshold=2.1650309562683105
  SimulatedQuantizeParams(in_scale=0.0005180209, out_scale=4.835690958771011e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%178] -> add[%179]
  bit=32, threshold=2.1650309562683105
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.835690958771011e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%179] -> nn.relu[%180]
  bit=32, threshold=9.735280990600586
  SimulatedQuantizeParams(in_scale=4.835691e-09, out_scale=4.835690958771011e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%160] -> nn.conv2d[%182]
  bit=8, threshold=12.267464637756348
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=0.09583956748247147, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%181] -> nn.conv2d[%182]
  bit=8, threshold=0.600529134273529
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004691633861511946, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%182] -> add[%184]
  bit=32, threshold=2.64369535446167
  SimulatedQuantizeParams(in_scale=0.00044964417, out_scale=4.69497507538108e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%183] -> add[%184]
  bit=32, threshold=2.64369535446167
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.69497507538108e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%184] -> nn.relu[%185]
  bit=32, threshold=8.269901275634766
  SimulatedQuantizeParams(in_scale=4.694975e-09, out_scale=4.69497507538108e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%185] -> nn.conv2d[%187]
  bit=8, threshold=8.269901275634766
  SimulatedQuantizeParams(in_scale=4.694975e-09, out_scale=0.0646086037158966, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%186] -> nn.conv2d[%187]
  bit=8, threshold=0.6314414739608765
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004933136515319347, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%187] -> add[%189]
  bit=32, threshold=2.755542755126953
  SimulatedQuantizeParams(in_scale=0.00031872306, out_scale=3.926667879738943e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%188] -> add[%189]
  bit=32, threshold=2.755542755126953
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.926667879738943e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%189] -> nn.relu[%190]
  bit=32, threshold=7.066683769226074
  SimulatedQuantizeParams(in_scale=3.926668e-09, out_scale=3.926667879738943e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%190] -> nn.conv2d[%192]
  bit=8, threshold=7.066683769226074
  SimulatedQuantizeParams(in_scale=3.926668e-09, out_scale=0.055208466947078705, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%191] -> nn.conv2d[%192]
  bit=8, threshold=0.7650089859962463
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0059766327030956745, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%192] -> add[%194]
  bit=32, threshold=2.237175703048706
  SimulatedQuantizeParams(in_scale=0.00032996072, out_scale=4.005986653510263e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%193] -> add[%194]
  bit=32, threshold=2.237175703048706
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.005986653510263e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%194] -> nn.relu[%195]
  bit=32, threshold=8.079626083374023
  SimulatedQuantizeParams(in_scale=4.0059867e-09, out_scale=4.005986653510263e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%195] -> nn.conv2d[%197]
  bit=8, threshold=8.079626083374023
  SimulatedQuantizeParams(in_scale=4.0059867e-09, out_scale=0.06312207877635956, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%196] -> nn.conv2d[%197]
  bit=8, threshold=0.7510474324226379
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005867558065801859, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%197] -> add[%199]
  bit=32, threshold=2.471312999725342
  SimulatedQuantizeParams(in_scale=0.00037037247, out_scale=5.843741934796753e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%198] -> add[%199]
  bit=32, threshold=2.471312999725342
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.843741934796753e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%199] -> nn.relu[%200]
  bit=32, threshold=12.33205509185791
  SimulatedQuantizeParams(in_scale=5.843742e-09, out_scale=5.843741934796753e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%200] -> nn.conv2d[%202]
  bit=8, threshold=12.33205509185791
  SimulatedQuantizeParams(in_scale=5.843742e-09, out_scale=0.09634418040513992, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%201] -> nn.conv2d[%202]
  bit=8, threshold=0.5172447562217712
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004040974657982588, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%202] -> add[%204]
  bit=32, threshold=2.7970874309539795
  SimulatedQuantizeParams(in_scale=0.00038932438, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%203] -> add[%204]
  bit=32, threshold=2.7970874309539795
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%204] -> nn.relu[%205]
  bit=32, threshold=19.085237503051758
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%160] -> nn.avg_pool2d[%206]
  not quantized
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%206] -> nn.conv2d[%208]
  bit=8, threshold=7.356801986694336
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0574750155210495, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%207] -> nn.conv2d[%208]
  bit=8, threshold=1.4476805925369263
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.011310004629194736, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%208] -> add[%210]
  bit=32, threshold=4.6358513832092285
  SimulatedQuantizeParams(in_scale=0.00065004267, out_scale=4.577606738109807e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%209] -> add[%210]
  bit=32, threshold=4.6358513832092285
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.577606738109807e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%210] -> nn.relu[%211]
  bit=32, threshold=8.537124633789062
  SimulatedQuantizeParams(in_scale=4.5776067e-09, out_scale=4.577606738109807e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%165] -> concatenate[%212]
  bit=32, threshold=6.11529541015625
  SimulatedQuantizeParams(in_scale=4.888048e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%180] -> concatenate[%212]
  bit=32, threshold=6.11529541015625
  SimulatedQuantizeParams(in_scale=4.835691e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%205] -> concatenate[%212]
  bit=32, threshold=6.11529541015625
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%211] -> concatenate[%212]
  bit=32, threshold=6.11529541015625
  SimulatedQuantizeParams(in_scale=4.5776067e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%212] -> nn.conv2d[%214]
  bit=8, threshold=19.085237503051758
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=0.14910341799259186, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%213] -> nn.conv2d[%214]
  bit=8, threshold=0.679200291633606
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0053062522783875465, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%214] -> add[%216]
  bit=32, threshold=2.1934781074523926
  SimulatedQuantizeParams(in_scale=0.00079118036, out_scale=5.7101687822580516e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%215] -> add[%216]
  bit=32, threshold=2.1934781074523926
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.7101687822580516e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%216] -> nn.relu[%217]
  bit=32, threshold=11.482173919677734
  SimulatedQuantizeParams(in_scale=5.710169e-09, out_scale=5.7101687822580516e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%212] -> nn.conv2d[%219]
  bit=8, threshold=19.085237503051758
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=0.14910341799259186, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%218] -> nn.conv2d[%219]
  bit=8, threshold=0.959825873374939
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007498639635741711, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%219] -> add[%221]
  bit=32, threshold=1.8335822820663452
  SimulatedQuantizeParams(in_scale=0.0011180728, out_scale=6.199324609212908e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%220] -> add[%221]
  bit=32, threshold=1.8335822820663452
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.199324609212908e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%221] -> nn.relu[%222]
  bit=32, threshold=13.235675811767578
  SimulatedQuantizeParams(in_scale=6.1993246e-09, out_scale=6.199324609212908e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%222] -> nn.conv2d[%224]
  bit=8, threshold=13.235675811767578
  SimulatedQuantizeParams(in_scale=6.1993246e-09, out_scale=0.1034037172794342, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%223] -> nn.conv2d[%224]
  bit=8, threshold=0.6195498704910278
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004840233363211155, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%224] -> add[%226]
  bit=32, threshold=1.5446676015853882
  SimulatedQuantizeParams(in_scale=0.0005004981, out_scale=4.4167194346300676e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%225] -> add[%226]
  bit=32, threshold=1.5446676015853882
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.4167194346300676e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%226] -> nn.relu[%227]
  bit=32, threshold=9.890748977661133
  SimulatedQuantizeParams(in_scale=4.4167194e-09, out_scale=4.4167194346300676e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%227] -> nn.conv2d[%229]
  bit=8, threshold=9.890748977661133
  SimulatedQuantizeParams(in_scale=4.4167194e-09, out_scale=0.0772714763879776, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%228] -> nn.conv2d[%229]
  bit=8, threshold=0.7080584168434143
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005531706381589174, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%229] -> add[%231]
  bit=32, threshold=1.8019605875015259
  SimulatedQuantizeParams(in_scale=0.00042744313, out_scale=4.5321599806413815e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%230] -> add[%231]
  bit=32, threshold=1.8019605875015259
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.5321599806413815e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%231] -> nn.relu[%232]
  bit=32, threshold=8.958961486816406
  SimulatedQuantizeParams(in_scale=4.53216e-09, out_scale=4.5321599806413815e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%212] -> nn.conv2d[%234]
  bit=8, threshold=19.085237503051758
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=0.14910341799259186, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%233] -> nn.conv2d[%234]
  bit=8, threshold=0.8844839930534363
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006910031195729971, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%234] -> add[%236]
  bit=32, threshold=3.312314510345459
  SimulatedQuantizeParams(in_scale=0.0010303092, out_scale=5.28766719298801e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%235] -> add[%236]
  bit=32, threshold=3.312314510345459
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.28766719298801e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%236] -> nn.relu[%237]
  bit=32, threshold=10.339098930358887
  SimulatedQuantizeParams(in_scale=5.287667e-09, out_scale=5.28766719298801e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%237] -> nn.conv2d[%239]
  bit=8, threshold=10.339098930358887
  SimulatedQuantizeParams(in_scale=5.287667e-09, out_scale=0.0807742103934288, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%238] -> nn.conv2d[%239]
  bit=8, threshold=0.5185532569885254
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004051197320222855, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%239] -> add[%241]
  bit=32, threshold=1.4913455247879028
  SimulatedQuantizeParams(in_scale=0.00032723226, out_scale=5.363597566088174e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%240] -> add[%241]
  bit=32, threshold=1.4913455247879028
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.363597566088174e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%241] -> nn.relu[%242]
  bit=32, threshold=11.444397926330566
  SimulatedQuantizeParams(in_scale=5.3635976e-09, out_scale=5.363597566088174e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%242] -> nn.conv2d[%244]
  bit=8, threshold=11.444397926330566
  SimulatedQuantizeParams(in_scale=5.3635976e-09, out_scale=0.08940935879945755, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%243] -> nn.conv2d[%244]
  bit=8, threshold=0.7107781767845154
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005552954506129026, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%244] -> add[%246]
  bit=32, threshold=2.3764243125915527
  SimulatedQuantizeParams(in_scale=0.0004964861, out_scale=7.196979900925271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%245] -> add[%246]
  bit=32, threshold=2.3764243125915527
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.196979900925271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%246] -> nn.relu[%247]
  bit=32, threshold=14.238655090332031
  SimulatedQuantizeParams(in_scale=7.19698e-09, out_scale=7.196979900925271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%247] -> nn.conv2d[%249]
  bit=8, threshold=14.238655090332031
  SimulatedQuantizeParams(in_scale=7.19698e-09, out_scale=0.111239492893219, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%248] -> nn.conv2d[%249]
  bit=8, threshold=0.6394439339637756
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004995655734091997, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%249] -> add[%251]
  bit=32, threshold=2.3433687686920166
  SimulatedQuantizeParams(in_scale=0.0005557142, out_scale=1.0033215147586816e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%250] -> add[%251]
  bit=32, threshold=2.3433687686920166
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0033215147586816e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%251] -> nn.relu[%252]
  bit=32, threshold=20.109031677246094
  SimulatedQuantizeParams(in_scale=1.0033215e-08, out_scale=1.0033215147586816e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%252] -> nn.conv2d[%254]
  bit=8, threshold=20.109031677246094
  SimulatedQuantizeParams(in_scale=1.0033215e-08, out_scale=0.1571018099784851, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%253] -> nn.conv2d[%254]
  bit=8, threshold=0.6675122380256653
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00521493935957551, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%254] -> add[%256]
  bit=32, threshold=3.320685386657715
  SimulatedQuantizeParams(in_scale=0.00081927644, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%255] -> add[%256]
  bit=32, threshold=3.320685386657715
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%256] -> nn.relu[%257]
  bit=32, threshold=22.33852767944336
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%212] -> nn.avg_pool2d[%258]
  not quantized
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%258] -> nn.conv2d[%260]
  bit=8, threshold=5.780173301696777
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.04515760391950607, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%259] -> nn.conv2d[%260]
  bit=8, threshold=0.9621531963348389
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007516821846365929, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%260] -> add[%262]
  bit=32, threshold=2.7727677822113037
  SimulatedQuantizeParams(in_scale=0.00033944167, out_scale=3.8256802170621995e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%261] -> add[%262]
  bit=32, threshold=2.7727677822113037
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.8256802170621995e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%262] -> nn.relu[%263]
  bit=32, threshold=8.354458808898926
  SimulatedQuantizeParams(in_scale=3.82568e-09, out_scale=3.8256802170621995e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%217] -> concatenate[%264]
  bit=32, threshold=6.464248180389404
  SimulatedQuantizeParams(in_scale=5.710169e-09, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%232] -> concatenate[%264]
  bit=32, threshold=6.464248180389404
  SimulatedQuantizeParams(in_scale=4.53216e-09, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%257] -> concatenate[%264]
  bit=32, threshold=6.464248180389404
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%263] -> concatenate[%264]
  bit=32, threshold=6.464248180389404
  SimulatedQuantizeParams(in_scale=3.82568e-09, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%264] -> nn.conv2d[%266]
  bit=8, threshold=22.33852767944336
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=0.17451974749565125, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%265] -> nn.conv2d[%266]
  bit=8, threshold=1.1758604049682617
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.009186409413814545, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%266] -> add[%268]
  bit=32, threshold=1.8875170946121216
  SimulatedQuantizeParams(in_scale=0.0016032099, out_scale=6.290805210085182e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%267] -> add[%268]
  bit=32, threshold=1.8875170946121216
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.290805210085182e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%268] -> nn.relu[%269]
  bit=32, threshold=13.82950210571289
  SimulatedQuantizeParams(in_scale=6.290805e-09, out_scale=6.290805210085182e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%264] -> nn.conv2d[%271]
  bit=8, threshold=22.33852767944336
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=0.17451974749565125, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%270] -> nn.conv2d[%271]
  bit=8, threshold=0.5845543742179871
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004566831048578024, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%271] -> add[%273]
  bit=32, threshold=2.8225395679473877
  SimulatedQuantizeParams(in_scale=0.0007970022, out_scale=6.896744064732729e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%272] -> add[%273]
  bit=32, threshold=2.8225395679473877
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.896744064732729e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%273] -> nn.relu[%274]
  bit=32, threshold=12.803604125976562
  SimulatedQuantizeParams(in_scale=6.896744e-09, out_scale=6.896744064732729e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%274] -> nn.conv2d[%276]
  bit=8, threshold=12.803604125976562
  SimulatedQuantizeParams(in_scale=6.896744e-09, out_scale=0.1000281572341919, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%275] -> nn.conv2d[%276]
  bit=8, threshold=1.127054214477539
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008805111050605774, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%276] -> add[%278]
  bit=32, threshold=1.5113341808319092
  SimulatedQuantizeParams(in_scale=0.00088075903, out_scale=7.318159411795477e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%277] -> add[%278]
  bit=32, threshold=1.5113341808319092
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.318159411795477e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%278] -> nn.relu[%279]
  bit=32, threshold=15.453755378723145
  SimulatedQuantizeParams(in_scale=7.3181594e-09, out_scale=7.318159411795477e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%279] -> nn.conv2d[%281]
  bit=8, threshold=15.453755378723145
  SimulatedQuantizeParams(in_scale=7.3181594e-09, out_scale=0.12073246389627457, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%280] -> nn.conv2d[%281]
  bit=8, threshold=0.6649886965751648
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005195224191993475, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%281] -> add[%283]
  bit=32, threshold=2.850898504257202
  SimulatedQuantizeParams(in_scale=0.0006272322, out_scale=6.16979045631183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%282] -> add[%283]
  bit=32, threshold=2.850898504257202
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.16979045631183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%283] -> nn.relu[%284]
  bit=32, threshold=13.102632522583008
  SimulatedQuantizeParams(in_scale=6.1697905e-09, out_scale=6.16979045631183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%264] -> nn.conv2d[%286]
  bit=8, threshold=22.33852767944336
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=0.17451974749565125, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%285] -> nn.conv2d[%286]
  bit=8, threshold=0.8486541509628296
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006630110554397106, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%286] -> add[%288]
  bit=32, threshold=2.2719626426696777
  SimulatedQuantizeParams(in_scale=0.0011570852, out_scale=7.916712618794008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%287] -> add[%288]
  bit=32, threshold=2.2719626426696777
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.916712618794008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%288] -> nn.relu[%289]
  bit=32, threshold=15.889891624450684
  SimulatedQuantizeParams(in_scale=7.916713e-09, out_scale=7.916712618794008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%289] -> nn.conv2d[%291]
  bit=8, threshold=15.889891624450684
  SimulatedQuantizeParams(in_scale=7.916713e-09, out_scale=0.12413977831602097, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%290] -> nn.conv2d[%291]
  bit=8, threshold=0.9434528350830078
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0073707252740859985, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%291] -> add[%293]
  bit=32, threshold=1.8475465774536133
  SimulatedQuantizeParams(in_scale=0.0009150002, out_scale=7.64214647119843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%292] -> add[%293]
  bit=32, threshold=1.8475465774536133
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.64214647119843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%293] -> nn.relu[%294]
  bit=32, threshold=16.72710609436035
  SimulatedQuantizeParams(in_scale=7.6421465e-09, out_scale=7.64214647119843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%294] -> nn.conv2d[%296]
  bit=8, threshold=16.72710609436035
  SimulatedQuantizeParams(in_scale=7.6421465e-09, out_scale=0.13068051636219025, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%295] -> nn.conv2d[%296]
  bit=8, threshold=0.6284889578819275
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0049100699834525585, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%296] -> add[%298]
  bit=32, threshold=2.077789545059204
  SimulatedQuantizeParams(in_scale=0.00064165046, out_scale=5.389896529095495e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%297] -> add[%298]
  bit=32, threshold=2.077789545059204
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.389896529095495e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%298] -> nn.relu[%299]
  bit=32, threshold=11.313255310058594
  SimulatedQuantizeParams(in_scale=5.3898965e-09, out_scale=5.389896529095495e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%299] -> nn.conv2d[%301]
  bit=8, threshold=11.313255310058594
  SimulatedQuantizeParams(in_scale=5.3898965e-09, out_scale=0.08838480710983276, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%300] -> nn.conv2d[%301]
  bit=8, threshold=0.9571489095687866
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0074777258560061455, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%301] -> add[%303]
  bit=32, threshold=1.308233618736267
  SimulatedQuantizeParams(in_scale=0.0006609174, out_scale=7.242821009612044e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%302] -> add[%303]
  bit=32, threshold=1.308233618736267
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.242821009612044e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%303] -> nn.relu[%304]
  bit=32, threshold=16.044204711914062
  SimulatedQuantizeParams(in_scale=7.242821e-09, out_scale=7.242821009612044e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%304] -> nn.conv2d[%306]
  bit=8, threshold=11.781047821044922
  SimulatedQuantizeParams(in_scale=7.242821e-09, out_scale=0.09203943610191345, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%305] -> nn.conv2d[%306]
  bit=8, threshold=0.9496046900749207
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007418786641210318, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%306] -> add[%308]
  bit=32, threshold=2.860248327255249
  SimulatedQuantizeParams(in_scale=0.0006828209, out_scale=5.331318497781012e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%307] -> add[%308]
  bit=32, threshold=2.860248327255249
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.331318497781012e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%308] -> nn.relu[%309]
  bit=32, threshold=10.766971588134766
  SimulatedQuantizeParams(in_scale=5.3313185e-09, out_scale=5.331318497781012e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%264] -> nn.avg_pool2d[%310]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%310] -> nn.conv2d[%312]
  bit=8, threshold=6.167629241943359
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.048184603452682495, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%311] -> nn.conv2d[%312]
  bit=8, threshold=1.790992259979248
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.013992127031087875, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%312] -> add[%314]
  bit=32, threshold=1.1869275569915771
  SimulatedQuantizeParams(in_scale=0.0006742051, out_scale=3.861404529459378e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%313] -> add[%314]
  bit=32, threshold=1.1869275569915771
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.861404529459378e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%314] -> nn.relu[%315]
  bit=32, threshold=7.954532623291016
  SimulatedQuantizeParams(in_scale=3.8614045e-09, out_scale=3.861404529459378e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%269] -> concatenate[%316]
  bit=32, threshold=5.845179557800293
  SimulatedQuantizeParams(in_scale=6.290805e-09, out_scale=6.4398637533713554e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%284] -> concatenate[%316]
  bit=32, threshold=5.845179557800293
  SimulatedQuantizeParams(in_scale=6.1697905e-09, out_scale=6.4398637533713554e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%309] -> concatenate[%316]
  bit=32, threshold=5.845179557800293
  SimulatedQuantizeParams(in_scale=5.3313185e-09, out_scale=6.4398637533713554e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%315] -> concatenate[%316]
  bit=32, threshold=5.845179557800293
  SimulatedQuantizeParams(in_scale=3.8614045e-09, out_scale=6.4398637533713554e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%316] -> nn.conv2d[%318]
  bit=8, threshold=13.93353271484375
  SimulatedQuantizeParams(in_scale=6.4398638e-09, out_scale=0.1088557243347168, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%317] -> nn.conv2d[%318]
  bit=8, threshold=0.986774742603302
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007709177676588297, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%318] -> add[%320]
  bit=32, threshold=0.8015092611312866
  SimulatedQuantizeParams(in_scale=0.0008391881, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%319] -> add[%320]
  bit=32, threshold=0.8015092611312866
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%320] -> nn.relu[%321]
  bit=32, threshold=15.364890098571777
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%316] -> nn.conv2d[%323]
  bit=8, threshold=13.93353271484375
  SimulatedQuantizeParams(in_scale=6.4398638e-09, out_scale=0.1088557243347168, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%322] -> nn.conv2d[%323]
  bit=8, threshold=0.9906911849975586
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0077397748827934265, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%323] -> add[%325]
  bit=32, threshold=1.167878270149231
  SimulatedQuantizeParams(in_scale=0.0008425188, out_scale=5.593841834183877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%324] -> add[%325]
  bit=32, threshold=1.167878270149231
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.593841834183877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%325] -> nn.relu[%326]
  bit=32, threshold=11.638570785522461
  SimulatedQuantizeParams(in_scale=5.593842e-09, out_scale=5.593841834183877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%326] -> nn.conv2d[%328]
  bit=8, threshold=11.638570785522461
  SimulatedQuantizeParams(in_scale=5.593842e-09, out_scale=0.09092633426189423, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%327] -> nn.conv2d[%328]
  bit=8, threshold=1.0864348411560059
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008487772196531296, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%328] -> add[%330]
  bit=32, threshold=1.434431552886963
  SimulatedQuantizeParams(in_scale=0.000771762, out_scale=6.458874324266617e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%329] -> add[%330]
  bit=32, threshold=1.434431552886963
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.458874324266617e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%330] -> nn.relu[%331]
  bit=32, threshold=14.039323806762695
  SimulatedQuantizeParams(in_scale=6.4588743e-09, out_scale=6.458874324266617e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%331] -> nn.conv2d[%333]
  bit=8, threshold=14.039323806762695
  SimulatedQuantizeParams(in_scale=6.4588743e-09, out_scale=0.10968221724033356, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%332] -> nn.conv2d[%333]
  bit=8, threshold=0.3395962715148926
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0026530958712100983, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%333] -> add[%335]
  bit=32, threshold=2.9625234603881836
  SimulatedQuantizeParams(in_scale=0.00029099744, out_scale=4.602799918984601e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%334] -> add[%335]
  bit=32, threshold=2.9625234603881836
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.602799918984601e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%335] -> nn.relu[%336]
  bit=32, threshold=10.09786319732666
  SimulatedQuantizeParams(in_scale=4.6028e-09, out_scale=4.602799918984601e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%316] -> nn.conv2d[%338]
  bit=8, threshold=13.93353271484375
  SimulatedQuantizeParams(in_scale=6.4398638e-09, out_scale=0.1088557243347168, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%337] -> nn.conv2d[%338]
  bit=8, threshold=0.6093745827674866
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004760738927870989, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%338] -> add[%340]
  bit=32, threshold=2.3437814712524414
  SimulatedQuantizeParams(in_scale=0.0005182337, out_scale=6.269122554414253e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%339] -> add[%340]
  bit=32, threshold=2.3437814712524414
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.269122554414253e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%340] -> nn.relu[%341]
  bit=32, threshold=12.59747314453125
  SimulatedQuantizeParams(in_scale=6.2691226e-09, out_scale=6.269122554414253e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%341] -> nn.conv2d[%343]
  bit=8, threshold=12.59747314453125
  SimulatedQuantizeParams(in_scale=6.2691226e-09, out_scale=0.09841775894165039, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%342] -> nn.conv2d[%343]
  bit=8, threshold=0.5574187636375427
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0043548340909183025, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%343] -> add[%345]
  bit=32, threshold=1.5553131103515625
  SimulatedQuantizeParams(in_scale=0.00042859302, out_scale=5.721004114889183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%344] -> add[%345]
  bit=32, threshold=1.5553131103515625
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.721004114889183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%345] -> nn.relu[%346]
  bit=32, threshold=12.173563957214355
  SimulatedQuantizeParams(in_scale=5.721004e-09, out_scale=5.721004114889183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%346] -> nn.conv2d[%348]
  bit=8, threshold=12.173563957214355
  SimulatedQuantizeParams(in_scale=5.721004e-09, out_scale=0.09510596841573715, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%347] -> nn.conv2d[%348]
  bit=8, threshold=0.6530459523200989
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0051019215025007725, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%348] -> add[%350]
  bit=32, threshold=1.9077832698822021
  SimulatedQuantizeParams(in_scale=0.0004852232, out_scale=6.055622669975946e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%349] -> add[%350]
  bit=32, threshold=1.9077832698822021
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.055622669975946e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%350] -> nn.relu[%351]
  bit=32, threshold=13.047298431396484
  SimulatedQuantizeParams(in_scale=6.0556227e-09, out_scale=6.055622669975946e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%351] -> nn.conv2d[%353]
  bit=8, threshold=13.047298431396484
  SimulatedQuantizeParams(in_scale=6.0556227e-09, out_scale=0.10193201899528503, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%352] -> nn.conv2d[%353]
  bit=8, threshold=0.4091434180736542
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0031964329537004232, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%353] -> add[%355]
  bit=32, threshold=2.121026039123535
  SimulatedQuantizeParams(in_scale=0.00032581887, out_scale=6.2256906296909165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%354] -> add[%355]
  bit=32, threshold=2.121026039123535
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.2256906296909165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%355] -> nn.relu[%356]
  bit=32, threshold=13.940378189086914
  SimulatedQuantizeParams(in_scale=6.2256906e-09, out_scale=6.2256906296909165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%356] -> nn.conv2d[%358]
  bit=8, threshold=13.940378189086914
  SimulatedQuantizeParams(in_scale=6.2256906e-09, out_scale=0.10890920460224152, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%357] -> nn.conv2d[%358]
  bit=8, threshold=0.3439410924911499
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0026870397850871086, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%358] -> add[%360]
  bit=32, threshold=1.6823570728302002
  SimulatedQuantizeParams(in_scale=0.00029264335, out_scale=4.0570169446141335e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%359] -> add[%360]
  bit=32, threshold=1.6823570728302002
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.0570169446141335e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%360] -> nn.relu[%361]
  bit=32, threshold=8.996960639953613
  SimulatedQuantizeParams(in_scale=4.057017e-09, out_scale=4.0570169446141335e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%316] -> nn.avg_pool2d[%362]
  not quantized
  SimulatedQuantizeParams(in_scale=6.4398638e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%362] -> nn.conv2d[%364]
  bit=8, threshold=6.527836799621582
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.05099872499704361, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%363] -> nn.conv2d[%364]
  bit=8, threshold=0.9189664721488953
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007179425563663244, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%364] -> add[%366]
  bit=32, threshold=0.9521695375442505
  SimulatedQuantizeParams(in_scale=0.00036614155, out_scale=3.3753286832194362e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%365] -> add[%366]
  bit=32, threshold=0.9521695375442505
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.3753286832194362e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%366] -> nn.relu[%367]
  bit=32, threshold=7.16987419128418
  SimulatedQuantizeParams(in_scale=3.3753287e-09, out_scale=3.3753286832194362e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%321] -> concatenate[%368]
  bit=32, threshold=4.94903039932251
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%336] -> concatenate[%368]
  bit=32, threshold=4.94903039932251
  SimulatedQuantizeParams(in_scale=4.6028e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%361] -> concatenate[%368]
  bit=32, threshold=4.94903039932251
  SimulatedQuantizeParams(in_scale=4.057017e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%367] -> concatenate[%368]
  bit=32, threshold=4.94903039932251
  SimulatedQuantizeParams(in_scale=3.3753287e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%368] -> nn.conv2d[%370]
  bit=8, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=0.06958131492137909, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%369] -> nn.conv2d[%370]
  bit=8, threshold=0.8908913135528564
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006960088387131691, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%370] -> add[%372]
  bit=32, threshold=2.821382999420166
  SimulatedQuantizeParams(in_scale=0.0004842921, out_scale=6.749371284087147e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%371] -> add[%372]
  bit=32, threshold=2.821382999420166
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.749371284087147e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%372] -> nn.relu[%373]
  bit=32, threshold=14.85698127746582
  SimulatedQuantizeParams(in_scale=6.7493713e-09, out_scale=6.749371284087147e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%373] -> nn.conv2d[%375]
  bit=8, threshold=11.869260787963867
  SimulatedQuantizeParams(in_scale=6.7493713e-09, out_scale=0.09272859990596771, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%374] -> nn.conv2d[%375]
  bit=8, threshold=0.6321445107460022
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004938628990203142, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%375] -> add[%377]
  bit=32, threshold=3.4323699474334717
  SimulatedQuantizeParams(in_scale=0.00045795215, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%376] -> add[%377]
  bit=32, threshold=3.4323699474334717
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%377] -> nn.relu[%378]
  bit=32, threshold=16.574100494384766
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%368] -> nn.conv2d[%380]
  bit=8, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=0.06958131492137909, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%379] -> nn.conv2d[%380]
  bit=8, threshold=1.2494585514068604
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.009761394932866096, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%380] -> add[%382]
  bit=32, threshold=2.9126672744750977
  SimulatedQuantizeParams(in_scale=0.0006792107, out_scale=6.758319237576416e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%381] -> add[%382]
  bit=32, threshold=2.9126672744750977
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.758319237576416e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%382] -> nn.relu[%383]
  bit=32, threshold=14.695056915283203
  SimulatedQuantizeParams(in_scale=6.7583192e-09, out_scale=6.758319237576416e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%383] -> nn.conv2d[%385]
  bit=8, threshold=12.045427322387695
  SimulatedQuantizeParams(in_scale=6.7583192e-09, out_scale=0.09410490095615387, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%384] -> nn.conv2d[%385]
  bit=8, threshold=0.49153587222099304
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003840124001726508, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%385] -> add[%387]
  bit=32, threshold=2.605875015258789
  SimulatedQuantizeParams(in_scale=0.0003613745, out_scale=6.278922715097224e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%386] -> add[%387]
  bit=32, threshold=2.605875015258789
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.278922715097224e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%387] -> nn.relu[%388]
  bit=32, threshold=13.627424240112305
  SimulatedQuantizeParams(in_scale=6.2789227e-09, out_scale=6.278922715097224e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%388] -> nn.conv2d[%390]
  bit=8, threshold=13.627424240112305
  SimulatedQuantizeParams(in_scale=6.2789227e-09, out_scale=0.10646425187587738, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%389] -> nn.conv2d[%390]
  bit=8, threshold=0.4021762013435364
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003142001572996378, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%390] -> add[%392]
  bit=32, threshold=3.6154825687408447
  SimulatedQuantizeParams(in_scale=0.00033451084, out_scale=4.7092596489051175e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%391] -> add[%392]
  bit=32, threshold=3.6154825687408447
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.7092596489051175e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%392] -> nn.relu[%393]
  bit=32, threshold=9.992453575134277
  SimulatedQuantizeParams(in_scale=4.7092596e-09, out_scale=4.7092596489051175e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%393] -> nn.conv2d[%395]
  bit=8, threshold=9.711898803710938
  SimulatedQuantizeParams(in_scale=4.7092596e-09, out_scale=0.0758742094039917, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%394] -> nn.conv2d[%395]
  bit=8, threshold=0.9571426510810852
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007477676961570978, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%395] -> add[%397]
  bit=32, threshold=2.029176950454712
  SimulatedQuantizeParams(in_scale=0.0005673628, out_scale=5.0616910662881764e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%396] -> add[%397]
  bit=32, threshold=2.029176950454712
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.0616910662881764e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%397] -> nn.relu[%398]
  bit=32, threshold=10.032608032226562
  SimulatedQuantizeParams(in_scale=5.061691e-09, out_scale=5.0616910662881764e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%368] -> nn.max_pool2d[%399]
  bit=32, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=4.147369558893388e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%378] -> concatenate[%400]
  bit=32, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%398] -> concatenate[%400]
  bit=32, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=5.061691e-09, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%399] -> concatenate[%400]
  bit=32, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=4.1473696e-09, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%400] -> nn.conv2d[%402]
  bit=8, threshold=16.574100494384766
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=0.12948516011238098, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%401] -> nn.conv2d[%402]
  bit=8, threshold=0.9461250901222229
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007391602266579866, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%402] -> add[%404]
  bit=32, threshold=1.7416651248931885
  SimulatedQuantizeParams(in_scale=0.0009571028, out_scale=4.635082095916232e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%403] -> add[%404]
  bit=32, threshold=1.7416651248931885
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.635082095916232e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%404] -> nn.relu[%405]
  bit=32, threshold=10.777520179748535
  SimulatedQuantizeParams(in_scale=4.635082e-09, out_scale=4.635082095916232e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%400] -> nn.conv2d[%407]
  bit=8, threshold=16.574100494384766
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=0.12948516011238098, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%406] -> nn.conv2d[%407]
  bit=8, threshold=0.7819865345954895
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006109269801527262, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%407] -> add[%409]
  bit=32, threshold=1.3491286039352417
  SimulatedQuantizeParams(in_scale=0.0007910598, out_scale=4.726279367872621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%408] -> add[%409]
  bit=32, threshold=1.3491286039352417
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.726279367872621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%409] -> nn.relu[%410]
  bit=32, threshold=9.828022003173828
  SimulatedQuantizeParams(in_scale=4.7262794e-09, out_scale=4.726279367872621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%410] -> nn.conv2d[%412]
  bit=8, threshold=9.828022003173828
  SimulatedQuantizeParams(in_scale=4.7262794e-09, out_scale=0.07678142189979553, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%411] -> nn.conv2d[%412]
  bit=8, threshold=0.6497980952262878
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005076547618955374, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%412] -> add[%414]
  bit=32, threshold=1.2121844291687012
  SimulatedQuantizeParams(in_scale=0.00038978455, out_scale=4.80139394909429e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%413] -> add[%414]
  bit=32, threshold=1.2121844291687012
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.80139394909429e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%414] -> nn.relu[%415]
  bit=32, threshold=10.601075172424316
  SimulatedQuantizeParams(in_scale=4.801394e-09, out_scale=4.80139394909429e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%410] -> nn.conv2d[%417]
  bit=8, threshold=9.828022003173828
  SimulatedQuantizeParams(in_scale=4.7262794e-09, out_scale=0.07678142189979553, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%416] -> nn.conv2d[%417]
  bit=8, threshold=1.0432422161102295
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008150329813361168, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%417] -> add[%419]
  bit=32, threshold=0.8334571123123169
  SimulatedQuantizeParams(in_scale=0.00062579394, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%418] -> add[%419]
  bit=32, threshold=0.8334571123123169
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%419] -> nn.relu[%420]
  bit=32, threshold=18.286447525024414
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%415] -> concatenate[%421]
  bit=32, threshold=18.286447525024414
  SimulatedQuantizeParams(in_scale=4.801394e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%420] -> concatenate[%421]
  bit=32, threshold=18.286447525024414
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%400] -> nn.conv2d[%423]
  bit=8, threshold=16.574100494384766
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=0.12948516011238098, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%422] -> nn.conv2d[%423]
  bit=8, threshold=0.7320553660392761
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005719182547181845, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%423] -> add[%425]
  bit=32, threshold=3.173121452331543
  SimulatedQuantizeParams(in_scale=0.0007405493, out_scale=4.2537506850237605e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%424] -> add[%425]
  bit=32, threshold=3.173121452331543
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.2537506850237605e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%425] -> nn.relu[%426]
  bit=32, threshold=9.873434066772461
  SimulatedQuantizeParams(in_scale=4.2537507e-09, out_scale=4.2537506850237605e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%426] -> nn.conv2d[%428]
  bit=8, threshold=7.434506416320801
  SimulatedQuantizeParams(in_scale=4.2537507e-09, out_scale=0.058082081377506256, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%427] -> nn.conv2d[%428]
  bit=8, threshold=0.32545602321624756
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.002542625181376934, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%428] -> add[%430]
  bit=32, threshold=2.3684961795806885
  SimulatedQuantizeParams(in_scale=0.00014768096, out_scale=3.887672406222009e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%429] -> add[%430]
  bit=32, threshold=2.3684961795806885
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.887672406222009e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%430] -> nn.relu[%431]
  bit=32, threshold=8.034111022949219
  SimulatedQuantizeParams(in_scale=3.8876724e-09, out_scale=3.887672406222009e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%431] -> nn.conv2d[%433]
  bit=8, threshold=6.94843864440918
  SimulatedQuantizeParams(in_scale=3.8876724e-09, out_scale=0.054284676909446716, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%432] -> nn.conv2d[%433]
  bit=8, threshold=0.6720498204231262
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005250389222055674, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%433] -> add[%435]
  bit=32, threshold=3.2491979598999023
  SimulatedQuantizeParams(in_scale=0.00028501567, out_scale=4.258463359718689e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%434] -> add[%435]
  bit=32, threshold=3.2491979598999023
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.258463359718689e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%435] -> nn.relu[%436]
  bit=32, threshold=9.11160659790039
  SimulatedQuantizeParams(in_scale=4.2584634e-09, out_scale=4.258463359718689e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%431] -> nn.conv2d[%438]
  bit=8, threshold=6.94843864440918
  SimulatedQuantizeParams(in_scale=3.8876724e-09, out_scale=0.054284676909446716, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%437] -> nn.conv2d[%438]
  bit=8, threshold=0.4612494707107544
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0036035114899277687, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%438] -> add[%440]
  bit=32, threshold=3.5127429962158203
  SimulatedQuantizeParams(in_scale=0.00019561546, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%439] -> add[%440]
  bit=32, threshold=3.5127429962158203
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%440] -> nn.relu[%441]
  bit=32, threshold=8.9388427734375
  SimulatedQuantizeParams(in_scale=4.4178647e-09, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%436] -> concatenate[%442]
  bit=32, threshold=6.666423797607422
  SimulatedQuantizeParams(in_scale=4.2584634e-09, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%441] -> concatenate[%442]
  bit=32, threshold=6.666423797607422
  SimulatedQuantizeParams(in_scale=4.4178647e-09, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%400] -> nn.avg_pool2d[%443]
  not quantized
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%443] -> nn.conv2d[%445]
  bit=8, threshold=8.041417121887207
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0628235712647438, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%444] -> nn.conv2d[%445]
  bit=8, threshold=1.1557105779647827
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.009028988890349865, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%445] -> add[%447]
  bit=32, threshold=1.0772409439086914
  SimulatedQuantizeParams(in_scale=0.0005672333, out_scale=3.7089007420831877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%446] -> add[%447]
  bit=32, threshold=1.0772409439086914
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.7089007420831877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%447] -> nn.relu[%448]
  bit=32, threshold=8.030485153198242
  SimulatedQuantizeParams(in_scale=3.7089007e-09, out_scale=3.7089007420831877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%405] -> concatenate[%449]
  bit=32, threshold=3.7890758514404297
  SimulatedQuantizeParams(in_scale=4.635082e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%421] -> concatenate[%449]
  bit=32, threshold=3.7890758514404297
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%442] -> concatenate[%449]
  bit=32, threshold=3.7890758514404297
  SimulatedQuantizeParams(in_scale=4.4178647e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%448] -> concatenate[%449]
  bit=32, threshold=3.7890758514404297
  SimulatedQuantizeParams(in_scale=3.7089007e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%449] -> nn.conv2d[%451]
  bit=8, threshold=18.803203582763672
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=0.1469000279903412, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%450] -> nn.conv2d[%451]
  bit=8, threshold=2.4350264072418213
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.01902364380657673, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%451] -> add[%453]
  bit=32, threshold=0.35487693548202515
  SimulatedQuantizeParams(in_scale=0.0027945738, out_scale=6.016696030286539e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%452] -> add[%453]
  bit=32, threshold=0.35487693548202515
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.016696030286539e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%453] -> nn.relu[%454]
  bit=32, threshold=12.86056137084961
  SimulatedQuantizeParams(in_scale=6.016696e-09, out_scale=6.016696030286539e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%449] -> nn.conv2d[%456]
  bit=8, threshold=18.803203582763672
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=0.1469000279903412, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%455] -> nn.conv2d[%456]
  bit=8, threshold=1.086700439453125
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008489847183227539, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%456] -> add[%458]
  bit=32, threshold=1.4524096250534058
  SimulatedQuantizeParams(in_scale=0.0012471587, out_scale=7.307435545556018e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%457] -> add[%458]
  bit=32, threshold=1.4524096250534058
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.307435545556018e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%458] -> nn.relu[%459]
  bit=32, threshold=15.603008270263672
  SimulatedQuantizeParams(in_scale=7.3074355e-09, out_scale=7.307435545556018e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%459] -> nn.conv2d[%461]
  bit=8, threshold=15.603008270263672
  SimulatedQuantizeParams(in_scale=7.3074355e-09, out_scale=0.12189850211143494, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%460] -> nn.conv2d[%461]
  bit=8, threshold=1.9439151287078857
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.015186836943030357, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%461] -> add[%463]
  bit=32, threshold=0.5548152923583984
  SimulatedQuantizeParams(in_scale=0.0018512526, out_scale=1.6128288038430583e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%462] -> add[%463]
  bit=32, threshold=0.5548152923583984
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6128288038430583e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%463] -> nn.relu[%464]
  bit=32, threshold=34.27431106567383
  SimulatedQuantizeParams(in_scale=1.6128288e-08, out_scale=1.6128288038430583e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%459] -> nn.conv2d[%466]
  bit=8, threshold=15.603008270263672
  SimulatedQuantizeParams(in_scale=7.3074355e-09, out_scale=0.12189850211143494, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%465] -> nn.conv2d[%466]
  bit=8, threshold=2.057163953781128
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.016071593388915062, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%466] -> add[%468]
  bit=32, threshold=0.5418263673782349
  SimulatedQuantizeParams(in_scale=0.0019591032, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%467] -> add[%468]
  bit=32, threshold=0.5418263673782349
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%468] -> nn.relu[%469]
  bit=32, threshold=34.57212448120117
  SimulatedQuantizeParams(in_scale=1.626129e-08, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%464] -> concatenate[%470]
  bit=32, threshold=34.57212448120117
  SimulatedQuantizeParams(in_scale=1.6128288e-08, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%469] -> concatenate[%470]
  bit=32, threshold=34.57212448120117
  SimulatedQuantizeParams(in_scale=1.626129e-08, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%449] -> nn.conv2d[%472]
  bit=8, threshold=18.803203582763672
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=0.1469000279903412, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%471] -> nn.conv2d[%472]
  bit=8, threshold=0.8023834228515625
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006268620491027832, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%472] -> add[%474]
  bit=32, threshold=1.9777050018310547
  SimulatedQuantizeParams(in_scale=0.0009208605, out_scale=5.8777760436612425e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%473] -> add[%474]
  bit=32, threshold=1.9777050018310547
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.8777760436612425e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%474] -> nn.relu[%475]
  bit=32, threshold=12.829500198364258
  SimulatedQuantizeParams(in_scale=5.877776e-09, out_scale=5.8777760436612425e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%475] -> nn.conv2d[%477]
  bit=8, threshold=10.481781005859375
  SimulatedQuantizeParams(in_scale=5.877776e-09, out_scale=0.08188891410827637, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%476] -> nn.conv2d[%477]
  bit=8, threshold=0.6431017518043518
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0050242324359714985, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%477] -> add[%479]
  bit=32, threshold=2.903168201446533
  SimulatedQuantizeParams(in_scale=0.00041142895, out_scale=8.565093523316136e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%478] -> add[%479]
  bit=32, threshold=2.903168201446533
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.565093523316136e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%479] -> nn.relu[%480]
  bit=32, threshold=18.597047805786133
  SimulatedQuantizeParams(in_scale=8.5650935e-09, out_scale=8.565093523316136e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%480] -> nn.conv2d[%482]
  bit=8, threshold=18.597047805786133
  SimulatedQuantizeParams(in_scale=8.5650935e-09, out_scale=0.14528943598270416, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%481] -> nn.conv2d[%482]
  bit=8, threshold=0.9153760075569153
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007151375059038401, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%482] -> add[%484]
  bit=32, threshold=0.6826770901679993
  SimulatedQuantizeParams(in_scale=0.0010390192, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%483] -> add[%484]
  bit=32, threshold=0.6826770901679993
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%484] -> nn.relu[%485]
  bit=32, threshold=16.534765243530273
  SimulatedQuantizeParams(in_scale=7.844385e-09, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%480] -> nn.conv2d[%487]
  bit=8, threshold=18.597047805786133
  SimulatedQuantizeParams(in_scale=8.5650935e-09, out_scale=0.14528943598270416, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%486] -> nn.conv2d[%487]
  bit=8, threshold=0.939830482006073
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007342425640672445, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%487] -> add[%489]
  bit=32, threshold=0.5877044200897217
  SimulatedQuantizeParams(in_scale=0.0010667769, out_scale=6.817605591180609e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%488] -> add[%489]
  bit=32, threshold=0.5877044200897217
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.817605591180609e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%489] -> nn.relu[%490]
  bit=32, threshold=14.328747749328613
  SimulatedQuantizeParams(in_scale=6.8176056e-09, out_scale=6.817605591180609e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%485] -> concatenate[%491]
  bit=32, threshold=14.328747749328613
  SimulatedQuantizeParams(in_scale=7.844385e-09, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%490] -> concatenate[%491]
  bit=32, threshold=14.328747749328613
  SimulatedQuantizeParams(in_scale=6.8176056e-09, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%449] -> nn.avg_pool2d[%492]
  not quantized
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%492] -> nn.conv2d[%494]
  bit=8, threshold=9.986809730529785
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.07802195101976395, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%493] -> nn.conv2d[%494]
  bit=8, threshold=2.859543561935425
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.022340184077620506, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%494] -> add[%496]
  bit=32, threshold=0.7978425025939941
  SimulatedQuantizeParams(in_scale=0.0017430248, out_scale=4.033631650912639e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%495] -> add[%496]
  bit=32, threshold=0.7978425025939941
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.033631650912639e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%496] -> nn.relu[%497]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=4.0336317e-09, out_scale=4.033631650912639e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%454] -> concatenate[%498]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=6.016696e-09, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%470] -> concatenate[%498]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=1.626129e-08, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%491] -> concatenate[%498]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=7.844385e-09, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%497] -> concatenate[%498]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=4.0336317e-09, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%498] -> nn.avg_pool2d[%499]
  not quantized
  SimulatedQuantizeParams(in_scale=1.626129e-08, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%499] -> nn.batch_flatten[%500]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.batch_flatten[%500] -> nn.dense[%502]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%501] -> nn.dense[%502]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.dense[%502] -> add[%504]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%503] -> add[%504]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
add[%504] -> OUT
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
INFO:compile_engine:Use implementation simulated_quantize.cuda for op nn.simulated_quantize
measures
Measure(version=0.1, strategy=Strategy(model_hash=-5004976668240111371, bits=[8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32], thresholds=[2.632871389389038, 2.3602352142333984, 12.065823554992676, 4.038895130157471, 13.020015716552734, 13.020015716552734, 1.9623639583587646, 16.575393676757812, 4.98871374130249, 16.22219467163086, 16.22219467163086, 1.2790210247039795, 18.2012996673584, 4.041762828826904, 15.711963653564453, 13.954992294311523, 13.954992294311523, 1.3126699924468994, 16.296649932861328, 7.094275951385498, 15.00935173034668, 15.00935173034668, 0.7347288131713867, 12.65969467163086, 9.251598358154297, 12.553302764892578, 12.553302764892578, 12.553302764892578, 0.5553937554359436, 10.610788345336914, 6.458847999572754, 10.835762977600098, 10.835762977600098, 0.5140682458877563, 10.755268096923828, 3.0780153274536133, 9.361836433410645, 9.361836433410645, 0.6689160466194153, 9.371060371398926, 4.61334753036499, 9.97728157043457, 9.97728157043457, 0.9048811793327332, 13.447190284729004, 7.991888999938965, 10.989872932434082, 10.989872932434082, 0.7721172571182251, 12.10438060760498, 6.1439104080200195, 13.117056846618652, 13.117056846618652, 0.7478156685829163, 19.330097198486328, 3.0267882347106934, 18.47281837463379, 18.47281837463379, 7.913825988769531, 1.4154207706451416, 11.809602737426758, 6.655673503875732, 7.329173564910889, 7.051487922668457, 18.47281837463379, 0.7486107349395752, 14.490365028381348, 4.264531135559082, 14.008376121520996, 14.008376121520996, 0.5955356955528259, 14.672113418579102, 6.930329322814941, 13.359471321105957, 13.359471321105957, 0.5041235089302063, 11.800966262817383, 4.1340012550354, 10.409193992614746, 10.409193992614746, 0.6865484714508057, 12.078744888305664, 5.231709957122803, 8.374465942382812, 8.374465942382812, 0.7145971059799194, 10.136137962341309, 4.646483421325684, 9.44502067565918, 9.44502067565918, 1.188536524772644, 17.309581756591797, 2.289642810821533, 16.920352935791016, 16.920352935791016, 11.094852447509766, 1.1952425241470337, 15.430564880371094, 4.416873931884766, 13.282967567443848, 10.640891075134277, 16.920352935791016, 1.0737723112106323, 16.343807220458984, 4.300724029541016, 13.963201522827148, 10.449176788330078, 0.6418415904045105, 11.628702163696289, 2.9343960285186768, 10.764244079589844, 10.764244079589844, 0.3671499788761139, 9.588830947875977, 2.7083351612091064, 9.77314281463623, 9.342620849609375, 0.8906435966491699, 14.05899429321289, 5.0458526611328125, 12.8213472366333, 12.8213472366333, 0.6013656258583069, 11.428003311157227, 2.11478590965271, 11.316946983337402, 11.316946983337402, 0.6002355217933655, 8.868476867675781, 1.9522919654846191, 8.879393577575684, 8.879393577575684, 9.808819770812988, 1.4492331743240356, 11.832208633422852, 4.774984836578369, 12.31700325012207, 6.145162582397461, 10.595314025878906, 0.509999692440033, 10.543149948120117, 4.899190902709961, 8.520537376403809, 8.520537376403809, 0.665273904800415, 12.044025421142578, 3.554874897003174, 10.223426818847656, 10.223426818847656, 0.5947378873825073, 10.728867530822754, 3.0386290550231934, 10.034647941589355, 10.034647941589355, 0.5644394755363464, 12.614764213562012, 1.423962950706482, 12.267464637756348, 12.267464637756348, 10.595314025878906, 12.267464637756348, 0.6881512999534607, 10.497003555297852, 3.3307907581329346, 9.8707914352417, 9.8707914352417, 0.781169056892395, 11.282829284667969, 2.5749738216400146, 10.19361686706543, 10.19361686706543, 0.9599223732948303, 10.883995056152344, 1.9074257612228394, 10.738835334777832, 10.738835334777832, 0.7903327941894531, 10.384567260742188, 2.1650309562683105, 9.735280990600586, 9.735280990600586, 0.600529134273529, 10.082382202148438, 2.64369535446167, 8.269901275634766, 8.269901275634766, 0.6314414739608765, 8.432455062866211, 2.755542755126953, 7.066683769226074, 7.066683769226074, 0.7650089859962463, 8.602790832519531, 2.237175703048706, 8.079626083374023, 8.079626083374023, 0.7510474324226379, 12.54934024810791, 2.471312999725342, 12.33205509185791, 12.33205509185791, 0.5172447562217712, 19.15447425842285, 2.7970874309539795, 19.085237503051758, 19.085237503051758, 7.356801986694336, 1.4476805925369263, 9.83033561706543, 4.6358513832092285, 8.537124633789062, 6.11529541015625, 19.085237503051758, 0.679200291633606, 12.262494087219238, 2.1934781074523926, 11.482173919677734, 10.633267402648926, 0.959825873374939, 13.312948226928711, 1.8335822820663452, 13.235675811767578, 13.235675811767578, 0.6195498704910278, 9.484832763671875, 1.5446676015853882, 9.890748977661133, 9.890748977661133, 0.7080584168434143, 9.732739448547363, 1.8019605875015259, 8.958961486816406, 8.958961486816406, 0.8844839930534363, 11.355178833007812, 3.312314510345459, 10.339098930358887, 10.339098930358887, 0.5185532569885254, 11.518238067626953, 1.4913455247879028, 11.444397926330566, 11.444397926330566, 0.7107781767845154, 15.45539665222168, 2.3764243125915527, 14.238655090332031, 14.238655090332031, 0.6394439339637756, 21.546165466308594, 2.3433687686920166, 20.109031677246094, 20.109031677246094, 0.6675122380256653, 22.700807571411133, 3.320685386657715, 22.33852767944336, 22.33852767944336, 5.780173301696777, 0.9621531963348389, 8.215585708618164, 2.7727677822113037, 8.354458808898926, 6.464248180389404, 22.33852767944336, 1.1758604049682617, 13.509401321411133, 1.8875170946121216, 13.82950210571289, 13.82950210571289, 0.5845543742179871, 14.81064510345459, 2.8225395679473877, 12.803604125976562, 12.803604125976562, 1.127054214477539, 15.715627670288086, 1.5113341808319092, 15.453755378723145, 15.453755378723145, 0.6649886965751648, 13.249524116516113, 2.850898504257202, 13.102632522583008, 13.102632522583008, 0.8486541509628296, 17.00101089477539, 2.2719626426696777, 15.889891624450684, 15.889891624450684, 0.9434528350830078, 16.41138458251953, 1.8475465774536133, 16.72710609436035, 16.72710609436035, 0.6284889578819275, 11.574714660644531, 2.077789545059204, 11.313255310058594, 11.313255310058594, 0.9571489095687866, 15.553839683532715, 1.308233618736267, 16.044204711914062, 11.781047821044922, 0.9496046900749207, 11.448919296264648, 2.860248327255249, 10.766971588134766, 10.534952163696289, 6.167629241943359, 1.790992259979248, 8.292303085327148, 1.1869275569915771, 7.954532623291016, 5.845179557800293, 13.93353271484375, 0.986774742603302, 15.454689979553223, 0.8015092611312866, 15.364890098571777, 7.52471923828125, 0.9906911849975586, 12.012683868408203, 1.167878270149231, 11.638570785522461, 11.638570785522461, 1.0864348411560059, 13.87032699584961, 1.434431552886963, 14.039323806762695, 14.039323806762695, 0.3395962715148926, 9.884437561035156, 2.9625234603881836, 10.09786319732666, 8.62557601928711, 0.6093745827674866, 13.462838172912598, 2.3437814712524414, 12.59747314453125, 12.59747314453125, 0.5574187636375427, 12.285762786865234, 1.5553131103515625, 12.173563957214355, 12.173563957214355, 0.6530459523200989, 13.004350662231445, 1.9077832698822021, 13.047298431396484, 13.047298431396484, 0.4091434180736542, 13.369568824768066, 2.121026039123535, 13.940378189086914, 13.940378189086914, 0.3439410924911499, 8.712377548217773, 1.6823570728302002, 8.996960639953613, 8.264077186584473, 6.527836799621582, 0.9189664721488953, 7.248463153839111, 0.9521695375442505, 7.16987419128418, 4.94903039932251, 8.906408309936523, 0.8908913135528564, 14.49416446685791, 2.821382999420166, 14.85698127746582, 11.869260787963867, 0.6321445107460022, 16.669294357299805, 3.4323699474334717, 16.574100494384766, 16.574100494384766, 1.2494585514068604, 14.51338005065918, 2.9126672744750977, 14.695056915283203, 12.045427322387695, 0.49153587222099304, 13.48388385772705, 2.605875015258789, 13.627424240112305, 13.627424240112305, 0.4021762013435364, 10.113058090209961, 3.6154825687408447, 9.992453575134277, 9.711898803710938, 0.9571426510810852, 10.869898796081543, 2.029176950454712, 10.032608032226562, 9.04861068725586, 8.906408309936523, 16.574100494384766, 0.9461250901222229, 9.953763008117676, 1.7416651248931885, 10.777520179748535, 6.138876438140869, 0.7819865345954895, 10.14960765838623, 1.3491286039352417, 9.828022003173828, 9.828022003173828, 0.6497980952262878, 10.310914993286133, 1.2121844291687012, 10.601075172424316, 8.434523582458496, 1.0432422161102295, 18.792377471923828, 0.8334571123123169, 18.286447525024414, 18.286447525024414, 18.286447525024414, 0.7320553660392761, 9.134860038757324, 3.173121452331543, 9.873434066772461, 7.434506416320801, 0.32545602321624756, 8.348712921142578, 2.3684961795806885, 8.034111022949219, 6.94843864440918, 0.6720498204231262, 9.144980430603027, 3.2491979598999023, 9.11160659790039, 8.678695678710938, 0.4612494707107544, 9.487292289733887, 3.5127429962158203, 8.9388427734375, 6.666423797607422, 8.678695678710938, 8.041417121887207, 1.1557105779647827, 7.964803695678711, 1.0772409439086914, 8.030485153198242, 3.7890758514404297, 18.803203582763672, 2.4350264072418213, 12.920756340026855, 0.35487693548202515, 12.86056137084961, 12.86056137084961, 1.086700439453125, 15.692598342895508, 1.4524096250534058, 15.603008270263672, 15.603008270263672, 1.9439151287078857, 34.63523483276367, 0.5548152923583984, 34.27431106567383, 34.27431106567383, 2.057163953781128, 34.92085266113281, 0.5418263673782349, 34.57212448120117, 34.57212448120117, 39.721336364746094, 0.8023834228515625, 12.622427940368652, 1.9777050018310547, 12.829500198364258, 10.481781005859375, 0.6431017518043518, 18.39339828491211, 2.903168201446533, 18.597047805786133, 18.597047805786133, 0.9153760075569153, 16.845687866210938, 0.6826770901679993, 16.534765243530273, 16.534765243530273, 0.939830482006073, 14.64069652557373, 0.5877044200897217, 14.328747749328613, 14.328747749328613, 16.940732955932617, 9.986809730529785, 2.859543561935425, 8.662158012390137, 0.7978425025939941, 8.95612907409668, 8.95612907409668, 39.721336364746094, 6.822216987609863, 6.822216987609863, 0.2335735708475113, 19.51112174987793, 1.2304999828338623, 19.59913444519043]), result=MeasureResult(accuracy=0.8046875, kl_distance=None))
best_measure
Measure(version=0.1, strategy=Strategy(model_hash=-5004976668240111371, bits=[8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 8, 8, 32, 32, 32, 32, 32, 32, 32], thresholds=[2.632871389389038, 2.3602352142333984, 12.065823554992676, 4.038895130157471, 13.020015716552734, 13.020015716552734, 1.9623639583587646, 16.575393676757812, 4.98871374130249, 16.22219467163086, 16.22219467163086, 1.2790210247039795, 18.2012996673584, 4.041762828826904, 15.711963653564453, 13.954992294311523, 13.954992294311523, 1.3126699924468994, 16.296649932861328, 7.094275951385498, 15.00935173034668, 15.00935173034668, 0.7347288131713867, 12.65969467163086, 9.251598358154297, 12.553302764892578, 12.553302764892578, 12.553302764892578, 0.5553937554359436, 10.610788345336914, 6.458847999572754, 10.835762977600098, 10.835762977600098, 0.5140682458877563, 10.755268096923828, 3.0780153274536133, 9.361836433410645, 9.361836433410645, 0.6689160466194153, 9.371060371398926, 4.61334753036499, 9.97728157043457, 9.97728157043457, 0.9048811793327332, 13.447190284729004, 7.991888999938965, 10.989872932434082, 10.989872932434082, 0.7721172571182251, 12.10438060760498, 6.1439104080200195, 13.117056846618652, 13.117056846618652, 0.7478156685829163, 19.330097198486328, 3.0267882347106934, 18.47281837463379, 18.47281837463379, 7.913825988769531, 1.4154207706451416, 11.809602737426758, 6.655673503875732, 7.329173564910889, 7.051487922668457, 18.47281837463379, 0.7486107349395752, 14.490365028381348, 4.264531135559082, 14.008376121520996, 14.008376121520996, 0.5955356955528259, 14.672113418579102, 6.930329322814941, 13.359471321105957, 13.359471321105957, 0.5041235089302063, 11.800966262817383, 4.1340012550354, 10.409193992614746, 10.409193992614746, 0.6865484714508057, 12.078744888305664, 5.231709957122803, 8.374465942382812, 8.374465942382812, 0.7145971059799194, 10.136137962341309, 4.646483421325684, 9.44502067565918, 9.44502067565918, 1.188536524772644, 17.309581756591797, 2.289642810821533, 16.920352935791016, 16.920352935791016, 11.094852447509766, 1.1952425241470337, 15.430564880371094, 4.416873931884766, 13.282967567443848, 10.640891075134277, 16.920352935791016, 1.0737723112106323, 16.343807220458984, 4.300724029541016, 13.963201522827148, 10.449176788330078, 0.6418415904045105, 11.628702163696289, 2.9343960285186768, 10.764244079589844, 10.764244079589844, 0.3671499788761139, 9.588830947875977, 2.7083351612091064, 9.77314281463623, 9.342620849609375, 0.8906435966491699, 14.05899429321289, 5.0458526611328125, 12.8213472366333, 12.8213472366333, 0.6013656258583069, 11.428003311157227, 2.11478590965271, 11.316946983337402, 11.316946983337402, 0.6002355217933655, 8.868476867675781, 1.9522919654846191, 8.879393577575684, 8.879393577575684, 9.808819770812988, 1.4492331743240356, 11.832208633422852, 4.774984836578369, 12.31700325012207, 6.145162582397461, 10.595314025878906, 0.509999692440033, 10.543149948120117, 4.899190902709961, 8.520537376403809, 8.520537376403809, 0.665273904800415, 12.044025421142578, 3.554874897003174, 10.223426818847656, 10.223426818847656, 0.5947378873825073, 10.728867530822754, 3.0386290550231934, 10.034647941589355, 10.034647941589355, 0.5644394755363464, 12.614764213562012, 1.423962950706482, 12.267464637756348, 12.267464637756348, 10.595314025878906, 12.267464637756348, 0.6881512999534607, 10.497003555297852, 3.3307907581329346, 9.8707914352417, 9.8707914352417, 0.781169056892395, 11.282829284667969, 2.5749738216400146, 10.19361686706543, 10.19361686706543, 0.9599223732948303, 10.883995056152344, 1.9074257612228394, 10.738835334777832, 10.738835334777832, 0.7903327941894531, 10.384567260742188, 2.1650309562683105, 9.735280990600586, 9.735280990600586, 0.600529134273529, 10.082382202148438, 2.64369535446167, 8.269901275634766, 8.269901275634766, 0.6314414739608765, 8.432455062866211, 2.755542755126953, 7.066683769226074, 7.066683769226074, 0.7650089859962463, 8.602790832519531, 2.237175703048706, 8.079626083374023, 8.079626083374023, 0.7510474324226379, 12.54934024810791, 2.471312999725342, 12.33205509185791, 12.33205509185791, 0.5172447562217712, 19.15447425842285, 2.7970874309539795, 19.085237503051758, 19.085237503051758, 7.356801986694336, 1.4476805925369263, 9.83033561706543, 4.6358513832092285, 8.537124633789062, 6.11529541015625, 19.085237503051758, 0.679200291633606, 12.262494087219238, 2.1934781074523926, 11.482173919677734, 10.633267402648926, 0.959825873374939, 13.312948226928711, 1.8335822820663452, 13.235675811767578, 13.235675811767578, 0.6195498704910278, 9.484832763671875, 1.5446676015853882, 9.890748977661133, 9.890748977661133, 0.7080584168434143, 9.732739448547363, 1.8019605875015259, 8.958961486816406, 8.958961486816406, 0.8844839930534363, 11.355178833007812, 3.312314510345459, 10.339098930358887, 10.339098930358887, 0.5185532569885254, 11.518238067626953, 1.4913455247879028, 11.444397926330566, 11.444397926330566, 0.7107781767845154, 15.45539665222168, 2.3764243125915527, 14.238655090332031, 14.238655090332031, 0.6394439339637756, 21.546165466308594, 2.3433687686920166, 20.109031677246094, 20.109031677246094, 0.6675122380256653, 22.700807571411133, 3.320685386657715, 22.33852767944336, 22.33852767944336, 5.780173301696777, 0.9621531963348389, 8.215585708618164, 2.7727677822113037, 8.354458808898926, 6.464248180389404, 22.33852767944336, 1.1758604049682617, 13.509401321411133, 1.8875170946121216, 13.82950210571289, 13.82950210571289, 0.5845543742179871, 14.81064510345459, 2.8225395679473877, 12.803604125976562, 12.803604125976562, 1.127054214477539, 15.715627670288086, 1.5113341808319092, 15.453755378723145, 15.453755378723145, 0.6649886965751648, 13.249524116516113, 2.850898504257202, 13.102632522583008, 13.102632522583008, 0.8486541509628296, 17.00101089477539, 2.2719626426696777, 15.889891624450684, 15.889891624450684, 0.9434528350830078, 16.41138458251953, 1.8475465774536133, 16.72710609436035, 16.72710609436035, 0.6284889578819275, 11.574714660644531, 2.077789545059204, 11.313255310058594, 11.313255310058594, 0.9571489095687866, 15.553839683532715, 1.308233618736267, 16.044204711914062, 11.781047821044922, 0.9496046900749207, 11.448919296264648, 2.860248327255249, 10.766971588134766, 10.534952163696289, 6.167629241943359, 1.790992259979248, 8.292303085327148, 1.1869275569915771, 7.954532623291016, 5.845179557800293, 13.93353271484375, 0.986774742603302, 15.454689979553223, 0.8015092611312866, 15.364890098571777, 7.52471923828125, 0.9906911849975586, 12.012683868408203, 1.167878270149231, 11.638570785522461, 11.638570785522461, 1.0864348411560059, 13.87032699584961, 1.434431552886963, 14.039323806762695, 14.039323806762695, 0.3395962715148926, 9.884437561035156, 2.9625234603881836, 10.09786319732666, 8.62557601928711, 0.6093745827674866, 13.462838172912598, 2.3437814712524414, 12.59747314453125, 12.59747314453125, 0.5574187636375427, 12.285762786865234, 1.5553131103515625, 12.173563957214355, 12.173563957214355, 0.6530459523200989, 13.004350662231445, 1.9077832698822021, 13.047298431396484, 13.047298431396484, 0.4091434180736542, 13.369568824768066, 2.121026039123535, 13.940378189086914, 13.940378189086914, 0.3439410924911499, 8.712377548217773, 1.6823570728302002, 8.996960639953613, 8.264077186584473, 6.527836799621582, 0.9189664721488953, 7.248463153839111, 0.9521695375442505, 7.16987419128418, 4.94903039932251, 8.906408309936523, 0.8908913135528564, 14.49416446685791, 2.821382999420166, 14.85698127746582, 11.869260787963867, 0.6321445107460022, 16.669294357299805, 3.4323699474334717, 16.574100494384766, 16.574100494384766, 1.2494585514068604, 14.51338005065918, 2.9126672744750977, 14.695056915283203, 12.045427322387695, 0.49153587222099304, 13.48388385772705, 2.605875015258789, 13.627424240112305, 13.627424240112305, 0.4021762013435364, 10.113058090209961, 3.6154825687408447, 9.992453575134277, 9.711898803710938, 0.9571426510810852, 10.869898796081543, 2.029176950454712, 10.032608032226562, 9.04861068725586, 8.906408309936523, 16.574100494384766, 0.9461250901222229, 9.953763008117676, 1.7416651248931885, 10.777520179748535, 6.138876438140869, 0.7819865345954895, 10.14960765838623, 1.3491286039352417, 9.828022003173828, 9.828022003173828, 0.6497980952262878, 10.310914993286133, 1.2121844291687012, 10.601075172424316, 8.434523582458496, 1.0432422161102295, 18.792377471923828, 0.8334571123123169, 18.286447525024414, 18.286447525024414, 18.286447525024414, 0.7320553660392761, 9.134860038757324, 3.173121452331543, 9.873434066772461, 7.434506416320801, 0.32545602321624756, 8.348712921142578, 2.3684961795806885, 8.034111022949219, 6.94843864440918, 0.6720498204231262, 9.144980430603027, 3.2491979598999023, 9.11160659790039, 8.678695678710938, 0.4612494707107544, 9.487292289733887, 3.5127429962158203, 8.9388427734375, 6.666423797607422, 8.678695678710938, 8.041417121887207, 1.1557105779647827, 7.964803695678711, 1.0772409439086914, 8.030485153198242, 3.7890758514404297, 18.803203582763672, 2.4350264072418213, 12.920756340026855, 0.35487693548202515, 12.86056137084961, 12.86056137084961, 1.086700439453125, 15.692598342895508, 1.4524096250534058, 15.603008270263672, 15.603008270263672, 1.9439151287078857, 34.63523483276367, 0.5548152923583984, 34.27431106567383, 34.27431106567383, 2.057163953781128, 34.92085266113281, 0.5418263673782349, 34.57212448120117, 34.57212448120117, 39.721336364746094, 0.8023834228515625, 12.622427940368652, 1.9777050018310547, 12.829500198364258, 10.481781005859375, 0.6431017518043518, 18.39339828491211, 2.903168201446533, 18.597047805786133, 18.597047805786133, 0.9153760075569153, 16.845687866210938, 0.6826770901679993, 16.534765243530273, 16.534765243530273, 0.939830482006073, 14.64069652557373, 0.5877044200897217, 14.328747749328613, 14.328747749328613, 16.940732955932617, 9.986809730529785, 2.859543561935425, 8.662158012390137, 0.7978425025939941, 8.95612907409668, 8.95612907409668, 39.721336364746094, 6.822216987609863, 6.822216987609863, 0.2335735708475113, 19.51112174987793, 1.2304999828338623, 19.59913444519043]), result=MeasureResult(accuracy=0.8046875, kl_distance=None))
data
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
nn.max_pool2d
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
constant
nn.conv2d
constant
add
nn.relu
concatenate
nn.avg_pool2d
constant
nn.conv2d
constant
add
nn.relu
concatenate
nn.avg_pool2d
nn.batch_flatten
constant
nn.dense
constant
add
data -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.max_pool2d
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.max_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.max_pool2d -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.max_pool2d
nn.relu -> concatenate
nn.relu -> concatenate
nn.max_pool2d -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.max_pool2d
nn.relu -> concatenate
nn.relu -> concatenate
nn.max_pool2d -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
concatenate -> concatenate
concatenate -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
nn.relu -> concatenate
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.conv2d
constant -> nn.conv2d
nn.conv2d -> add
constant -> add
add -> nn.relu
nn.relu -> concatenate
concatenate -> concatenate
concatenate -> concatenate
nn.relu -> concatenate
concatenate -> nn.avg_pool2d
nn.avg_pool2d -> nn.batch_flatten
nn.batch_flatten -> nn.dense
constant -> nn.dense
nn.dense -> add
constant -> add
analyzed condition
node_conds: [False, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, False, True, False, True, True, True, False, False, False, False, False, False]
edge_conds: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]

select descriptor
---------
nn.conv2d[%2]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%4]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%5]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%7]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%9]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%10]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%12]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%14]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%15]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%16]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%18]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%20]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%21]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%23]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%25]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%26]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%27]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%29]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%31]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%32]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%34]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%36]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%37]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%39]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%41]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%42]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%44]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%46]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%47]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%49]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%51]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%52]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%54]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%56]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%57]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%60]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%62]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%63]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%64]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%66]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%68]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%69]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%71]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%73]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%74]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%76]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%78]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%79]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%81]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%83]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%84]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%86]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%88]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%89]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%91]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%93]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%94]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%97]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%99]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%100]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%101]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%103]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%105]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%106]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%108]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%110]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%111]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%113]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%115]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%116]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%118]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%120]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%121]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%123]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%125]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%126]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%128]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%130]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%131]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%134]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%136]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%137]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%138]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%140]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%142]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%143]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%145]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%147]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%148]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%150]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%152]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%153]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%155]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%157]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%158]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%159]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%160]
  in bits: [32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%162]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%164]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%165]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%167]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%169]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%170]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%172]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%174]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%175]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%177]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%179]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%180]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%182]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%184]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%185]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%187]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%189]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%190]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%192]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%194]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%195]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%197]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%199]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%200]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%202]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%204]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%205]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%208]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%210]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%211]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%212]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%214]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%216]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%217]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%219]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%221]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%222]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%224]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%226]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%227]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%229]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%231]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%232]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%234]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%236]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%237]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%239]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%241]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%242]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%244]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%246]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%247]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%249]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%251]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%252]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%254]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%256]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%257]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%260]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%262]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%263]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%264]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%266]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%268]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%269]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%271]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%273]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%274]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%276]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%278]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%279]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%281]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%283]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%284]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%286]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%288]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%289]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%291]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%293]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%294]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%296]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%298]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%299]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%301]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%303]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%304]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%306]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%308]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%309]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%312]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%314]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%315]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%316]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%318]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%320]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%321]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%323]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%325]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%326]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%328]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%330]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%331]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%333]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%335]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%336]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%338]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%340]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%341]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%343]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%345]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%346]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%348]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%350]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%351]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%353]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%355]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%356]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%358]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%360]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%361]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%364]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%366]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%367]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%368]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%370]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%372]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%373]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%375]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%377]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%378]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%380]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%382]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%383]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%385]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%387]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%388]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%390]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%392]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%393]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%395]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%397]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%398]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.max_pool2d[%399]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%400]
  in bits: [32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%402]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%404]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%405]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%407]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%409]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%410]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%412]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%414]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%415]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%417]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%419]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%420]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%421]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%423]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%425]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%426]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%428]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%430]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%431]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%433]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%435]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%436]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%438]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%440]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%441]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%442]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%445]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%447]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%448]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%449]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%451]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%453]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%454]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%456]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%458]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%459]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%461]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%463]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%464]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%466]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%468]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%469]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%470]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%472]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%474]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%475]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%477]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%479]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%480]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%482]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%484]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%485]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%487]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%489]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%490]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%491]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.conv2d[%494]
  in bits: [8, 8]
  OpDesc[in_dtypes=int8, out_dtypes=int32]
---------
add[%496]
  in bits: [32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
nn.relu[%497]
  in bits: [32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
---------
concatenate[%498]
  in bits: [32, 32, 32, 32]
  OpDesc[in_dtypes=int32, out_dtypes=int32]
fn (%data: Tensor[(32, 3, 299, 299), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(32, 3, 3, 3), float32] */ /* ty=Tensor[(32, 3, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %1 = add(%0, meta[relay.Constant][1] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %3 = nn.conv2d(%2, meta[relay.Constant][2] /* ty=Tensor[(32, 32, 3, 3), float32] */ /* ty=Tensor[(32, 32, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %4 = add(%3, meta[relay.Constant][3] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %5 = nn.relu(%4) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %6 = nn.conv2d(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */ /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %7 = add(%6, meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %8 = nn.relu(%7) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %9 = nn.max_pool2d(%8, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 64, 73, 73), float32] */;
  %10 = nn.conv2d(%9, meta[relay.Constant][6] /* ty=Tensor[(80, 64, 1, 1), float32] */ /* ty=Tensor[(80, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=80, kernel_size=[1, 1]) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %11 = add(%10, meta[relay.Constant][7] /* ty=Tensor[(80, 1, 1), float32] */ /* ty=Tensor[(80, 1, 1), float32] */) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %12 = nn.relu(%11) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %13 = nn.conv2d(%12, meta[relay.Constant][8] /* ty=Tensor[(192, 80, 3, 3), float32] */ /* ty=Tensor[(192, 80, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %14 = add(%13, meta[relay.Constant][9] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %15 = nn.relu(%14) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %16 = nn.max_pool2d(%15, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %17 = nn.conv2d(%16, meta[relay.Constant][10] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %18 = add(%17, meta[relay.Constant][11] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %19 = nn.relu(%18) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %20 = nn.conv2d(%16, meta[relay.Constant][12] /* ty=Tensor[(48, 192, 1, 1), float32] */ /* ty=Tensor[(48, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %21 = add(%20, meta[relay.Constant][13] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %22 = nn.relu(%21) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %23 = nn.conv2d(%22, meta[relay.Constant][14] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %24 = add(%23, meta[relay.Constant][15] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %25 = nn.relu(%24) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %26 = nn.conv2d(%16, meta[relay.Constant][16] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %27 = add(%26, meta[relay.Constant][17] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %28 = nn.relu(%27) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %29 = nn.conv2d(%28, meta[relay.Constant][18] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %30 = add(%29, meta[relay.Constant][19] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %31 = nn.relu(%30) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %32 = nn.conv2d(%31, meta[relay.Constant][20] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %33 = add(%32, meta[relay.Constant][21] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %34 = nn.relu(%33) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %35 = nn.avg_pool2d(%16, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %36 = nn.conv2d(%35, meta[relay.Constant][22] /* ty=Tensor[(32, 192, 1, 1), float32] */ /* ty=Tensor[(32, 192, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %37 = add(%36, meta[relay.Constant][23] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %38 = nn.relu(%37) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %39 = (%19, %25, %34, %38);
  %40 = concatenate(%39, axis=1) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %41 = nn.conv2d(%40, meta[relay.Constant][24] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %42 = add(%41, meta[relay.Constant][25] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %43 = nn.relu(%42) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %44 = nn.conv2d(%40, meta[relay.Constant][26] /* ty=Tensor[(48, 256, 1, 1), float32] */ /* ty=Tensor[(48, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %45 = add(%44, meta[relay.Constant][27] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %46 = nn.relu(%45) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %47 = nn.conv2d(%46, meta[relay.Constant][28] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %48 = add(%47, meta[relay.Constant][29] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %49 = nn.relu(%48) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %50 = nn.conv2d(%40, meta[relay.Constant][30] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %51 = add(%50, meta[relay.Constant][31] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %52 = nn.relu(%51) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %53 = nn.conv2d(%52, meta[relay.Constant][32] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %54 = add(%53, meta[relay.Constant][33] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %55 = nn.relu(%54) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %56 = nn.conv2d(%55, meta[relay.Constant][34] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %57 = add(%56, meta[relay.Constant][35] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %59 = nn.avg_pool2d(%40, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %60 = nn.conv2d(%59, meta[relay.Constant][36] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %61 = add(%60, meta[relay.Constant][37] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %62 = nn.relu(%61) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %63 = (%43, %49, %58, %62);
  %64 = concatenate(%63, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %65 = nn.conv2d(%64, meta[relay.Constant][38] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %66 = add(%65, meta[relay.Constant][39] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %67 = nn.relu(%66) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %68 = nn.conv2d(%64, meta[relay.Constant][40] /* ty=Tensor[(48, 288, 1, 1), float32] */ /* ty=Tensor[(48, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %69 = add(%68, meta[relay.Constant][41] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %70 = nn.relu(%69) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %71 = nn.conv2d(%70, meta[relay.Constant][42] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %72 = add(%71, meta[relay.Constant][43] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %73 = nn.relu(%72) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %74 = nn.conv2d(%64, meta[relay.Constant][44] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %75 = add(%74, meta[relay.Constant][45] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %76 = nn.relu(%75) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %77 = nn.conv2d(%76, meta[relay.Constant][46] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %78 = add(%77, meta[relay.Constant][47] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %79 = nn.relu(%78) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %80 = nn.conv2d(%79, meta[relay.Constant][48] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %81 = add(%80, meta[relay.Constant][49] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %82 = nn.relu(%81) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %83 = nn.avg_pool2d(%64, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %84 = nn.conv2d(%83, meta[relay.Constant][50] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %85 = add(%84, meta[relay.Constant][51] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %86 = nn.relu(%85) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %87 = (%67, %73, %82, %86);
  %88 = concatenate(%87, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %89 = nn.conv2d(%88, meta[relay.Constant][52] /* ty=Tensor[(384, 288, 3, 3), float32] */ /* ty=Tensor[(384, 288, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %90 = add(%89, meta[relay.Constant][53] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %91 = nn.relu(%90) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %92 = nn.conv2d(%88, meta[relay.Constant][54] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %93 = add(%92, meta[relay.Constant][55] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %94 = nn.relu(%93) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %95 = nn.conv2d(%94, meta[relay.Constant][56] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %96 = add(%95, meta[relay.Constant][57] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %97 = nn.relu(%96) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %98 = nn.conv2d(%97, meta[relay.Constant][58] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %99 = add(%98, meta[relay.Constant][59] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %100 = nn.relu(%99) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %101 = nn.max_pool2d(%88, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 288, 17, 17), float32] */;
  %102 = (%91, %100, %101);
  %103 = concatenate(%102, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %104 = nn.conv2d(%103, meta[relay.Constant][60] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %105 = add(%104, meta[relay.Constant][61] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %106 = nn.relu(%105) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %107 = nn.conv2d(%103, meta[relay.Constant][62] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %108 = add(%107, meta[relay.Constant][63] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %109 = nn.relu(%108) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %110 = nn.conv2d(%109, meta[relay.Constant][64] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %111 = add(%110, meta[relay.Constant][65] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %112 = nn.relu(%111) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %113 = nn.conv2d(%112, meta[relay.Constant][66] /* ty=Tensor[(192, 128, 7, 1), float32] */ /* ty=Tensor[(192, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %114 = add(%113, meta[relay.Constant][67] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %115 = nn.relu(%114) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %116 = nn.conv2d(%103, meta[relay.Constant][68] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %117 = add(%116, meta[relay.Constant][69] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %118 = nn.relu(%117) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %119 = nn.conv2d(%118, meta[relay.Constant][70] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %120 = add(%119, meta[relay.Constant][71] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %121 = nn.relu(%120) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %122 = nn.conv2d(%121, meta[relay.Constant][72] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %123 = add(%122, meta[relay.Constant][73] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %125 = nn.conv2d(%124, meta[relay.Constant][74] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %126 = add(%125, meta[relay.Constant][75] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %127 = nn.relu(%126) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %128 = nn.conv2d(%127, meta[relay.Constant][76] /* ty=Tensor[(192, 128, 1, 7), float32] */ /* ty=Tensor[(192, 128, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %129 = add(%128, meta[relay.Constant][77] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %130 = nn.relu(%129) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %131 = nn.avg_pool2d(%103, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %132 = nn.conv2d(%131, meta[relay.Constant][78] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %133 = add(%132, meta[relay.Constant][79] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %134 = nn.relu(%133) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %135 = (%106, %115, %130, %134);
  %136 = concatenate(%135, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %137 = nn.conv2d(%136, meta[relay.Constant][80] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %138 = add(%137, meta[relay.Constant][81] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %139 = nn.relu(%138) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %140 = nn.conv2d(%136, meta[relay.Constant][82] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %141 = add(%140, meta[relay.Constant][83] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %142 = nn.relu(%141) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %143 = nn.conv2d(%142, meta[relay.Constant][84] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %144 = add(%143, meta[relay.Constant][85] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %145 = nn.relu(%144) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %146 = nn.conv2d(%145, meta[relay.Constant][86] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %147 = add(%146, meta[relay.Constant][87] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %148 = nn.relu(%147) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %149 = nn.conv2d(%136, meta[relay.Constant][88] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %150 = add(%149, meta[relay.Constant][89] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %151 = nn.relu(%150) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %152 = nn.conv2d(%151, meta[relay.Constant][90] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %153 = add(%152, meta[relay.Constant][91] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %154 = nn.relu(%153) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %155 = nn.conv2d(%154, meta[relay.Constant][92] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %156 = add(%155, meta[relay.Constant][93] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %158 = nn.conv2d(%157, meta[relay.Constant][94] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %159 = add(%158, meta[relay.Constant][95] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %160 = nn.relu(%159) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %161 = nn.conv2d(%160, meta[relay.Constant][96] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %162 = add(%161, meta[relay.Constant][97] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %163 = nn.relu(%162) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %164 = nn.avg_pool2d(%136, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %165 = nn.conv2d(%164, meta[relay.Constant][98] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %166 = add(%165, meta[relay.Constant][99] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %167 = nn.relu(%166) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %168 = (%139, %148, %163, %167);
  %169 = concatenate(%168, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %170 = nn.conv2d(%169, meta[relay.Constant][100] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %171 = add(%170, meta[relay.Constant][101] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %172 = nn.relu(%171) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %173 = nn.conv2d(%169, meta[relay.Constant][102] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %174 = add(%173, meta[relay.Constant][103] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %175 = nn.relu(%174) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %176 = nn.conv2d(%175, meta[relay.Constant][104] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %177 = add(%176, meta[relay.Constant][105] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %178 = nn.relu(%177) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %179 = nn.conv2d(%178, meta[relay.Constant][106] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %180 = add(%179, meta[relay.Constant][107] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %181 = nn.relu(%180) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %182 = nn.conv2d(%169, meta[relay.Constant][108] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %183 = add(%182, meta[relay.Constant][109] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %184 = nn.relu(%183) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %185 = nn.conv2d(%184, meta[relay.Constant][110] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %186 = add(%185, meta[relay.Constant][111] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %187 = nn.relu(%186) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %188 = nn.conv2d(%187, meta[relay.Constant][112] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %189 = add(%188, meta[relay.Constant][113] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %190 = nn.relu(%189) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %191 = nn.conv2d(%190, meta[relay.Constant][114] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %192 = add(%191, meta[relay.Constant][115] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %193 = nn.relu(%192) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %194 = nn.conv2d(%193, meta[relay.Constant][116] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %195 = add(%194, meta[relay.Constant][117] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %196 = nn.relu(%195) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %197 = nn.avg_pool2d(%169, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %198 = nn.conv2d(%197, meta[relay.Constant][118] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %199 = add(%198, meta[relay.Constant][119] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %200 = nn.relu(%199) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %201 = (%172, %181, %196, %200);
  %202 = concatenate(%201, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %203 = nn.conv2d(%202, meta[relay.Constant][120] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %204 = add(%203, meta[relay.Constant][121] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %206 = nn.conv2d(%202, meta[relay.Constant][122] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %207 = add(%206, meta[relay.Constant][123] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %208 = nn.relu(%207) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %209 = nn.conv2d(%208, meta[relay.Constant][124] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %210 = add(%209, meta[relay.Constant][125] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %211 = nn.relu(%210) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %212 = nn.conv2d(%211, meta[relay.Constant][126] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %213 = add(%212, meta[relay.Constant][127] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %214 = nn.relu(%213) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %215 = nn.conv2d(%202, meta[relay.Constant][128] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %216 = add(%215, meta[relay.Constant][129] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %217 = nn.relu(%216) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %218 = nn.conv2d(%217, meta[relay.Constant][130] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %219 = add(%218, meta[relay.Constant][131] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %220 = nn.relu(%219) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %221 = nn.conv2d(%220, meta[relay.Constant][132] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %222 = add(%221, meta[relay.Constant][133] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %223 = nn.relu(%222) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %224 = nn.conv2d(%223, meta[relay.Constant][134] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %225 = add(%224, meta[relay.Constant][135] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %226 = nn.relu(%225) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %227 = nn.conv2d(%226, meta[relay.Constant][136] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %228 = add(%227, meta[relay.Constant][137] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %229 = nn.relu(%228) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %230 = nn.avg_pool2d(%202, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %231 = nn.conv2d(%230, meta[relay.Constant][138] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %232 = add(%231, meta[relay.Constant][139] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %233 = nn.relu(%232) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %234 = (%205, %214, %229, %233);
  %235 = concatenate(%234, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %236 = nn.conv2d(%235, meta[relay.Constant][140] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %237 = add(%236, meta[relay.Constant][141] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %238 = nn.relu(%237) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %239 = nn.conv2d(%238, meta[relay.Constant][142] /* ty=Tensor[(320, 192, 3, 3), float32] */ /* ty=Tensor[(320, 192, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=320, kernel_size=[3, 3]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %240 = add(%239, meta[relay.Constant][143] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %241 = nn.relu(%240) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %242 = nn.conv2d(%235, meta[relay.Constant][144] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %243 = add(%242, meta[relay.Constant][145] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %244 = nn.relu(%243) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %245 = nn.conv2d(%244, meta[relay.Constant][146] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %246 = add(%245, meta[relay.Constant][147] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %247 = nn.relu(%246) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %248 = nn.conv2d(%247, meta[relay.Constant][148] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %249 = add(%248, meta[relay.Constant][149] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %250 = nn.relu(%249) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %251 = nn.conv2d(%250, meta[relay.Constant][150] /* ty=Tensor[(192, 192, 3, 3), float32] */ /* ty=Tensor[(192, 192, 3, 3), float32] */, strides=[2, 2], padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %252 = add(%251, meta[relay.Constant][151] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %253 = nn.relu(%252) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %254 = nn.max_pool2d(%235, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %255 = (%241, %253, %254);
  %256 = concatenate(%255, axis=1) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %257 = nn.conv2d(%256, meta[relay.Constant][152] /* ty=Tensor[(320, 1280, 1, 1), float32] */ /* ty=Tensor[(320, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %258 = add(%257, meta[relay.Constant][153] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %259 = nn.relu(%258) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %260 = nn.conv2d(%256, meta[relay.Constant][154] /* ty=Tensor[(384, 1280, 1, 1), float32] */ /* ty=Tensor[(384, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %261 = add(%260, meta[relay.Constant][155] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %262 = nn.relu(%261) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %263 = nn.conv2d(%262, meta[relay.Constant][156] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %264 = add(%263, meta[relay.Constant][157] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %265 = nn.relu(%264) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %266 = nn.conv2d(%262, meta[relay.Constant][158] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %267 = add(%266, meta[relay.Constant][159] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %268 = nn.relu(%267) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %269 = (%265, %268);
  %270 = concatenate(%269, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %271 = nn.conv2d(%256, meta[relay.Constant][160] /* ty=Tensor[(448, 1280, 1, 1), float32] */ /* ty=Tensor[(448, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %272 = add(%271, meta[relay.Constant][161] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %273 = nn.relu(%272) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %274 = nn.conv2d(%273, meta[relay.Constant][162] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %275 = add(%274, meta[relay.Constant][163] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %276 = nn.relu(%275) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %277 = nn.conv2d(%276, meta[relay.Constant][164] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %278 = add(%277, meta[relay.Constant][165] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %279 = nn.relu(%278) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %280 = nn.conv2d(%276, meta[relay.Constant][166] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %281 = add(%280, meta[relay.Constant][167] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %282 = nn.relu(%281) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %283 = (%279, %282);
  %284 = concatenate(%283, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %285 = nn.avg_pool2d(%256, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %286 = nn.conv2d(%285, meta[relay.Constant][168] /* ty=Tensor[(192, 1280, 1, 1), float32] */ /* ty=Tensor[(192, 1280, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %287 = add(%286, meta[relay.Constant][169] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %288 = nn.relu(%287) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %289 = (%259, %270, %284, %288);
  %290 = concatenate(%289, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %291 = nn.conv2d(%290, meta[relay.Constant][170] /* ty=Tensor[(320, 2048, 1, 1), float32] */ /* ty=Tensor[(320, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %292 = add(%291, meta[relay.Constant][171] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %293 = nn.relu(%292) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %294 = nn.conv2d(%290, meta[relay.Constant][172] /* ty=Tensor[(384, 2048, 1, 1), float32] */ /* ty=Tensor[(384, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %295 = add(%294, meta[relay.Constant][173] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %296 = nn.relu(%295) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %297 = nn.conv2d(%296, meta[relay.Constant][174] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %298 = add(%297, meta[relay.Constant][175] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %299 = nn.relu(%298) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %300 = nn.conv2d(%296, meta[relay.Constant][176] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %301 = add(%300, meta[relay.Constant][177] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %302 = nn.relu(%301) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %303 = (%299, %302);
  %304 = concatenate(%303, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %305 = nn.conv2d(%290, meta[relay.Constant][178] /* ty=Tensor[(448, 2048, 1, 1), float32] */ /* ty=Tensor[(448, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %306 = add(%305, meta[relay.Constant][179] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %307 = nn.relu(%306) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %308 = nn.conv2d(%307, meta[relay.Constant][180] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %309 = add(%308, meta[relay.Constant][181] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %310 = nn.relu(%309) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %311 = nn.conv2d(%310, meta[relay.Constant][182] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %312 = add(%311, meta[relay.Constant][183] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %313 = nn.relu(%312) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %314 = nn.conv2d(%310, meta[relay.Constant][184] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %315 = add(%314, meta[relay.Constant][185] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %316 = nn.relu(%315) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %317 = (%313, %316);
  %318 = concatenate(%317, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %319 = nn.avg_pool2d(%290, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %320 = nn.conv2d(%319, meta[relay.Constant][186] /* ty=Tensor[(192, 2048, 1, 1), float32] */ /* ty=Tensor[(192, 2048, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %321 = add(%320, meta[relay.Constant][187] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %322 = nn.relu(%321) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %323 = (%293, %304, %318, %322);
  %324 = concatenate(%323, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %325 = nn.avg_pool2d(%324, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %326 = nn.batch_flatten(%325) /* ty=Tensor[(32, 2048), float32] */;
  %327 = nn.dense(%326, meta[relay.Constant][188] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%327, meta[relay.Constant][189] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data

calculate parameters
---------
data[%0] -> nn.conv2d[%2]
  bit=8, threshold=2.632871389389038
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.02056930772960186, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%1] -> nn.conv2d[%2]
  bit=8, threshold=2.3602352142333984
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.018439337611198425, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%2] -> add[%4]
  bit=32, threshold=4.038895130157471
  SimulatedQuantizeParams(in_scale=0.0003792844, out_scale=5.618586929045932e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%3] -> add[%4]
  bit=32, threshold=4.038895130157471
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.618586929045932e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%4] -> nn.relu[%5]
  bit=32, threshold=13.020015716552734
  SimulatedQuantizeParams(in_scale=5.618587e-09, out_scale=5.618586929045932e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%5] -> nn.conv2d[%7]
  bit=8, threshold=13.020015716552734
  SimulatedQuantizeParams(in_scale=5.618587e-09, out_scale=0.10171887278556824, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%6] -> nn.conv2d[%7]
  bit=8, threshold=1.9623639583587646
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.015330968424677849, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%7] -> add[%9]
  bit=32, threshold=4.98871374130249
  SimulatedQuantizeParams(in_scale=0.0015594488, out_scale=7.718519157151604e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%8] -> add[%9]
  bit=32, threshold=4.98871374130249
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.718519157151604e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%9] -> nn.relu[%10]
  bit=32, threshold=16.22219467163086
  SimulatedQuantizeParams(in_scale=7.718519e-09, out_scale=7.718519157151604e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%10] -> nn.conv2d[%12]
  bit=8, threshold=16.22219467163086
  SimulatedQuantizeParams(in_scale=7.718519e-09, out_scale=0.1267358958721161, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%11] -> nn.conv2d[%12]
  bit=8, threshold=1.2790210247039795
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00999235175549984, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%12] -> add[%14]
  bit=32, threshold=4.041762828826904
  SimulatedQuantizeParams(in_scale=0.0012663896, out_scale=8.475640633776038e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%13] -> add[%14]
  bit=32, threshold=4.041762828826904
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.475640633776038e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%14] -> nn.relu[%15]
  bit=32, threshold=15.711963653564453
  SimulatedQuantizeParams(in_scale=8.475641e-09, out_scale=8.475640633776038e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%15] -> nn.max_pool2d[%16]
  bit=32, threshold=13.954992294311523
  SimulatedQuantizeParams(in_scale=8.475641e-09, out_scale=6.498299676138686e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%16] -> nn.conv2d[%18]
  bit=8, threshold=13.954992294311523
  SimulatedQuantizeParams(in_scale=6.4982997e-09, out_scale=0.10902337729930878, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%17] -> nn.conv2d[%18]
  bit=8, threshold=1.3126699924468994
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.010255234315991402, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%18] -> add[%20]
  bit=32, threshold=7.094275951385498
  SimulatedQuantizeParams(in_scale=0.0011180603, out_scale=7.588718986539789e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%19] -> add[%20]
  bit=32, threshold=7.094275951385498
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.588718986539789e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%20] -> nn.relu[%21]
  bit=32, threshold=15.00935173034668
  SimulatedQuantizeParams(in_scale=7.588719e-09, out_scale=7.588718986539789e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%21] -> nn.conv2d[%23]
  bit=8, threshold=15.00935173034668
  SimulatedQuantizeParams(in_scale=7.588719e-09, out_scale=0.11726056039333344, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%22] -> nn.conv2d[%23]
  bit=8, threshold=0.7347288131713867
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005740068852901459, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%23] -> add[%25]
  bit=32, threshold=9.251598358154297
  SimulatedQuantizeParams(in_scale=0.0006730837, out_scale=5.895129717714553e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%24] -> add[%25]
  bit=32, threshold=9.251598358154297
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.895129717714553e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%25] -> nn.relu[%26]
  bit=32, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.8951297e-09, out_scale=5.895129717714553e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%26] -> nn.max_pool2d[%27]
  bit=32, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.8951297e-09, out_scale=5.8455871254636804e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%27] -> nn.conv2d[%29]
  bit=8, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.845587e-09, out_scale=0.09807267785072327, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%28] -> nn.conv2d[%29]
  bit=8, threshold=0.5553937554359436
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004339013714343309, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%29] -> add[%31]
  bit=32, threshold=6.458847999572754
  SimulatedQuantizeParams(in_scale=0.0004255387, out_scale=4.941033360239544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%30] -> add[%31]
  bit=32, threshold=6.458847999572754
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.941033360239544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%31] -> nn.relu[%32]
  bit=32, threshold=10.835762977600098
  SimulatedQuantizeParams(in_scale=4.9410334e-09, out_scale=4.941033360239544e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%27] -> nn.conv2d[%34]
  bit=8, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.845587e-09, out_scale=0.09807267785072327, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%33] -> nn.conv2d[%34]
  bit=8, threshold=0.5140682458877563
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0040161581709980965, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%34] -> add[%36]
  bit=32, threshold=3.0780153274536133
  SimulatedQuantizeParams(in_scale=0.00039387538, out_scale=5.008311987353409e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%35] -> add[%36]
  bit=32, threshold=3.0780153274536133
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.008311987353409e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%36] -> nn.relu[%37]
  bit=32, threshold=9.361836433410645
  SimulatedQuantizeParams(in_scale=5.008312e-09, out_scale=5.008311987353409e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%37] -> nn.conv2d[%39]
  bit=8, threshold=9.361836433410645
  SimulatedQuantizeParams(in_scale=5.008312e-09, out_scale=0.07313934713602066, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%38] -> nn.conv2d[%39]
  bit=8, threshold=0.6689160466194153
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005225906614214182, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%39] -> add[%41]
  bit=32, threshold=4.61334753036499
  SimulatedQuantizeParams(in_scale=0.0003822194, out_scale=4.363740035984165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%40] -> add[%41]
  bit=32, threshold=4.61334753036499
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.363740035984165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%41] -> nn.relu[%42]
  bit=32, threshold=9.97728157043457
  SimulatedQuantizeParams(in_scale=4.36374e-09, out_scale=4.363740035984165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%27] -> nn.conv2d[%44]
  bit=8, threshold=12.553302764892578
  SimulatedQuantizeParams(in_scale=5.845587e-09, out_scale=0.09807267785072327, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%43] -> nn.conv2d[%44]
  bit=8, threshold=0.9048811793327332
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007069384213536978, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%44] -> add[%46]
  bit=32, threshold=7.991888999938965
  SimulatedQuantizeParams(in_scale=0.0006933134, out_scale=6.261835938659033e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%45] -> add[%46]
  bit=32, threshold=7.991888999938965
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.261835938659033e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%46] -> nn.relu[%47]
  bit=32, threshold=10.989872932434082
  SimulatedQuantizeParams(in_scale=6.261836e-09, out_scale=6.261835938659033e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%47] -> nn.conv2d[%49]
  bit=8, threshold=10.989872932434082
  SimulatedQuantizeParams(in_scale=6.261836e-09, out_scale=0.08585838228464127, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%48] -> nn.conv2d[%49]
  bit=8, threshold=0.7721172571182251
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006032166071236134, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%49] -> add[%51]
  bit=32, threshold=6.1439104080200195
  SimulatedQuantizeParams(in_scale=0.00051791203, out_scale=5.63654145580017e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%50] -> add[%51]
  bit=32, threshold=6.1439104080200195
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.63654145580017e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%51] -> nn.relu[%52]
  bit=32, threshold=13.117056846618652
  SimulatedQuantizeParams(in_scale=5.6365415e-09, out_scale=5.63654145580017e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%52] -> nn.conv2d[%54]
  bit=8, threshold=13.117056846618652
  SimulatedQuantizeParams(in_scale=5.6365415e-09, out_scale=0.10247700661420822, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%53] -> nn.conv2d[%54]
  bit=8, threshold=0.7478156685829163
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005842309910804033, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%54] -> add[%56]
  bit=32, threshold=3.0267882347106934
  SimulatedQuantizeParams(in_scale=0.0005987024, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%55] -> add[%56]
  bit=32, threshold=3.0267882347106934
  SimulatedQuantizeParams(in_scale=1.0, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%56] -> nn.relu[%57]
  bit=32, threshold=18.47281837463379
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%27] -> nn.avg_pool2d[%58]
  not quantized
  SimulatedQuantizeParams(in_scale=5.845587e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%58] -> nn.conv2d[%60]
  bit=8, threshold=7.913825988769531
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.06182676553726196, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%59] -> nn.conv2d[%60]
  bit=8, threshold=1.4154207706451416
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.011057974770665169, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%60] -> add[%62]
  bit=32, threshold=6.655673503875732
  SimulatedQuantizeParams(in_scale=0.00068367884, out_scale=5.4992748133031455e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%61] -> add[%62]
  bit=32, threshold=6.655673503875732
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.4992748133031455e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%62] -> nn.relu[%63]
  bit=32, threshold=7.329173564910889
  SimulatedQuantizeParams(in_scale=5.499275e-09, out_scale=5.4992748133031455e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%32] -> concatenate[%64]
  bit=32, threshold=7.051487922668457
  SimulatedQuantizeParams(in_scale=4.9410334e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%42] -> concatenate[%64]
  bit=32, threshold=7.051487922668457
  SimulatedQuantizeParams(in_scale=4.36374e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%57] -> concatenate[%64]
  bit=32, threshold=7.051487922668457
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%63] -> concatenate[%64]
  bit=32, threshold=7.051487922668457
  SimulatedQuantizeParams(in_scale=5.499275e-09, out_scale=9.001277945230868e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%64] -> nn.conv2d[%66]
  bit=8, threshold=18.47281837463379
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=0.14431889355182648, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%65] -> nn.conv2d[%66]
  bit=8, threshold=0.7486107349395752
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005848521366715431, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%66] -> add[%68]
  bit=32, threshold=4.264531135559082
  SimulatedQuantizeParams(in_scale=0.0008440521, out_scale=6.747602032675104e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%67] -> add[%68]
  bit=32, threshold=4.264531135559082
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.747602032675104e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%68] -> nn.relu[%69]
  bit=32, threshold=14.008376121520996
  SimulatedQuantizeParams(in_scale=6.747602e-09, out_scale=6.747602032675104e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%64] -> nn.conv2d[%71]
  bit=8, threshold=18.47281837463379
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=0.14431889355182648, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%70] -> nn.conv2d[%71]
  bit=8, threshold=0.5955356955528259
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0046526226215064526, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%71] -> add[%73]
  bit=32, threshold=6.930329322814941
  SimulatedQuantizeParams(in_scale=0.00067146134, out_scale=6.8322352220206994e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%72] -> add[%73]
  bit=32, threshold=6.930329322814941
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.8322352220206994e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%73] -> nn.relu[%74]
  bit=32, threshold=13.359471321105957
  SimulatedQuantizeParams(in_scale=6.832235e-09, out_scale=6.8322352220206994e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%74] -> nn.conv2d[%76]
  bit=8, threshold=13.359471321105957
  SimulatedQuantizeParams(in_scale=6.832235e-09, out_scale=0.10437086969614029, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%75] -> nn.conv2d[%76]
  bit=8, threshold=0.5041235089302063
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003938464913517237, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%76] -> add[%78]
  bit=32, threshold=4.1340012550354
  SimulatedQuantizeParams(in_scale=0.00041106102, out_scale=5.495253141418743e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%77] -> add[%78]
  bit=32, threshold=4.1340012550354
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.495253141418743e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%78] -> nn.relu[%79]
  bit=32, threshold=10.409193992614746
  SimulatedQuantizeParams(in_scale=5.495253e-09, out_scale=5.495253141418743e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%64] -> nn.conv2d[%81]
  bit=8, threshold=18.47281837463379
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=0.14431889355182648, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%80] -> nn.conv2d[%81]
  bit=8, threshold=0.6865484714508057
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005363659933209419, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%81] -> add[%83]
  bit=32, threshold=5.231709957122803
  SimulatedQuantizeParams(in_scale=0.0007740775, out_scale=5.624603893750191e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%82] -> add[%83]
  bit=32, threshold=5.231709957122803
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.624603893750191e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%83] -> nn.relu[%84]
  bit=32, threshold=8.374465942382812
  SimulatedQuantizeParams(in_scale=5.624604e-09, out_scale=5.624603893750191e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%84] -> nn.conv2d[%86]
  bit=8, threshold=8.374465942382812
  SimulatedQuantizeParams(in_scale=5.624604e-09, out_scale=0.06542551517486572, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%85] -> nn.conv2d[%86]
  bit=8, threshold=0.7145971059799194
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005582789890468121, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%86] -> add[%88]
  bit=32, threshold=4.646483421325684
  SimulatedQuantizeParams(in_scale=0.0003652569, out_scale=4.720007051872699e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%87] -> add[%88]
  bit=32, threshold=4.646483421325684
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.720007051872699e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%88] -> nn.relu[%89]
  bit=32, threshold=9.44502067565918
  SimulatedQuantizeParams(in_scale=4.720007e-09, out_scale=4.720007051872699e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%89] -> nn.conv2d[%91]
  bit=8, threshold=9.44502067565918
  SimulatedQuantizeParams(in_scale=4.720007e-09, out_scale=0.07378922402858734, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%90] -> nn.conv2d[%91]
  bit=8, threshold=1.188536524772644
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.009285441599786282, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%91] -> add[%93]
  bit=32, threshold=2.289642810821533
  SimulatedQuantizeParams(in_scale=0.0006851655, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%92] -> add[%93]
  bit=32, threshold=2.289642810821533
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%93] -> nn.relu[%94]
  bit=32, threshold=16.920352935791016
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%64] -> nn.avg_pool2d[%95]
  not quantized
  SimulatedQuantizeParams(in_scale=9.001278e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%95] -> nn.conv2d[%97]
  bit=8, threshold=11.094852447509766
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.08667853474617004, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%96] -> nn.conv2d[%97]
  bit=8, threshold=1.1952425241470337
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0093378322198987, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%97] -> add[%99]
  bit=32, threshold=4.416873931884766
  SimulatedQuantizeParams(in_scale=0.00080938963, out_scale=7.185416706079195e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%98] -> add[%99]
  bit=32, threshold=4.416873931884766
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.185416706079195e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%99] -> nn.relu[%100]
  bit=32, threshold=13.282967567443848
  SimulatedQuantizeParams(in_scale=7.1854167e-09, out_scale=7.185416706079195e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%69] -> concatenate[%101]
  bit=32, threshold=10.640891075134277
  SimulatedQuantizeParams(in_scale=6.747602e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%79] -> concatenate[%101]
  bit=32, threshold=10.640891075134277
  SimulatedQuantizeParams(in_scale=5.495253e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%94] -> concatenate[%101]
  bit=32, threshold=10.640891075134277
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%100] -> concatenate[%101]
  bit=32, threshold=10.640891075134277
  SimulatedQuantizeParams(in_scale=7.1854167e-09, out_scale=8.060402123533095e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%101] -> nn.conv2d[%103]
  bit=8, threshold=16.920352935791016
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=0.1321902573108673, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%102] -> nn.conv2d[%103]
  bit=8, threshold=1.0737723112106323
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008388846181333065, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%103] -> add[%105]
  bit=32, threshold=4.300724029541016
  SimulatedQuantizeParams(in_scale=0.0011089237, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%104] -> add[%105]
  bit=32, threshold=4.300724029541016
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%105] -> nn.relu[%106]
  bit=32, threshold=13.963201522827148
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%101] -> nn.conv2d[%108]
  bit=8, threshold=16.920352935791016
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=0.1321902573108673, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%107] -> nn.conv2d[%108]
  bit=8, threshold=0.6418415904045105
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005014387425035238, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%108] -> add[%110]
  bit=32, threshold=2.9343960285186768
  SimulatedQuantizeParams(in_scale=0.0006628532, out_scale=5.415036419265107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%109] -> add[%110]
  bit=32, threshold=2.9343960285186768
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.415036419265107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%110] -> nn.relu[%111]
  bit=32, threshold=10.764244079589844
  SimulatedQuantizeParams(in_scale=5.4150364e-09, out_scale=5.415036419265107e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%111] -> nn.conv2d[%113]
  bit=8, threshold=10.764244079589844
  SimulatedQuantizeParams(in_scale=5.4150364e-09, out_scale=0.08409565687179565, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%112] -> nn.conv2d[%113]
  bit=8, threshold=0.3671499788761139
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0028683592099696398, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%113] -> add[%115]
  bit=32, threshold=2.7083351612091064
  SimulatedQuantizeParams(in_scale=0.00024121655, out_scale=4.465147362964217e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%114] -> add[%115]
  bit=32, threshold=2.7083351612091064
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.465147362964217e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%115] -> nn.relu[%116]
  bit=32, threshold=9.77314281463623
  SimulatedQuantizeParams(in_scale=4.4651474e-09, out_scale=4.465147362964217e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%101] -> nn.conv2d[%118]
  bit=8, threshold=16.920352935791016
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=0.1321902573108673, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%117] -> nn.conv2d[%118]
  bit=8, threshold=0.8906435966491699
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00695815309882164, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%118] -> add[%120]
  bit=32, threshold=5.0458526611328125
  SimulatedQuantizeParams(in_scale=0.0009198001, out_scale=6.5467293808296745e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%119] -> add[%120]
  bit=32, threshold=5.0458526611328125
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.5467293808296745e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%120] -> nn.relu[%121]
  bit=32, threshold=12.8213472366333
  SimulatedQuantizeParams(in_scale=6.5467294e-09, out_scale=6.5467293808296745e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%121] -> nn.conv2d[%123]
  bit=8, threshold=12.8213472366333
  SimulatedQuantizeParams(in_scale=6.5467294e-09, out_scale=0.10016677528619766, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%122] -> nn.conv2d[%123]
  bit=8, threshold=0.6013656258583069
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0046981689520180225, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%123] -> add[%125]
  bit=32, threshold=2.11478590965271
  SimulatedQuantizeParams(in_scale=0.00047060044, out_scale=5.321578733230581e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%124] -> add[%125]
  bit=32, threshold=2.11478590965271
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.321578733230581e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%125] -> nn.relu[%126]
  bit=32, threshold=11.316946983337402
  SimulatedQuantizeParams(in_scale=5.3215787e-09, out_scale=5.321578733230581e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%126] -> nn.conv2d[%128]
  bit=8, threshold=11.316946983337402
  SimulatedQuantizeParams(in_scale=5.3215787e-09, out_scale=0.08841364830732346, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%127] -> nn.conv2d[%128]
  bit=8, threshold=0.6002355217933655
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004689340014010668, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%128] -> add[%130]
  bit=32, threshold=1.9522919654846191
  SimulatedQuantizeParams(in_scale=0.00041460167, out_scale=4.1297063546608115e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%129] -> add[%130]
  bit=32, threshold=1.9522919654846191
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.1297063546608115e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%130] -> nn.relu[%131]
  bit=32, threshold=8.879393577575684
  SimulatedQuantizeParams(in_scale=4.1297064e-09, out_scale=4.1297063546608115e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%101] -> nn.avg_pool2d[%132]
  not quantized
  SimulatedQuantizeParams(in_scale=8.060402e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%132] -> nn.conv2d[%134]
  bit=8, threshold=9.808819770812988
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.07663140445947647, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%133] -> nn.conv2d[%134]
  bit=8, threshold=1.4492331743240356
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.011322134174406528, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%134] -> add[%136]
  bit=32, threshold=4.774984836578369
  SimulatedQuantizeParams(in_scale=0.00086763105, out_scale=5.509801503933431e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%135] -> add[%136]
  bit=32, threshold=4.774984836578369
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.509801503933431e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%136] -> nn.relu[%137]
  bit=32, threshold=12.31700325012207
  SimulatedQuantizeParams(in_scale=5.5098015e-09, out_scale=5.509801503933431e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%106] -> concatenate[%138]
  bit=32, threshold=6.145162582397461
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%116] -> concatenate[%138]
  bit=32, threshold=6.145162582397461
  SimulatedQuantizeParams(in_scale=4.4651474e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%131] -> concatenate[%138]
  bit=32, threshold=6.145162582397461
  SimulatedQuantizeParams(in_scale=4.1297064e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%137] -> concatenate[%138]
  bit=32, threshold=6.145162582397461
  SimulatedQuantizeParams(in_scale=5.5098015e-09, out_scale=7.610678309788454e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%138] -> nn.conv2d[%140]
  bit=8, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=0.08277589082717896, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%139] -> nn.conv2d[%140]
  bit=8, threshold=0.509999692440033
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0039843725971877575, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%140] -> add[%142]
  bit=32, threshold=4.899190902709961
  SimulatedQuantizeParams(in_scale=0.00032981, out_scale=4.909536777120138e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%141] -> add[%142]
  bit=32, threshold=4.899190902709961
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.909536777120138e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%142] -> nn.relu[%143]
  bit=32, threshold=8.520537376403809
  SimulatedQuantizeParams(in_scale=4.9095368e-09, out_scale=4.909536777120138e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%138] -> nn.conv2d[%145]
  bit=8, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=0.08277589082717896, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%144] -> nn.conv2d[%145]
  bit=8, threshold=0.665273904800415
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0051974523812532425, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%145] -> add[%147]
  bit=32, threshold=3.554874897003174
  SimulatedQuantizeParams(in_scale=0.00043022376, out_scale=5.608436381976389e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%146] -> add[%147]
  bit=32, threshold=3.554874897003174
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.608436381976389e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%147] -> nn.relu[%148]
  bit=32, threshold=10.223426818847656
  SimulatedQuantizeParams(in_scale=5.6084364e-09, out_scale=5.608436381976389e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%148] -> nn.conv2d[%150]
  bit=8, threshold=10.223426818847656
  SimulatedQuantizeParams(in_scale=5.6084364e-09, out_scale=0.07987052202224731, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%149] -> nn.conv2d[%150]
  bit=8, threshold=0.5947378873825073
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0046463897451758385, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%150] -> add[%152]
  bit=32, threshold=3.0386290550231934
  SimulatedQuantizeParams(in_scale=0.00037110958, out_scale=4.9960182657571295e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%151] -> add[%152]
  bit=32, threshold=3.0386290550231934
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.9960182657571295e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%152] -> nn.relu[%153]
  bit=32, threshold=10.034647941589355
  SimulatedQuantizeParams(in_scale=4.9960183e-09, out_scale=4.9960182657571295e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%153] -> nn.conv2d[%155]
  bit=8, threshold=10.034647941589355
  SimulatedQuantizeParams(in_scale=4.9960183e-09, out_scale=0.07839568704366684, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%154] -> nn.conv2d[%155]
  bit=8, threshold=0.5644394755363464
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0044096834026277065, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%155] -> add[%157]
  bit=32, threshold=1.423962950706482
  SimulatedQuantizeParams(in_scale=0.00034570016, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%156] -> add[%157]
  bit=32, threshold=1.423962950706482
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%157] -> nn.relu[%158]
  bit=32, threshold=12.267464637756348
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%138] -> nn.max_pool2d[%159]
  bit=32, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=7.610678e-09, out_scale=4.933827568720517e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%143] -> concatenate[%160]
  bit=32, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=4.9095368e-09, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%158] -> concatenate[%160]
  bit=32, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%159] -> concatenate[%160]
  bit=32, threshold=10.595314025878906
  SimulatedQuantizeParams(in_scale=4.9338276e-09, out_scale=5.874207342770887e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%160] -> nn.conv2d[%162]
  bit=8, threshold=12.267464637756348
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=0.09583956748247147, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%161] -> nn.conv2d[%162]
  bit=8, threshold=0.6881512999534607
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005376182030886412, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%162] -> add[%164]
  bit=32, threshold=3.3307907581329346
  SimulatedQuantizeParams(in_scale=0.00051525095, out_scale=4.888048188433913e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%163] -> add[%164]
  bit=32, threshold=3.3307907581329346
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.888048188433913e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%164] -> nn.relu[%165]
  bit=32, threshold=9.8707914352417
  SimulatedQuantizeParams(in_scale=4.888048e-09, out_scale=4.888048188433913e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%160] -> nn.conv2d[%167]
  bit=8, threshold=12.267464637756348
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=0.09583956748247147, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%166] -> nn.conv2d[%167]
  bit=8, threshold=0.781169056892395
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006102883256971836, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%167] -> add[%169]
  bit=32, threshold=2.5749738216400146
  SimulatedQuantizeParams(in_scale=0.0005848977, out_scale=5.253976809171945e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%168] -> add[%169]
  bit=32, threshold=2.5749738216400146
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.253976809171945e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%169] -> nn.relu[%170]
  bit=32, threshold=10.19361686706543
  SimulatedQuantizeParams(in_scale=5.253977e-09, out_scale=5.253976809171945e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%170] -> nn.conv2d[%172]
  bit=8, threshold=10.19361686706543
  SimulatedQuantizeParams(in_scale=5.253977e-09, out_scale=0.07963763177394867, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%171] -> nn.conv2d[%172]
  bit=8, threshold=0.9599223732948303
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007499393541365862, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%172] -> add[%174]
  bit=32, threshold=1.9074257612228394
  SimulatedQuantizeParams(in_scale=0.00059723394, out_scale=5.06825514889897e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%173] -> add[%174]
  bit=32, threshold=1.9074257612228394
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.06825514889897e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%174] -> nn.relu[%175]
  bit=32, threshold=10.738835334777832
  SimulatedQuantizeParams(in_scale=5.068255e-09, out_scale=5.06825514889897e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%175] -> nn.conv2d[%177]
  bit=8, threshold=10.738835334777832
  SimulatedQuantizeParams(in_scale=5.068255e-09, out_scale=0.08389715105295181, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%176] -> nn.conv2d[%177]
  bit=8, threshold=0.7903327941894531
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0061744749546051025, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%177] -> add[%179]
  bit=32, threshold=2.1650309562683105
  SimulatedQuantizeParams(in_scale=0.0005180209, out_scale=4.835690958771011e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%178] -> add[%179]
  bit=32, threshold=2.1650309562683105
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.835690958771011e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%179] -> nn.relu[%180]
  bit=32, threshold=9.735280990600586
  SimulatedQuantizeParams(in_scale=4.835691e-09, out_scale=4.835690958771011e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%160] -> nn.conv2d[%182]
  bit=8, threshold=12.267464637756348
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=0.09583956748247147, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%181] -> nn.conv2d[%182]
  bit=8, threshold=0.600529134273529
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004691633861511946, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%182] -> add[%184]
  bit=32, threshold=2.64369535446167
  SimulatedQuantizeParams(in_scale=0.00044964417, out_scale=4.69497507538108e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%183] -> add[%184]
  bit=32, threshold=2.64369535446167
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.69497507538108e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%184] -> nn.relu[%185]
  bit=32, threshold=8.269901275634766
  SimulatedQuantizeParams(in_scale=4.694975e-09, out_scale=4.69497507538108e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%185] -> nn.conv2d[%187]
  bit=8, threshold=8.269901275634766
  SimulatedQuantizeParams(in_scale=4.694975e-09, out_scale=0.0646086037158966, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%186] -> nn.conv2d[%187]
  bit=8, threshold=0.6314414739608765
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004933136515319347, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%187] -> add[%189]
  bit=32, threshold=2.755542755126953
  SimulatedQuantizeParams(in_scale=0.00031872306, out_scale=3.926667879738943e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%188] -> add[%189]
  bit=32, threshold=2.755542755126953
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.926667879738943e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%189] -> nn.relu[%190]
  bit=32, threshold=7.066683769226074
  SimulatedQuantizeParams(in_scale=3.926668e-09, out_scale=3.926667879738943e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%190] -> nn.conv2d[%192]
  bit=8, threshold=7.066683769226074
  SimulatedQuantizeParams(in_scale=3.926668e-09, out_scale=0.055208466947078705, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%191] -> nn.conv2d[%192]
  bit=8, threshold=0.7650089859962463
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0059766327030956745, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%192] -> add[%194]
  bit=32, threshold=2.237175703048706
  SimulatedQuantizeParams(in_scale=0.00032996072, out_scale=4.005986653510263e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%193] -> add[%194]
  bit=32, threshold=2.237175703048706
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.005986653510263e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%194] -> nn.relu[%195]
  bit=32, threshold=8.079626083374023
  SimulatedQuantizeParams(in_scale=4.0059867e-09, out_scale=4.005986653510263e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%195] -> nn.conv2d[%197]
  bit=8, threshold=8.079626083374023
  SimulatedQuantizeParams(in_scale=4.0059867e-09, out_scale=0.06312207877635956, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%196] -> nn.conv2d[%197]
  bit=8, threshold=0.7510474324226379
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005867558065801859, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%197] -> add[%199]
  bit=32, threshold=2.471312999725342
  SimulatedQuantizeParams(in_scale=0.00037037247, out_scale=5.843741934796753e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%198] -> add[%199]
  bit=32, threshold=2.471312999725342
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.843741934796753e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%199] -> nn.relu[%200]
  bit=32, threshold=12.33205509185791
  SimulatedQuantizeParams(in_scale=5.843742e-09, out_scale=5.843741934796753e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%200] -> nn.conv2d[%202]
  bit=8, threshold=12.33205509185791
  SimulatedQuantizeParams(in_scale=5.843742e-09, out_scale=0.09634418040513992, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%201] -> nn.conv2d[%202]
  bit=8, threshold=0.5172447562217712
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004040974657982588, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%202] -> add[%204]
  bit=32, threshold=2.7970874309539795
  SimulatedQuantizeParams(in_scale=0.00038932438, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%203] -> add[%204]
  bit=32, threshold=2.7970874309539795
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%204] -> nn.relu[%205]
  bit=32, threshold=19.085237503051758
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%160] -> nn.avg_pool2d[%206]
  not quantized
  SimulatedQuantizeParams(in_scale=5.8742073e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%206] -> nn.conv2d[%208]
  bit=8, threshold=7.356801986694336
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0574750155210495, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%207] -> nn.conv2d[%208]
  bit=8, threshold=1.4476805925369263
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.011310004629194736, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%208] -> add[%210]
  bit=32, threshold=4.6358513832092285
  SimulatedQuantizeParams(in_scale=0.00065004267, out_scale=4.577606738109807e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%209] -> add[%210]
  bit=32, threshold=4.6358513832092285
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.577606738109807e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%210] -> nn.relu[%211]
  bit=32, threshold=8.537124633789062
  SimulatedQuantizeParams(in_scale=4.5776067e-09, out_scale=4.577606738109807e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%165] -> concatenate[%212]
  bit=32, threshold=6.11529541015625
  SimulatedQuantizeParams(in_scale=4.888048e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%180] -> concatenate[%212]
  bit=32, threshold=6.11529541015625
  SimulatedQuantizeParams(in_scale=4.835691e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%205] -> concatenate[%212]
  bit=32, threshold=6.11529541015625
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%211] -> concatenate[%212]
  bit=32, threshold=6.11529541015625
  SimulatedQuantizeParams(in_scale=4.5776067e-09, out_scale=8.91949714088014e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%212] -> nn.conv2d[%214]
  bit=8, threshold=19.085237503051758
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=0.14910341799259186, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%213] -> nn.conv2d[%214]
  bit=8, threshold=0.679200291633606
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0053062522783875465, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%214] -> add[%216]
  bit=32, threshold=2.1934781074523926
  SimulatedQuantizeParams(in_scale=0.00079118036, out_scale=5.7101687822580516e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%215] -> add[%216]
  bit=32, threshold=2.1934781074523926
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.7101687822580516e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%216] -> nn.relu[%217]
  bit=32, threshold=11.482173919677734
  SimulatedQuantizeParams(in_scale=5.710169e-09, out_scale=5.7101687822580516e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%212] -> nn.conv2d[%219]
  bit=8, threshold=19.085237503051758
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=0.14910341799259186, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%218] -> nn.conv2d[%219]
  bit=8, threshold=0.959825873374939
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007498639635741711, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%219] -> add[%221]
  bit=32, threshold=1.8335822820663452
  SimulatedQuantizeParams(in_scale=0.0011180728, out_scale=6.199324609212908e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%220] -> add[%221]
  bit=32, threshold=1.8335822820663452
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.199324609212908e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%221] -> nn.relu[%222]
  bit=32, threshold=13.235675811767578
  SimulatedQuantizeParams(in_scale=6.1993246e-09, out_scale=6.199324609212908e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%222] -> nn.conv2d[%224]
  bit=8, threshold=13.235675811767578
  SimulatedQuantizeParams(in_scale=6.1993246e-09, out_scale=0.1034037172794342, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%223] -> nn.conv2d[%224]
  bit=8, threshold=0.6195498704910278
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004840233363211155, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%224] -> add[%226]
  bit=32, threshold=1.5446676015853882
  SimulatedQuantizeParams(in_scale=0.0005004981, out_scale=4.4167194346300676e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%225] -> add[%226]
  bit=32, threshold=1.5446676015853882
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.4167194346300676e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%226] -> nn.relu[%227]
  bit=32, threshold=9.890748977661133
  SimulatedQuantizeParams(in_scale=4.4167194e-09, out_scale=4.4167194346300676e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%227] -> nn.conv2d[%229]
  bit=8, threshold=9.890748977661133
  SimulatedQuantizeParams(in_scale=4.4167194e-09, out_scale=0.0772714763879776, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%228] -> nn.conv2d[%229]
  bit=8, threshold=0.7080584168434143
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005531706381589174, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%229] -> add[%231]
  bit=32, threshold=1.8019605875015259
  SimulatedQuantizeParams(in_scale=0.00042744313, out_scale=4.5321599806413815e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%230] -> add[%231]
  bit=32, threshold=1.8019605875015259
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.5321599806413815e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%231] -> nn.relu[%232]
  bit=32, threshold=8.958961486816406
  SimulatedQuantizeParams(in_scale=4.53216e-09, out_scale=4.5321599806413815e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%212] -> nn.conv2d[%234]
  bit=8, threshold=19.085237503051758
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=0.14910341799259186, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%233] -> nn.conv2d[%234]
  bit=8, threshold=0.8844839930534363
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006910031195729971, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%234] -> add[%236]
  bit=32, threshold=3.312314510345459
  SimulatedQuantizeParams(in_scale=0.0010303092, out_scale=5.28766719298801e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%235] -> add[%236]
  bit=32, threshold=3.312314510345459
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.28766719298801e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%236] -> nn.relu[%237]
  bit=32, threshold=10.339098930358887
  SimulatedQuantizeParams(in_scale=5.287667e-09, out_scale=5.28766719298801e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%237] -> nn.conv2d[%239]
  bit=8, threshold=10.339098930358887
  SimulatedQuantizeParams(in_scale=5.287667e-09, out_scale=0.0807742103934288, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%238] -> nn.conv2d[%239]
  bit=8, threshold=0.5185532569885254
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004051197320222855, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%239] -> add[%241]
  bit=32, threshold=1.4913455247879028
  SimulatedQuantizeParams(in_scale=0.00032723226, out_scale=5.363597566088174e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%240] -> add[%241]
  bit=32, threshold=1.4913455247879028
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.363597566088174e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%241] -> nn.relu[%242]
  bit=32, threshold=11.444397926330566
  SimulatedQuantizeParams(in_scale=5.3635976e-09, out_scale=5.363597566088174e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%242] -> nn.conv2d[%244]
  bit=8, threshold=11.444397926330566
  SimulatedQuantizeParams(in_scale=5.3635976e-09, out_scale=0.08940935879945755, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%243] -> nn.conv2d[%244]
  bit=8, threshold=0.7107781767845154
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005552954506129026, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%244] -> add[%246]
  bit=32, threshold=2.3764243125915527
  SimulatedQuantizeParams(in_scale=0.0004964861, out_scale=7.196979900925271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%245] -> add[%246]
  bit=32, threshold=2.3764243125915527
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.196979900925271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%246] -> nn.relu[%247]
  bit=32, threshold=14.238655090332031
  SimulatedQuantizeParams(in_scale=7.19698e-09, out_scale=7.196979900925271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%247] -> nn.conv2d[%249]
  bit=8, threshold=14.238655090332031
  SimulatedQuantizeParams(in_scale=7.19698e-09, out_scale=0.111239492893219, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%248] -> nn.conv2d[%249]
  bit=8, threshold=0.6394439339637756
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004995655734091997, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%249] -> add[%251]
  bit=32, threshold=2.3433687686920166
  SimulatedQuantizeParams(in_scale=0.0005557142, out_scale=1.0033215147586816e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%250] -> add[%251]
  bit=32, threshold=2.3433687686920166
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0033215147586816e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%251] -> nn.relu[%252]
  bit=32, threshold=20.109031677246094
  SimulatedQuantizeParams(in_scale=1.0033215e-08, out_scale=1.0033215147586816e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%252] -> nn.conv2d[%254]
  bit=8, threshold=20.109031677246094
  SimulatedQuantizeParams(in_scale=1.0033215e-08, out_scale=0.1571018099784851, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%253] -> nn.conv2d[%254]
  bit=8, threshold=0.6675122380256653
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.00521493935957551, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%254] -> add[%256]
  bit=32, threshold=3.320685386657715
  SimulatedQuantizeParams(in_scale=0.00081927644, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%255] -> add[%256]
  bit=32, threshold=3.320685386657715
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%256] -> nn.relu[%257]
  bit=32, threshold=22.33852767944336
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%212] -> nn.avg_pool2d[%258]
  not quantized
  SimulatedQuantizeParams(in_scale=8.919497e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%258] -> nn.conv2d[%260]
  bit=8, threshold=5.780173301696777
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.04515760391950607, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%259] -> nn.conv2d[%260]
  bit=8, threshold=0.9621531963348389
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007516821846365929, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%260] -> add[%262]
  bit=32, threshold=2.7727677822113037
  SimulatedQuantizeParams(in_scale=0.00033944167, out_scale=3.8256802170621995e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%261] -> add[%262]
  bit=32, threshold=2.7727677822113037
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.8256802170621995e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%262] -> nn.relu[%263]
  bit=32, threshold=8.354458808898926
  SimulatedQuantizeParams(in_scale=3.82568e-09, out_scale=3.8256802170621995e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%217] -> concatenate[%264]
  bit=32, threshold=6.464248180389404
  SimulatedQuantizeParams(in_scale=5.710169e-09, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%232] -> concatenate[%264]
  bit=32, threshold=6.464248180389404
  SimulatedQuantizeParams(in_scale=4.53216e-09, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%257] -> concatenate[%264]
  bit=32, threshold=6.464248180389404
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%263] -> concatenate[%264]
  bit=32, threshold=6.464248180389404
  SimulatedQuantizeParams(in_scale=3.82568e-09, out_scale=1.0570887276628582e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%264] -> nn.conv2d[%266]
  bit=8, threshold=22.33852767944336
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=0.17451974749565125, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%265] -> nn.conv2d[%266]
  bit=8, threshold=1.1758604049682617
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.009186409413814545, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%266] -> add[%268]
  bit=32, threshold=1.8875170946121216
  SimulatedQuantizeParams(in_scale=0.0016032099, out_scale=6.290805210085182e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%267] -> add[%268]
  bit=32, threshold=1.8875170946121216
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.290805210085182e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%268] -> nn.relu[%269]
  bit=32, threshold=13.82950210571289
  SimulatedQuantizeParams(in_scale=6.290805e-09, out_scale=6.290805210085182e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%264] -> nn.conv2d[%271]
  bit=8, threshold=22.33852767944336
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=0.17451974749565125, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%270] -> nn.conv2d[%271]
  bit=8, threshold=0.5845543742179871
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004566831048578024, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%271] -> add[%273]
  bit=32, threshold=2.8225395679473877
  SimulatedQuantizeParams(in_scale=0.0007970022, out_scale=6.896744064732729e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%272] -> add[%273]
  bit=32, threshold=2.8225395679473877
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.896744064732729e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%273] -> nn.relu[%274]
  bit=32, threshold=12.803604125976562
  SimulatedQuantizeParams(in_scale=6.896744e-09, out_scale=6.896744064732729e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%274] -> nn.conv2d[%276]
  bit=8, threshold=12.803604125976562
  SimulatedQuantizeParams(in_scale=6.896744e-09, out_scale=0.1000281572341919, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%275] -> nn.conv2d[%276]
  bit=8, threshold=1.127054214477539
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008805111050605774, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%276] -> add[%278]
  bit=32, threshold=1.5113341808319092
  SimulatedQuantizeParams(in_scale=0.00088075903, out_scale=7.318159411795477e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%277] -> add[%278]
  bit=32, threshold=1.5113341808319092
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.318159411795477e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%278] -> nn.relu[%279]
  bit=32, threshold=15.453755378723145
  SimulatedQuantizeParams(in_scale=7.3181594e-09, out_scale=7.318159411795477e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%279] -> nn.conv2d[%281]
  bit=8, threshold=15.453755378723145
  SimulatedQuantizeParams(in_scale=7.3181594e-09, out_scale=0.12073246389627457, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%280] -> nn.conv2d[%281]
  bit=8, threshold=0.6649886965751648
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005195224191993475, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%281] -> add[%283]
  bit=32, threshold=2.850898504257202
  SimulatedQuantizeParams(in_scale=0.0006272322, out_scale=6.16979045631183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%282] -> add[%283]
  bit=32, threshold=2.850898504257202
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.16979045631183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%283] -> nn.relu[%284]
  bit=32, threshold=13.102632522583008
  SimulatedQuantizeParams(in_scale=6.1697905e-09, out_scale=6.16979045631183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%264] -> nn.conv2d[%286]
  bit=8, threshold=22.33852767944336
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=0.17451974749565125, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%285] -> nn.conv2d[%286]
  bit=8, threshold=0.8486541509628296
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006630110554397106, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%286] -> add[%288]
  bit=32, threshold=2.2719626426696777
  SimulatedQuantizeParams(in_scale=0.0011570852, out_scale=7.916712618794008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%287] -> add[%288]
  bit=32, threshold=2.2719626426696777
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.916712618794008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%288] -> nn.relu[%289]
  bit=32, threshold=15.889891624450684
  SimulatedQuantizeParams(in_scale=7.916713e-09, out_scale=7.916712618794008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%289] -> nn.conv2d[%291]
  bit=8, threshold=15.889891624450684
  SimulatedQuantizeParams(in_scale=7.916713e-09, out_scale=0.12413977831602097, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%290] -> nn.conv2d[%291]
  bit=8, threshold=0.9434528350830078
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0073707252740859985, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%291] -> add[%293]
  bit=32, threshold=1.8475465774536133
  SimulatedQuantizeParams(in_scale=0.0009150002, out_scale=7.64214647119843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%292] -> add[%293]
  bit=32, threshold=1.8475465774536133
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.64214647119843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%293] -> nn.relu[%294]
  bit=32, threshold=16.72710609436035
  SimulatedQuantizeParams(in_scale=7.6421465e-09, out_scale=7.64214647119843e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%294] -> nn.conv2d[%296]
  bit=8, threshold=16.72710609436035
  SimulatedQuantizeParams(in_scale=7.6421465e-09, out_scale=0.13068051636219025, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%295] -> nn.conv2d[%296]
  bit=8, threshold=0.6284889578819275
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0049100699834525585, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%296] -> add[%298]
  bit=32, threshold=2.077789545059204
  SimulatedQuantizeParams(in_scale=0.00064165046, out_scale=5.389896529095495e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%297] -> add[%298]
  bit=32, threshold=2.077789545059204
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.389896529095495e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%298] -> nn.relu[%299]
  bit=32, threshold=11.313255310058594
  SimulatedQuantizeParams(in_scale=5.3898965e-09, out_scale=5.389896529095495e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%299] -> nn.conv2d[%301]
  bit=8, threshold=11.313255310058594
  SimulatedQuantizeParams(in_scale=5.3898965e-09, out_scale=0.08838480710983276, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%300] -> nn.conv2d[%301]
  bit=8, threshold=0.9571489095687866
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0074777258560061455, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%301] -> add[%303]
  bit=32, threshold=1.308233618736267
  SimulatedQuantizeParams(in_scale=0.0006609174, out_scale=7.242821009612044e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%302] -> add[%303]
  bit=32, threshold=1.308233618736267
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.242821009612044e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%303] -> nn.relu[%304]
  bit=32, threshold=16.044204711914062
  SimulatedQuantizeParams(in_scale=7.242821e-09, out_scale=7.242821009612044e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%304] -> nn.conv2d[%306]
  bit=8, threshold=11.781047821044922
  SimulatedQuantizeParams(in_scale=7.242821e-09, out_scale=0.09203943610191345, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%305] -> nn.conv2d[%306]
  bit=8, threshold=0.9496046900749207
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007418786641210318, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%306] -> add[%308]
  bit=32, threshold=2.860248327255249
  SimulatedQuantizeParams(in_scale=0.0006828209, out_scale=5.331318497781012e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%307] -> add[%308]
  bit=32, threshold=2.860248327255249
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.331318497781012e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%308] -> nn.relu[%309]
  bit=32, threshold=10.766971588134766
  SimulatedQuantizeParams(in_scale=5.3313185e-09, out_scale=5.331318497781012e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%264] -> nn.avg_pool2d[%310]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0570887e-08, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%310] -> nn.conv2d[%312]
  bit=8, threshold=6.167629241943359
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.048184603452682495, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%311] -> nn.conv2d[%312]
  bit=8, threshold=1.790992259979248
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.013992127031087875, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%312] -> add[%314]
  bit=32, threshold=1.1869275569915771
  SimulatedQuantizeParams(in_scale=0.0006742051, out_scale=3.861404529459378e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%313] -> add[%314]
  bit=32, threshold=1.1869275569915771
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.861404529459378e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%314] -> nn.relu[%315]
  bit=32, threshold=7.954532623291016
  SimulatedQuantizeParams(in_scale=3.8614045e-09, out_scale=3.861404529459378e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%269] -> concatenate[%316]
  bit=32, threshold=5.845179557800293
  SimulatedQuantizeParams(in_scale=6.290805e-09, out_scale=6.4398637533713554e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%284] -> concatenate[%316]
  bit=32, threshold=5.845179557800293
  SimulatedQuantizeParams(in_scale=6.1697905e-09, out_scale=6.4398637533713554e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%309] -> concatenate[%316]
  bit=32, threshold=5.845179557800293
  SimulatedQuantizeParams(in_scale=5.3313185e-09, out_scale=6.4398637533713554e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%315] -> concatenate[%316]
  bit=32, threshold=5.845179557800293
  SimulatedQuantizeParams(in_scale=3.8614045e-09, out_scale=6.4398637533713554e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%316] -> nn.conv2d[%318]
  bit=8, threshold=13.93353271484375
  SimulatedQuantizeParams(in_scale=6.4398638e-09, out_scale=0.1088557243347168, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%317] -> nn.conv2d[%318]
  bit=8, threshold=0.986774742603302
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007709177676588297, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%318] -> add[%320]
  bit=32, threshold=0.8015092611312866
  SimulatedQuantizeParams(in_scale=0.0008391881, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%319] -> add[%320]
  bit=32, threshold=0.8015092611312866
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%320] -> nn.relu[%321]
  bit=32, threshold=15.364890098571777
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%316] -> nn.conv2d[%323]
  bit=8, threshold=13.93353271484375
  SimulatedQuantizeParams(in_scale=6.4398638e-09, out_scale=0.1088557243347168, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%322] -> nn.conv2d[%323]
  bit=8, threshold=0.9906911849975586
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0077397748827934265, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%323] -> add[%325]
  bit=32, threshold=1.167878270149231
  SimulatedQuantizeParams(in_scale=0.0008425188, out_scale=5.593841834183877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%324] -> add[%325]
  bit=32, threshold=1.167878270149231
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.593841834183877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%325] -> nn.relu[%326]
  bit=32, threshold=11.638570785522461
  SimulatedQuantizeParams(in_scale=5.593842e-09, out_scale=5.593841834183877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%326] -> nn.conv2d[%328]
  bit=8, threshold=11.638570785522461
  SimulatedQuantizeParams(in_scale=5.593842e-09, out_scale=0.09092633426189423, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%327] -> nn.conv2d[%328]
  bit=8, threshold=1.0864348411560059
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008487772196531296, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%328] -> add[%330]
  bit=32, threshold=1.434431552886963
  SimulatedQuantizeParams(in_scale=0.000771762, out_scale=6.458874324266617e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%329] -> add[%330]
  bit=32, threshold=1.434431552886963
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.458874324266617e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%330] -> nn.relu[%331]
  bit=32, threshold=14.039323806762695
  SimulatedQuantizeParams(in_scale=6.4588743e-09, out_scale=6.458874324266617e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%331] -> nn.conv2d[%333]
  bit=8, threshold=14.039323806762695
  SimulatedQuantizeParams(in_scale=6.4588743e-09, out_scale=0.10968221724033356, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%332] -> nn.conv2d[%333]
  bit=8, threshold=0.3395962715148926
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0026530958712100983, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%333] -> add[%335]
  bit=32, threshold=2.9625234603881836
  SimulatedQuantizeParams(in_scale=0.00029099744, out_scale=4.602799918984601e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%334] -> add[%335]
  bit=32, threshold=2.9625234603881836
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.602799918984601e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%335] -> nn.relu[%336]
  bit=32, threshold=10.09786319732666
  SimulatedQuantizeParams(in_scale=4.6028e-09, out_scale=4.602799918984601e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%316] -> nn.conv2d[%338]
  bit=8, threshold=13.93353271484375
  SimulatedQuantizeParams(in_scale=6.4398638e-09, out_scale=0.1088557243347168, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%337] -> nn.conv2d[%338]
  bit=8, threshold=0.6093745827674866
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004760738927870989, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%338] -> add[%340]
  bit=32, threshold=2.3437814712524414
  SimulatedQuantizeParams(in_scale=0.0005182337, out_scale=6.269122554414253e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%339] -> add[%340]
  bit=32, threshold=2.3437814712524414
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.269122554414253e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%340] -> nn.relu[%341]
  bit=32, threshold=12.59747314453125
  SimulatedQuantizeParams(in_scale=6.2691226e-09, out_scale=6.269122554414253e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%341] -> nn.conv2d[%343]
  bit=8, threshold=12.59747314453125
  SimulatedQuantizeParams(in_scale=6.2691226e-09, out_scale=0.09841775894165039, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%342] -> nn.conv2d[%343]
  bit=8, threshold=0.5574187636375427
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0043548340909183025, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%343] -> add[%345]
  bit=32, threshold=1.5553131103515625
  SimulatedQuantizeParams(in_scale=0.00042859302, out_scale=5.721004114889183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%344] -> add[%345]
  bit=32, threshold=1.5553131103515625
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.721004114889183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%345] -> nn.relu[%346]
  bit=32, threshold=12.173563957214355
  SimulatedQuantizeParams(in_scale=5.721004e-09, out_scale=5.721004114889183e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%346] -> nn.conv2d[%348]
  bit=8, threshold=12.173563957214355
  SimulatedQuantizeParams(in_scale=5.721004e-09, out_scale=0.09510596841573715, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%347] -> nn.conv2d[%348]
  bit=8, threshold=0.6530459523200989
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0051019215025007725, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%348] -> add[%350]
  bit=32, threshold=1.9077832698822021
  SimulatedQuantizeParams(in_scale=0.0004852232, out_scale=6.055622669975946e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%349] -> add[%350]
  bit=32, threshold=1.9077832698822021
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.055622669975946e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%350] -> nn.relu[%351]
  bit=32, threshold=13.047298431396484
  SimulatedQuantizeParams(in_scale=6.0556227e-09, out_scale=6.055622669975946e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%351] -> nn.conv2d[%353]
  bit=8, threshold=13.047298431396484
  SimulatedQuantizeParams(in_scale=6.0556227e-09, out_scale=0.10193201899528503, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%352] -> nn.conv2d[%353]
  bit=8, threshold=0.4091434180736542
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0031964329537004232, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%353] -> add[%355]
  bit=32, threshold=2.121026039123535
  SimulatedQuantizeParams(in_scale=0.00032581887, out_scale=6.2256906296909165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%354] -> add[%355]
  bit=32, threshold=2.121026039123535
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.2256906296909165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%355] -> nn.relu[%356]
  bit=32, threshold=13.940378189086914
  SimulatedQuantizeParams(in_scale=6.2256906e-09, out_scale=6.2256906296909165e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%356] -> nn.conv2d[%358]
  bit=8, threshold=13.940378189086914
  SimulatedQuantizeParams(in_scale=6.2256906e-09, out_scale=0.10890920460224152, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%357] -> nn.conv2d[%358]
  bit=8, threshold=0.3439410924911499
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0026870397850871086, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%358] -> add[%360]
  bit=32, threshold=1.6823570728302002
  SimulatedQuantizeParams(in_scale=0.00029264335, out_scale=4.0570169446141335e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%359] -> add[%360]
  bit=32, threshold=1.6823570728302002
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.0570169446141335e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%360] -> nn.relu[%361]
  bit=32, threshold=8.996960639953613
  SimulatedQuantizeParams(in_scale=4.057017e-09, out_scale=4.0570169446141335e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%316] -> nn.avg_pool2d[%362]
  not quantized
  SimulatedQuantizeParams(in_scale=6.4398638e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%362] -> nn.conv2d[%364]
  bit=8, threshold=6.527836799621582
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.05099872499704361, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%363] -> nn.conv2d[%364]
  bit=8, threshold=0.9189664721488953
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007179425563663244, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%364] -> add[%366]
  bit=32, threshold=0.9521695375442505
  SimulatedQuantizeParams(in_scale=0.00036614155, out_scale=3.3753286832194362e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%365] -> add[%366]
  bit=32, threshold=0.9521695375442505
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.3753286832194362e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%366] -> nn.relu[%367]
  bit=32, threshold=7.16987419128418
  SimulatedQuantizeParams(in_scale=3.3753287e-09, out_scale=3.3753286832194362e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%321] -> concatenate[%368]
  bit=32, threshold=4.94903039932251
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%336] -> concatenate[%368]
  bit=32, threshold=4.94903039932251
  SimulatedQuantizeParams(in_scale=4.6028e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%361] -> concatenate[%368]
  bit=32, threshold=4.94903039932251
  SimulatedQuantizeParams(in_scale=4.057017e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%367] -> concatenate[%368]
  bit=32, threshold=4.94903039932251
  SimulatedQuantizeParams(in_scale=3.3753287e-09, out_scale=7.196650830820772e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%368] -> nn.conv2d[%370]
  bit=8, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=0.06958131492137909, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%369] -> nn.conv2d[%370]
  bit=8, threshold=0.8908913135528564
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006960088387131691, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%370] -> add[%372]
  bit=32, threshold=2.821382999420166
  SimulatedQuantizeParams(in_scale=0.0004842921, out_scale=6.749371284087147e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%371] -> add[%372]
  bit=32, threshold=2.821382999420166
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.749371284087147e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%372] -> nn.relu[%373]
  bit=32, threshold=14.85698127746582
  SimulatedQuantizeParams(in_scale=6.7493713e-09, out_scale=6.749371284087147e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%373] -> nn.conv2d[%375]
  bit=8, threshold=11.869260787963867
  SimulatedQuantizeParams(in_scale=6.7493713e-09, out_scale=0.09272859990596771, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%374] -> nn.conv2d[%375]
  bit=8, threshold=0.6321445107460022
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.004938628990203142, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%375] -> add[%377]
  bit=32, threshold=3.4323699474334717
  SimulatedQuantizeParams(in_scale=0.00045795215, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%376] -> add[%377]
  bit=32, threshold=3.4323699474334717
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%377] -> nn.relu[%378]
  bit=32, threshold=16.574100494384766
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%368] -> nn.conv2d[%380]
  bit=8, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=0.06958131492137909, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%379] -> nn.conv2d[%380]
  bit=8, threshold=1.2494585514068604
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.009761394932866096, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%380] -> add[%382]
  bit=32, threshold=2.9126672744750977
  SimulatedQuantizeParams(in_scale=0.0006792107, out_scale=6.758319237576416e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%381] -> add[%382]
  bit=32, threshold=2.9126672744750977
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.758319237576416e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%382] -> nn.relu[%383]
  bit=32, threshold=14.695056915283203
  SimulatedQuantizeParams(in_scale=6.7583192e-09, out_scale=6.758319237576416e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%383] -> nn.conv2d[%385]
  bit=8, threshold=12.045427322387695
  SimulatedQuantizeParams(in_scale=6.7583192e-09, out_scale=0.09410490095615387, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%384] -> nn.conv2d[%385]
  bit=8, threshold=0.49153587222099304
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003840124001726508, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%385] -> add[%387]
  bit=32, threshold=2.605875015258789
  SimulatedQuantizeParams(in_scale=0.0003613745, out_scale=6.278922715097224e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%386] -> add[%387]
  bit=32, threshold=2.605875015258789
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.278922715097224e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%387] -> nn.relu[%388]
  bit=32, threshold=13.627424240112305
  SimulatedQuantizeParams(in_scale=6.2789227e-09, out_scale=6.278922715097224e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%388] -> nn.conv2d[%390]
  bit=8, threshold=13.627424240112305
  SimulatedQuantizeParams(in_scale=6.2789227e-09, out_scale=0.10646425187587738, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%389] -> nn.conv2d[%390]
  bit=8, threshold=0.4021762013435364
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.003142001572996378, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%390] -> add[%392]
  bit=32, threshold=3.6154825687408447
  SimulatedQuantizeParams(in_scale=0.00033451084, out_scale=4.7092596489051175e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%391] -> add[%392]
  bit=32, threshold=3.6154825687408447
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.7092596489051175e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%392] -> nn.relu[%393]
  bit=32, threshold=9.992453575134277
  SimulatedQuantizeParams(in_scale=4.7092596e-09, out_scale=4.7092596489051175e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%393] -> nn.conv2d[%395]
  bit=8, threshold=9.711898803710938
  SimulatedQuantizeParams(in_scale=4.7092596e-09, out_scale=0.0758742094039917, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%394] -> nn.conv2d[%395]
  bit=8, threshold=0.9571426510810852
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007477676961570978, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%395] -> add[%397]
  bit=32, threshold=2.029176950454712
  SimulatedQuantizeParams(in_scale=0.0005673628, out_scale=5.0616910662881764e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%396] -> add[%397]
  bit=32, threshold=2.029176950454712
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.0616910662881764e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%397] -> nn.relu[%398]
  bit=32, threshold=10.032608032226562
  SimulatedQuantizeParams(in_scale=5.061691e-09, out_scale=5.0616910662881764e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%368] -> nn.max_pool2d[%399]
  bit=32, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=7.196651e-09, out_scale=4.147369558893388e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%378] -> concatenate[%400]
  bit=32, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%398] -> concatenate[%400]
  bit=32, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=5.061691e-09, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.max_pool2d[%399] -> concatenate[%400]
  bit=32, threshold=8.906408309936523
  SimulatedQuantizeParams(in_scale=4.1473696e-09, out_scale=7.762245068931861e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%400] -> nn.conv2d[%402]
  bit=8, threshold=16.574100494384766
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=0.12948516011238098, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%401] -> nn.conv2d[%402]
  bit=8, threshold=0.9461250901222229
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007391602266579866, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%402] -> add[%404]
  bit=32, threshold=1.7416651248931885
  SimulatedQuantizeParams(in_scale=0.0009571028, out_scale=4.635082095916232e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%403] -> add[%404]
  bit=32, threshold=1.7416651248931885
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.635082095916232e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%404] -> nn.relu[%405]
  bit=32, threshold=10.777520179748535
  SimulatedQuantizeParams(in_scale=4.635082e-09, out_scale=4.635082095916232e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%400] -> nn.conv2d[%407]
  bit=8, threshold=16.574100494384766
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=0.12948516011238098, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%406] -> nn.conv2d[%407]
  bit=8, threshold=0.7819865345954895
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006109269801527262, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%407] -> add[%409]
  bit=32, threshold=1.3491286039352417
  SimulatedQuantizeParams(in_scale=0.0007910598, out_scale=4.726279367872621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%408] -> add[%409]
  bit=32, threshold=1.3491286039352417
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.726279367872621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%409] -> nn.relu[%410]
  bit=32, threshold=9.828022003173828
  SimulatedQuantizeParams(in_scale=4.7262794e-09, out_scale=4.726279367872621e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%410] -> nn.conv2d[%412]
  bit=8, threshold=9.828022003173828
  SimulatedQuantizeParams(in_scale=4.7262794e-09, out_scale=0.07678142189979553, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%411] -> nn.conv2d[%412]
  bit=8, threshold=0.6497980952262878
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005076547618955374, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%412] -> add[%414]
  bit=32, threshold=1.2121844291687012
  SimulatedQuantizeParams(in_scale=0.00038978455, out_scale=4.80139394909429e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%413] -> add[%414]
  bit=32, threshold=1.2121844291687012
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.80139394909429e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%414] -> nn.relu[%415]
  bit=32, threshold=10.601075172424316
  SimulatedQuantizeParams(in_scale=4.801394e-09, out_scale=4.80139394909429e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%410] -> nn.conv2d[%417]
  bit=8, threshold=9.828022003173828
  SimulatedQuantizeParams(in_scale=4.7262794e-09, out_scale=0.07678142189979553, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%416] -> nn.conv2d[%417]
  bit=8, threshold=1.0432422161102295
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008150329813361168, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%417] -> add[%419]
  bit=32, threshold=0.8334571123123169
  SimulatedQuantizeParams(in_scale=0.00062579394, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%418] -> add[%419]
  bit=32, threshold=0.8334571123123169
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%419] -> nn.relu[%420]
  bit=32, threshold=18.286447525024414
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%415] -> concatenate[%421]
  bit=32, threshold=18.286447525024414
  SimulatedQuantizeParams(in_scale=4.801394e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%420] -> concatenate[%421]
  bit=32, threshold=18.286447525024414
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%400] -> nn.conv2d[%423]
  bit=8, threshold=16.574100494384766
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=0.12948516011238098, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%422] -> nn.conv2d[%423]
  bit=8, threshold=0.7320553660392761
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005719182547181845, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%423] -> add[%425]
  bit=32, threshold=3.173121452331543
  SimulatedQuantizeParams(in_scale=0.0007405493, out_scale=4.2537506850237605e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%424] -> add[%425]
  bit=32, threshold=3.173121452331543
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.2537506850237605e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%425] -> nn.relu[%426]
  bit=32, threshold=9.873434066772461
  SimulatedQuantizeParams(in_scale=4.2537507e-09, out_scale=4.2537506850237605e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%426] -> nn.conv2d[%428]
  bit=8, threshold=7.434506416320801
  SimulatedQuantizeParams(in_scale=4.2537507e-09, out_scale=0.058082081377506256, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%427] -> nn.conv2d[%428]
  bit=8, threshold=0.32545602321624756
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.002542625181376934, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%428] -> add[%430]
  bit=32, threshold=2.3684961795806885
  SimulatedQuantizeParams(in_scale=0.00014768096, out_scale=3.887672406222009e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%429] -> add[%430]
  bit=32, threshold=2.3684961795806885
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.887672406222009e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%430] -> nn.relu[%431]
  bit=32, threshold=8.034111022949219
  SimulatedQuantizeParams(in_scale=3.8876724e-09, out_scale=3.887672406222009e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%431] -> nn.conv2d[%433]
  bit=8, threshold=6.94843864440918
  SimulatedQuantizeParams(in_scale=3.8876724e-09, out_scale=0.054284676909446716, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%432] -> nn.conv2d[%433]
  bit=8, threshold=0.6720498204231262
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.005250389222055674, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%433] -> add[%435]
  bit=32, threshold=3.2491979598999023
  SimulatedQuantizeParams(in_scale=0.00028501567, out_scale=4.258463359718689e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%434] -> add[%435]
  bit=32, threshold=3.2491979598999023
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.258463359718689e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%435] -> nn.relu[%436]
  bit=32, threshold=9.11160659790039
  SimulatedQuantizeParams(in_scale=4.2584634e-09, out_scale=4.258463359718689e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%431] -> nn.conv2d[%438]
  bit=8, threshold=6.94843864440918
  SimulatedQuantizeParams(in_scale=3.8876724e-09, out_scale=0.054284676909446716, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%437] -> nn.conv2d[%438]
  bit=8, threshold=0.4612494707107544
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0036035114899277687, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%438] -> add[%440]
  bit=32, threshold=3.5127429962158203
  SimulatedQuantizeParams(in_scale=0.00019561546, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%439] -> add[%440]
  bit=32, threshold=3.5127429962158203
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%440] -> nn.relu[%441]
  bit=32, threshold=8.9388427734375
  SimulatedQuantizeParams(in_scale=4.4178647e-09, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%436] -> concatenate[%442]
  bit=32, threshold=6.666423797607422
  SimulatedQuantizeParams(in_scale=4.2584634e-09, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%441] -> concatenate[%442]
  bit=32, threshold=6.666423797607422
  SimulatedQuantizeParams(in_scale=4.4178647e-09, out_scale=4.417864740702271e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%400] -> nn.avg_pool2d[%443]
  not quantized
  SimulatedQuantizeParams(in_scale=7.762245e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%443] -> nn.conv2d[%445]
  bit=8, threshold=8.041417121887207
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0628235712647438, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%444] -> nn.conv2d[%445]
  bit=8, threshold=1.1557105779647827
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.009028988890349865, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%445] -> add[%447]
  bit=32, threshold=1.0772409439086914
  SimulatedQuantizeParams(in_scale=0.0005672333, out_scale=3.7089007420831877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%446] -> add[%447]
  bit=32, threshold=1.0772409439086914
  SimulatedQuantizeParams(in_scale=1.0, out_scale=3.7089007420831877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%447] -> nn.relu[%448]
  bit=32, threshold=8.030485153198242
  SimulatedQuantizeParams(in_scale=3.7089007e-09, out_scale=3.7089007420831877e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%405] -> concatenate[%449]
  bit=32, threshold=3.7890758514404297
  SimulatedQuantizeParams(in_scale=4.635082e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%421] -> concatenate[%449]
  bit=32, threshold=3.7890758514404297
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%442] -> concatenate[%449]
  bit=32, threshold=3.7890758514404297
  SimulatedQuantizeParams(in_scale=4.4178647e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%448] -> concatenate[%449]
  bit=32, threshold=3.7890758514404297
  SimulatedQuantizeParams(in_scale=3.7089007e-09, out_scale=8.750882685149008e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%449] -> nn.conv2d[%451]
  bit=8, threshold=18.803203582763672
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=0.1469000279903412, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%450] -> nn.conv2d[%451]
  bit=8, threshold=2.4350264072418213
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.01902364380657673, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%451] -> add[%453]
  bit=32, threshold=0.35487693548202515
  SimulatedQuantizeParams(in_scale=0.0027945738, out_scale=6.016696030286539e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%452] -> add[%453]
  bit=32, threshold=0.35487693548202515
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.016696030286539e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%453] -> nn.relu[%454]
  bit=32, threshold=12.86056137084961
  SimulatedQuantizeParams(in_scale=6.016696e-09, out_scale=6.016696030286539e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%449] -> nn.conv2d[%456]
  bit=8, threshold=18.803203582763672
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=0.1469000279903412, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%455] -> nn.conv2d[%456]
  bit=8, threshold=1.086700439453125
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.008489847183227539, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%456] -> add[%458]
  bit=32, threshold=1.4524096250534058
  SimulatedQuantizeParams(in_scale=0.0012471587, out_scale=7.307435545556018e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%457] -> add[%458]
  bit=32, threshold=1.4524096250534058
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.307435545556018e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%458] -> nn.relu[%459]
  bit=32, threshold=15.603008270263672
  SimulatedQuantizeParams(in_scale=7.3074355e-09, out_scale=7.307435545556018e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%459] -> nn.conv2d[%461]
  bit=8, threshold=15.603008270263672
  SimulatedQuantizeParams(in_scale=7.3074355e-09, out_scale=0.12189850211143494, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%460] -> nn.conv2d[%461]
  bit=8, threshold=1.9439151287078857
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.015186836943030357, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%461] -> add[%463]
  bit=32, threshold=0.5548152923583984
  SimulatedQuantizeParams(in_scale=0.0018512526, out_scale=1.6128288038430583e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%462] -> add[%463]
  bit=32, threshold=0.5548152923583984
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6128288038430583e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%463] -> nn.relu[%464]
  bit=32, threshold=34.27431106567383
  SimulatedQuantizeParams(in_scale=1.6128288e-08, out_scale=1.6128288038430583e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%459] -> nn.conv2d[%466]
  bit=8, threshold=15.603008270263672
  SimulatedQuantizeParams(in_scale=7.3074355e-09, out_scale=0.12189850211143494, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%465] -> nn.conv2d[%466]
  bit=8, threshold=2.057163953781128
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.016071593388915062, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%466] -> add[%468]
  bit=32, threshold=0.5418263673782349
  SimulatedQuantizeParams(in_scale=0.0019591032, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%467] -> add[%468]
  bit=32, threshold=0.5418263673782349
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%468] -> nn.relu[%469]
  bit=32, threshold=34.57212448120117
  SimulatedQuantizeParams(in_scale=1.626129e-08, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%464] -> concatenate[%470]
  bit=32, threshold=34.57212448120117
  SimulatedQuantizeParams(in_scale=1.6128288e-08, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%469] -> concatenate[%470]
  bit=32, threshold=34.57212448120117
  SimulatedQuantizeParams(in_scale=1.626129e-08, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%449] -> nn.conv2d[%472]
  bit=8, threshold=18.803203582763672
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=0.1469000279903412, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%471] -> nn.conv2d[%472]
  bit=8, threshold=0.8023834228515625
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.006268620491027832, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%472] -> add[%474]
  bit=32, threshold=1.9777050018310547
  SimulatedQuantizeParams(in_scale=0.0009208605, out_scale=5.8777760436612425e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%473] -> add[%474]
  bit=32, threshold=1.9777050018310547
  SimulatedQuantizeParams(in_scale=1.0, out_scale=5.8777760436612425e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%474] -> nn.relu[%475]
  bit=32, threshold=12.829500198364258
  SimulatedQuantizeParams(in_scale=5.877776e-09, out_scale=5.8777760436612425e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%475] -> nn.conv2d[%477]
  bit=8, threshold=10.481781005859375
  SimulatedQuantizeParams(in_scale=5.877776e-09, out_scale=0.08188891410827637, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%476] -> nn.conv2d[%477]
  bit=8, threshold=0.6431017518043518
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.0050242324359714985, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%477] -> add[%479]
  bit=32, threshold=2.903168201446533
  SimulatedQuantizeParams(in_scale=0.00041142895, out_scale=8.565093523316136e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%478] -> add[%479]
  bit=32, threshold=2.903168201446533
  SimulatedQuantizeParams(in_scale=1.0, out_scale=8.565093523316136e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%479] -> nn.relu[%480]
  bit=32, threshold=18.597047805786133
  SimulatedQuantizeParams(in_scale=8.5650935e-09, out_scale=8.565093523316136e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%480] -> nn.conv2d[%482]
  bit=8, threshold=18.597047805786133
  SimulatedQuantizeParams(in_scale=8.5650935e-09, out_scale=0.14528943598270416, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%481] -> nn.conv2d[%482]
  bit=8, threshold=0.9153760075569153
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007151375059038401, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%482] -> add[%484]
  bit=32, threshold=0.6826770901679993
  SimulatedQuantizeParams(in_scale=0.0010390192, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%483] -> add[%484]
  bit=32, threshold=0.6826770901679993
  SimulatedQuantizeParams(in_scale=1.0, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%484] -> nn.relu[%485]
  bit=32, threshold=16.534765243530273
  SimulatedQuantizeParams(in_scale=7.844385e-09, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%480] -> nn.conv2d[%487]
  bit=8, threshold=18.597047805786133
  SimulatedQuantizeParams(in_scale=8.5650935e-09, out_scale=0.14528943598270416, clip_min=-127, clip_max=127, in_dtype=int32, out_dtype=int8)
---------
constant[%486] -> nn.conv2d[%487]
  bit=8, threshold=0.939830482006073
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.007342425640672445, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%487] -> add[%489]
  bit=32, threshold=0.5877044200897217
  SimulatedQuantizeParams(in_scale=0.0010667769, out_scale=6.817605591180609e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%488] -> add[%489]
  bit=32, threshold=0.5877044200897217
  SimulatedQuantizeParams(in_scale=1.0, out_scale=6.817605591180609e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%489] -> nn.relu[%490]
  bit=32, threshold=14.328747749328613
  SimulatedQuantizeParams(in_scale=6.8176056e-09, out_scale=6.817605591180609e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%485] -> concatenate[%491]
  bit=32, threshold=14.328747749328613
  SimulatedQuantizeParams(in_scale=7.844385e-09, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%490] -> concatenate[%491]
  bit=32, threshold=14.328747749328613
  SimulatedQuantizeParams(in_scale=6.8176056e-09, out_scale=7.844384697364148e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%449] -> nn.avg_pool2d[%492]
  not quantized
  SimulatedQuantizeParams(in_scale=8.750883e-09, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%492] -> nn.conv2d[%494]
  bit=8, threshold=9.986809730529785
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.07802195101976395, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
constant[%493] -> nn.conv2d[%494]
  bit=8, threshold=2.859543561935425
  SimulatedQuantizeParams(in_scale=1.0, out_scale=0.022340184077620506, clip_min=-127, clip_max=127, in_dtype=float32, out_dtype=int8)
---------
nn.conv2d[%494] -> add[%496]
  bit=32, threshold=0.7978425025939941
  SimulatedQuantizeParams(in_scale=0.0017430248, out_scale=4.033631650912639e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
constant[%495] -> add[%496]
  bit=32, threshold=0.7978425025939941
  SimulatedQuantizeParams(in_scale=1.0, out_scale=4.033631650912639e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=float32, out_dtype=int32)
---------
add[%496] -> nn.relu[%497]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=4.0336317e-09, out_scale=4.033631650912639e-09, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%454] -> concatenate[%498]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=6.016696e-09, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%470] -> concatenate[%498]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=1.626129e-08, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%491] -> concatenate[%498]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=7.844385e-09, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
nn.relu[%497] -> concatenate[%498]
  bit=32, threshold=8.95612907409668
  SimulatedQuantizeParams(in_scale=4.0336317e-09, out_scale=1.6261289204066998e-08, clip_min=-2147483647, clip_max=2147483647, in_dtype=int32, out_dtype=int32)
---------
concatenate[%498] -> nn.avg_pool2d[%499]
  not quantized
  SimulatedQuantizeParams(in_scale=1.626129e-08, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=int32, out_dtype=float32)
---------
nn.avg_pool2d[%499] -> nn.batch_flatten[%500]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.batch_flatten[%500] -> nn.dense[%502]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%501] -> nn.dense[%502]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
nn.dense[%502] -> add[%504]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
constant[%503] -> add[%504]
  not quantized
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
---------
add[%504] -> OUT
  SimulatedQuantizeParams(in_scale=1.0, out_scale=1.0, clip_min=nan, clip_max=nan, in_dtype=float32, out_dtype=float32)
num of snodes:
505
num of nodes:
505
---------
simulated_quantize(data[%0])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0205693f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%6])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0184393f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%12])
  in_scale: 0.000379284f /* ty=float32 */
  out_scale: 5.61859e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%18])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.61859e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%24])
  in_scale: 5.61859e-09f /* ty=float32 */
  out_scale: 5.61859e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%30])
  in_scale: 5.61859e-09f /* ty=float32 */
  out_scale: 0.101719f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%36])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.015331f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%42])
  in_scale: 0.00155945f /* ty=float32 */
  out_scale: 7.71852e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%48])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.71852e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%54])
  in_scale: 7.71852e-09f /* ty=float32 */
  out_scale: 7.71852e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%60])
  in_scale: 7.71852e-09f /* ty=float32 */
  out_scale: 0.126736f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%66])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00999235f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%72])
  in_scale: 0.00126639f /* ty=float32 */
  out_scale: 8.47564e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%78])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.47564e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%84])
  in_scale: 8.47564e-09f /* ty=float32 */
  out_scale: 8.47564e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%90])
  in_scale: 8.47564e-09f /* ty=float32 */
  out_scale: 6.4983e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%96])
  in_scale: 6.4983e-09f /* ty=float32 */
  out_scale: 0.109023f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%102])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0102552f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%108])
  in_scale: 0.00111806f /* ty=float32 */
  out_scale: 7.58872e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%114])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.58872e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%120])
  in_scale: 7.58872e-09f /* ty=float32 */
  out_scale: 7.58872e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%126])
  in_scale: 7.58872e-09f /* ty=float32 */
  out_scale: 0.117261f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%132])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00574007f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%138])
  in_scale: 0.000673084f /* ty=float32 */
  out_scale: 5.89513e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%144])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.89513e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%150])
  in_scale: 5.89513e-09f /* ty=float32 */
  out_scale: 5.89513e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%156])
  in_scale: 5.89513e-09f /* ty=float32 */
  out_scale: 5.84559e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%162])
  in_scale: 5.84559e-09f /* ty=float32 */
  out_scale: 0.0980727f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%168])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00433901f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%174])
  in_scale: 0.000425539f /* ty=float32 */
  out_scale: 4.94103e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%180])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.94103e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%186])
  in_scale: 4.94103e-09f /* ty=float32 */
  out_scale: 4.94103e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%192])
  in_scale: 4.94103e-09f /* ty=float32 */
  out_scale: 9.00128e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%162])
  in_scale: 5.84559e-09f /* ty=float32 */
  out_scale: 0.0980727f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%203])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00401616f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%209])
  in_scale: 0.000393875f /* ty=float32 */
  out_scale: 5.00831e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%215])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.00831e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%221])
  in_scale: 5.00831e-09f /* ty=float32 */
  out_scale: 5.00831e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%227])
  in_scale: 5.00831e-09f /* ty=float32 */
  out_scale: 0.0731393f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%233])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00522591f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%239])
  in_scale: 0.000382219f /* ty=float32 */
  out_scale: 4.36374e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%245])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.36374e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%251])
  in_scale: 4.36374e-09f /* ty=float32 */
  out_scale: 4.36374e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%257])
  in_scale: 4.36374e-09f /* ty=float32 */
  out_scale: 9.00128e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%162])
  in_scale: 5.84559e-09f /* ty=float32 */
  out_scale: 0.0980727f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%268])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00706938f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%274])
  in_scale: 0.000693313f /* ty=float32 */
  out_scale: 6.26184e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%280])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.26184e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%286])
  in_scale: 6.26184e-09f /* ty=float32 */
  out_scale: 6.26184e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%292])
  in_scale: 6.26184e-09f /* ty=float32 */
  out_scale: 0.0858584f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%298])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00603217f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%304])
  in_scale: 0.000517912f /* ty=float32 */
  out_scale: 5.63654e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%310])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.63654e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%316])
  in_scale: 5.63654e-09f /* ty=float32 */
  out_scale: 5.63654e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%322])
  in_scale: 5.63654e-09f /* ty=float32 */
  out_scale: 0.102477f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%328])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00584231f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%334])
  in_scale: 0.000598702f /* ty=float32 */
  out_scale: 9.00128e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%340])
  in_scale: 1f /* ty=float32 */
  out_scale: 9.00128e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%346])
  in_scale: 9.00128e-09f /* ty=float32 */
  out_scale: 9.00128e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%352])
  in_scale: 9.00128e-09f /* ty=float32 */
  out_scale: 9.00128e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%162])
  in_scale: 5.84559e-09f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%363])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0618268f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%369])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.011058f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%375])
  in_scale: 0.000683679f /* ty=float32 */
  out_scale: 5.49927e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%381])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.49927e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%387])
  in_scale: 5.49927e-09f /* ty=float32 */
  out_scale: 5.49927e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%393])
  in_scale: 5.49927e-09f /* ty=float32 */
  out_scale: 9.00128e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%399])
  in_scale: 9.00128e-09f /* ty=float32 */
  out_scale: 0.144319f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%405])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00584852f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%411])
  in_scale: 0.000844052f /* ty=float32 */
  out_scale: 6.7476e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%417])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.7476e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%423])
  in_scale: 6.7476e-09f /* ty=float32 */
  out_scale: 6.7476e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%429])
  in_scale: 6.7476e-09f /* ty=float32 */
  out_scale: 8.0604e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%399])
  in_scale: 9.00128e-09f /* ty=float32 */
  out_scale: 0.144319f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%440])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00465262f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%446])
  in_scale: 0.000671461f /* ty=float32 */
  out_scale: 6.83224e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%452])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.83224e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%458])
  in_scale: 6.83224e-09f /* ty=float32 */
  out_scale: 6.83224e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%464])
  in_scale: 6.83224e-09f /* ty=float32 */
  out_scale: 0.104371f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%470])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00393846f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%476])
  in_scale: 0.000411061f /* ty=float32 */
  out_scale: 5.49525e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%482])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.49525e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%488])
  in_scale: 5.49525e-09f /* ty=float32 */
  out_scale: 5.49525e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%494])
  in_scale: 5.49525e-09f /* ty=float32 */
  out_scale: 8.0604e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%399])
  in_scale: 9.00128e-09f /* ty=float32 */
  out_scale: 0.144319f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%505])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00536366f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%511])
  in_scale: 0.000774077f /* ty=float32 */
  out_scale: 5.6246e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%517])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.6246e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%523])
  in_scale: 5.6246e-09f /* ty=float32 */
  out_scale: 5.6246e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%529])
  in_scale: 5.6246e-09f /* ty=float32 */
  out_scale: 0.0654255f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%535])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00558279f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%541])
  in_scale: 0.000365257f /* ty=float32 */
  out_scale: 4.72001e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%547])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.72001e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%553])
  in_scale: 4.72001e-09f /* ty=float32 */
  out_scale: 4.72001e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%559])
  in_scale: 4.72001e-09f /* ty=float32 */
  out_scale: 0.0737892f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%565])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00928544f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%571])
  in_scale: 0.000685166f /* ty=float32 */
  out_scale: 8.0604e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%577])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.0604e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%583])
  in_scale: 8.0604e-09f /* ty=float32 */
  out_scale: 8.0604e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%589])
  in_scale: 8.0604e-09f /* ty=float32 */
  out_scale: 8.0604e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%399])
  in_scale: 9.00128e-09f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%600])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0866785f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%606])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00933783f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%612])
  in_scale: 0.00080939f /* ty=float32 */
  out_scale: 7.18542e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%618])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.18542e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%624])
  in_scale: 7.18542e-09f /* ty=float32 */
  out_scale: 7.18542e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%630])
  in_scale: 7.18542e-09f /* ty=float32 */
  out_scale: 8.0604e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%636])
  in_scale: 8.0604e-09f /* ty=float32 */
  out_scale: 0.13219f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%642])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00838885f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%648])
  in_scale: 0.00110892f /* ty=float32 */
  out_scale: 7.61068e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%654])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.61068e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%660])
  in_scale: 7.61068e-09f /* ty=float32 */
  out_scale: 7.61068e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%666])
  in_scale: 7.61068e-09f /* ty=float32 */
  out_scale: 7.61068e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%636])
  in_scale: 8.0604e-09f /* ty=float32 */
  out_scale: 0.13219f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%677])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00501439f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%683])
  in_scale: 0.000662853f /* ty=float32 */
  out_scale: 5.41504e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%689])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.41504e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%695])
  in_scale: 5.41504e-09f /* ty=float32 */
  out_scale: 5.41504e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%701])
  in_scale: 5.41504e-09f /* ty=float32 */
  out_scale: 0.0840957f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%707])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00286836f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%713])
  in_scale: 0.000241217f /* ty=float32 */
  out_scale: 4.46515e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%719])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.46515e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%725])
  in_scale: 4.46515e-09f /* ty=float32 */
  out_scale: 4.46515e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%731])
  in_scale: 4.46515e-09f /* ty=float32 */
  out_scale: 7.61068e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%636])
  in_scale: 8.0604e-09f /* ty=float32 */
  out_scale: 0.13219f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%742])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00695815f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%748])
  in_scale: 0.0009198f /* ty=float32 */
  out_scale: 6.54673e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%754])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.54673e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%760])
  in_scale: 6.54673e-09f /* ty=float32 */
  out_scale: 6.54673e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%766])
  in_scale: 6.54673e-09f /* ty=float32 */
  out_scale: 0.100167f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%772])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00469817f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%778])
  in_scale: 0.0004706f /* ty=float32 */
  out_scale: 5.32158e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%784])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.32158e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%790])
  in_scale: 5.32158e-09f /* ty=float32 */
  out_scale: 5.32158e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%796])
  in_scale: 5.32158e-09f /* ty=float32 */
  out_scale: 0.0884136f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%802])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00468934f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%808])
  in_scale: 0.000414602f /* ty=float32 */
  out_scale: 4.12971e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%814])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.12971e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%820])
  in_scale: 4.12971e-09f /* ty=float32 */
  out_scale: 4.12971e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%826])
  in_scale: 4.12971e-09f /* ty=float32 */
  out_scale: 7.61068e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%636])
  in_scale: 8.0604e-09f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%837])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0766314f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%843])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0113221f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%849])
  in_scale: 0.000867631f /* ty=float32 */
  out_scale: 5.5098e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%855])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.5098e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%861])
  in_scale: 5.5098e-09f /* ty=float32 */
  out_scale: 5.5098e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%867])
  in_scale: 5.5098e-09f /* ty=float32 */
  out_scale: 7.61068e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%873])
  in_scale: 7.61068e-09f /* ty=float32 */
  out_scale: 0.0827759f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%879])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00398437f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%885])
  in_scale: 0.00032981f /* ty=float32 */
  out_scale: 4.90954e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%891])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.90954e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%897])
  in_scale: 4.90954e-09f /* ty=float32 */
  out_scale: 4.90954e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%903])
  in_scale: 4.90954e-09f /* ty=float32 */
  out_scale: 5.87421e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%873])
  in_scale: 7.61068e-09f /* ty=float32 */
  out_scale: 0.0827759f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%914])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00519745f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%920])
  in_scale: 0.000430224f /* ty=float32 */
  out_scale: 5.60844e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%926])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.60844e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%932])
  in_scale: 5.60844e-09f /* ty=float32 */
  out_scale: 5.60844e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%938])
  in_scale: 5.60844e-09f /* ty=float32 */
  out_scale: 0.0798705f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%944])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00464639f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%950])
  in_scale: 0.00037111f /* ty=float32 */
  out_scale: 4.99602e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%956])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.99602e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%962])
  in_scale: 4.99602e-09f /* ty=float32 */
  out_scale: 4.99602e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%968])
  in_scale: 4.99602e-09f /* ty=float32 */
  out_scale: 0.0783957f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%974])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00440968f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%980])
  in_scale: 0.0003457f /* ty=float32 */
  out_scale: 5.87421e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%986])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.87421e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%992])
  in_scale: 5.87421e-09f /* ty=float32 */
  out_scale: 5.87421e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%998])
  in_scale: 5.87421e-09f /* ty=float32 */
  out_scale: 5.87421e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%873])
  in_scale: 7.61068e-09f /* ty=float32 */
  out_scale: 4.93383e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%1009])
  in_scale: 4.93383e-09f /* ty=float32 */
  out_scale: 5.87421e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1015])
  in_scale: 5.87421e-09f /* ty=float32 */
  out_scale: 0.0958396f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1021])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00537618f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1027])
  in_scale: 0.000515251f /* ty=float32 */
  out_scale: 4.88805e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1033])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.88805e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1039])
  in_scale: 4.88805e-09f /* ty=float32 */
  out_scale: 4.88805e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1045])
  in_scale: 4.88805e-09f /* ty=float32 */
  out_scale: 8.9195e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1015])
  in_scale: 5.87421e-09f /* ty=float32 */
  out_scale: 0.0958396f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1056])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00610288f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1062])
  in_scale: 0.000584898f /* ty=float32 */
  out_scale: 5.25398e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1068])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.25398e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1074])
  in_scale: 5.25398e-09f /* ty=float32 */
  out_scale: 5.25398e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1080])
  in_scale: 5.25398e-09f /* ty=float32 */
  out_scale: 0.0796376f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1086])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00749939f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1092])
  in_scale: 0.000597234f /* ty=float32 */
  out_scale: 5.06826e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1098])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.06826e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1104])
  in_scale: 5.06826e-09f /* ty=float32 */
  out_scale: 5.06826e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1110])
  in_scale: 5.06826e-09f /* ty=float32 */
  out_scale: 0.0838972f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1116])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00617447f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1122])
  in_scale: 0.000518021f /* ty=float32 */
  out_scale: 4.83569e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1128])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.83569e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1134])
  in_scale: 4.83569e-09f /* ty=float32 */
  out_scale: 4.83569e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1140])
  in_scale: 4.83569e-09f /* ty=float32 */
  out_scale: 8.9195e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1015])
  in_scale: 5.87421e-09f /* ty=float32 */
  out_scale: 0.0958396f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1151])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00469163f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1157])
  in_scale: 0.000449644f /* ty=float32 */
  out_scale: 4.69498e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1163])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.69498e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1169])
  in_scale: 4.69498e-09f /* ty=float32 */
  out_scale: 4.69498e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1175])
  in_scale: 4.69498e-09f /* ty=float32 */
  out_scale: 0.0646086f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1181])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00493314f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1187])
  in_scale: 0.000318723f /* ty=float32 */
  out_scale: 3.92667e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1193])
  in_scale: 1f /* ty=float32 */
  out_scale: 3.92667e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1199])
  in_scale: 3.92667e-09f /* ty=float32 */
  out_scale: 3.92667e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1205])
  in_scale: 3.92667e-09f /* ty=float32 */
  out_scale: 0.0552085f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1211])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00597663f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1217])
  in_scale: 0.000329961f /* ty=float32 */
  out_scale: 4.00599e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1223])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.00599e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1229])
  in_scale: 4.00599e-09f /* ty=float32 */
  out_scale: 4.00599e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1235])
  in_scale: 4.00599e-09f /* ty=float32 */
  out_scale: 0.0631221f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1241])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00586756f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1247])
  in_scale: 0.000370372f /* ty=float32 */
  out_scale: 5.84374e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1253])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.84374e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1259])
  in_scale: 5.84374e-09f /* ty=float32 */
  out_scale: 5.84374e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1265])
  in_scale: 5.84374e-09f /* ty=float32 */
  out_scale: 0.0963442f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1271])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00404097f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1277])
  in_scale: 0.000389324f /* ty=float32 */
  out_scale: 8.9195e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1283])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.9195e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1289])
  in_scale: 8.9195e-09f /* ty=float32 */
  out_scale: 8.9195e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1295])
  in_scale: 8.9195e-09f /* ty=float32 */
  out_scale: 8.9195e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1015])
  in_scale: 5.87421e-09f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%1306])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.057475f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1312])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.01131f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1318])
  in_scale: 0.000650043f /* ty=float32 */
  out_scale: 4.57761e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1324])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.57761e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1330])
  in_scale: 4.57761e-09f /* ty=float32 */
  out_scale: 4.57761e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1336])
  in_scale: 4.57761e-09f /* ty=float32 */
  out_scale: 8.9195e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1342])
  in_scale: 8.9195e-09f /* ty=float32 */
  out_scale: 0.149103f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1348])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00530625f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1354])
  in_scale: 0.00079118f /* ty=float32 */
  out_scale: 5.71017e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1360])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.71017e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1366])
  in_scale: 5.71017e-09f /* ty=float32 */
  out_scale: 5.71017e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1372])
  in_scale: 5.71017e-09f /* ty=float32 */
  out_scale: 1.05709e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1342])
  in_scale: 8.9195e-09f /* ty=float32 */
  out_scale: 0.149103f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1383])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00749864f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1389])
  in_scale: 0.00111807f /* ty=float32 */
  out_scale: 6.19932e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1395])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.19932e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1401])
  in_scale: 6.19932e-09f /* ty=float32 */
  out_scale: 6.19932e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1407])
  in_scale: 6.19932e-09f /* ty=float32 */
  out_scale: 0.103404f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1413])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00484023f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1419])
  in_scale: 0.000500498f /* ty=float32 */
  out_scale: 4.41672e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1425])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.41672e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1431])
  in_scale: 4.41672e-09f /* ty=float32 */
  out_scale: 4.41672e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1437])
  in_scale: 4.41672e-09f /* ty=float32 */
  out_scale: 0.0772715f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1443])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00553171f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1449])
  in_scale: 0.000427443f /* ty=float32 */
  out_scale: 4.53216e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1455])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.53216e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1461])
  in_scale: 4.53216e-09f /* ty=float32 */
  out_scale: 4.53216e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1467])
  in_scale: 4.53216e-09f /* ty=float32 */
  out_scale: 1.05709e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1342])
  in_scale: 8.9195e-09f /* ty=float32 */
  out_scale: 0.149103f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1478])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00691003f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1484])
  in_scale: 0.00103031f /* ty=float32 */
  out_scale: 5.28767e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1490])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.28767e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1496])
  in_scale: 5.28767e-09f /* ty=float32 */
  out_scale: 5.28767e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1502])
  in_scale: 5.28767e-09f /* ty=float32 */
  out_scale: 0.0807742f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1508])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0040512f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1514])
  in_scale: 0.000327232f /* ty=float32 */
  out_scale: 5.3636e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1520])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.3636e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1526])
  in_scale: 5.3636e-09f /* ty=float32 */
  out_scale: 5.3636e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1532])
  in_scale: 5.3636e-09f /* ty=float32 */
  out_scale: 0.0894094f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1538])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00555295f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1544])
  in_scale: 0.000496486f /* ty=float32 */
  out_scale: 7.19698e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1550])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.19698e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1556])
  in_scale: 7.19698e-09f /* ty=float32 */
  out_scale: 7.19698e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1562])
  in_scale: 7.19698e-09f /* ty=float32 */
  out_scale: 0.111239f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1568])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00499566f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1574])
  in_scale: 0.000555714f /* ty=float32 */
  out_scale: 1.00332e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1580])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.00332e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1586])
  in_scale: 1.00332e-08f /* ty=float32 */
  out_scale: 1.00332e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1592])
  in_scale: 1.00332e-08f /* ty=float32 */
  out_scale: 0.157102f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1598])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00521494f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1604])
  in_scale: 0.000819276f /* ty=float32 */
  out_scale: 1.05709e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1610])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.05709e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1616])
  in_scale: 1.05709e-08f /* ty=float32 */
  out_scale: 1.05709e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1622])
  in_scale: 1.05709e-08f /* ty=float32 */
  out_scale: 1.05709e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1342])
  in_scale: 8.9195e-09f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%1633])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0451576f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1639])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00751682f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1645])
  in_scale: 0.000339442f /* ty=float32 */
  out_scale: 3.82568e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1651])
  in_scale: 1f /* ty=float32 */
  out_scale: 3.82568e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1657])
  in_scale: 3.82568e-09f /* ty=float32 */
  out_scale: 3.82568e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1663])
  in_scale: 3.82568e-09f /* ty=float32 */
  out_scale: 1.05709e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1669])
  in_scale: 1.05709e-08f /* ty=float32 */
  out_scale: 0.17452f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1675])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00918641f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1681])
  in_scale: 0.00160321f /* ty=float32 */
  out_scale: 6.29081e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1687])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.29081e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1693])
  in_scale: 6.29081e-09f /* ty=float32 */
  out_scale: 6.29081e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1699])
  in_scale: 6.29081e-09f /* ty=float32 */
  out_scale: 6.43986e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1669])
  in_scale: 1.05709e-08f /* ty=float32 */
  out_scale: 0.17452f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1710])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00456683f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1716])
  in_scale: 0.000797002f /* ty=float32 */
  out_scale: 6.89674e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1722])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.89674e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1728])
  in_scale: 6.89674e-09f /* ty=float32 */
  out_scale: 6.89674e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1734])
  in_scale: 6.89674e-09f /* ty=float32 */
  out_scale: 0.100028f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1740])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00880511f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1746])
  in_scale: 0.000880759f /* ty=float32 */
  out_scale: 7.31816e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1752])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.31816e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1758])
  in_scale: 7.31816e-09f /* ty=float32 */
  out_scale: 7.31816e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1764])
  in_scale: 7.31816e-09f /* ty=float32 */
  out_scale: 0.120732f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1770])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00519522f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1776])
  in_scale: 0.000627232f /* ty=float32 */
  out_scale: 6.16979e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1782])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.16979e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1788])
  in_scale: 6.16979e-09f /* ty=float32 */
  out_scale: 6.16979e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1794])
  in_scale: 6.16979e-09f /* ty=float32 */
  out_scale: 6.43986e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1669])
  in_scale: 1.05709e-08f /* ty=float32 */
  out_scale: 0.17452f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1805])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00663011f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1811])
  in_scale: 0.00115709f /* ty=float32 */
  out_scale: 7.91671e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1817])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.91671e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1823])
  in_scale: 7.91671e-09f /* ty=float32 */
  out_scale: 7.91671e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1829])
  in_scale: 7.91671e-09f /* ty=float32 */
  out_scale: 0.12414f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1835])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00737073f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1841])
  in_scale: 0.000915f /* ty=float32 */
  out_scale: 7.64215e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1847])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.64215e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1853])
  in_scale: 7.64215e-09f /* ty=float32 */
  out_scale: 7.64215e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1859])
  in_scale: 7.64215e-09f /* ty=float32 */
  out_scale: 0.130681f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1865])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00491007f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1871])
  in_scale: 0.00064165f /* ty=float32 */
  out_scale: 5.3899e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1877])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.3899e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1883])
  in_scale: 5.3899e-09f /* ty=float32 */
  out_scale: 5.3899e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1889])
  in_scale: 5.3899e-09f /* ty=float32 */
  out_scale: 0.0883848f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1895])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00747773f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1901])
  in_scale: 0.000660917f /* ty=float32 */
  out_scale: 7.24282e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1907])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.24282e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1913])
  in_scale: 7.24282e-09f /* ty=float32 */
  out_scale: 7.24282e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1919])
  in_scale: 7.24282e-09f /* ty=float32 */
  out_scale: 0.0920394f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1925])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00741879f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1931])
  in_scale: 0.000682821f /* ty=float32 */
  out_scale: 5.33132e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1937])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.33132e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1943])
  in_scale: 5.33132e-09f /* ty=float32 */
  out_scale: 5.33132e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1949])
  in_scale: 5.33132e-09f /* ty=float32 */
  out_scale: 6.43986e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1669])
  in_scale: 1.05709e-08f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%1960])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0481846f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1966])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0139921f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%1972])
  in_scale: 0.000674205f /* ty=float32 */
  out_scale: 3.8614e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%1978])
  in_scale: 1f /* ty=float32 */
  out_scale: 3.8614e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%1984])
  in_scale: 3.8614e-09f /* ty=float32 */
  out_scale: 3.8614e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%1990])
  in_scale: 3.8614e-09f /* ty=float32 */
  out_scale: 6.43986e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1996])
  in_scale: 6.43986e-09f /* ty=float32 */
  out_scale: 0.108856f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2002])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00770918f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2008])
  in_scale: 0.000839188f /* ty=float32 */
  out_scale: 7.19665e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2014])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.19665e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2020])
  in_scale: 7.19665e-09f /* ty=float32 */
  out_scale: 7.19665e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2026])
  in_scale: 7.19665e-09f /* ty=float32 */
  out_scale: 7.19665e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1996])
  in_scale: 6.43986e-09f /* ty=float32 */
  out_scale: 0.108856f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2037])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00773977f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2043])
  in_scale: 0.000842519f /* ty=float32 */
  out_scale: 5.59384e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2049])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.59384e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2055])
  in_scale: 5.59384e-09f /* ty=float32 */
  out_scale: 5.59384e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2061])
  in_scale: 5.59384e-09f /* ty=float32 */
  out_scale: 0.0909263f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2067])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00848777f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2073])
  in_scale: 0.000771762f /* ty=float32 */
  out_scale: 6.45887e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2079])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.45887e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2085])
  in_scale: 6.45887e-09f /* ty=float32 */
  out_scale: 6.45887e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2091])
  in_scale: 6.45887e-09f /* ty=float32 */
  out_scale: 0.109682f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2097])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0026531f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2103])
  in_scale: 0.000290997f /* ty=float32 */
  out_scale: 4.6028e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2109])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.6028e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2115])
  in_scale: 4.6028e-09f /* ty=float32 */
  out_scale: 4.6028e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2121])
  in_scale: 4.6028e-09f /* ty=float32 */
  out_scale: 7.19665e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1996])
  in_scale: 6.43986e-09f /* ty=float32 */
  out_scale: 0.108856f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2132])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00476074f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2138])
  in_scale: 0.000518234f /* ty=float32 */
  out_scale: 6.26912e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2144])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.26912e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2150])
  in_scale: 6.26912e-09f /* ty=float32 */
  out_scale: 6.26912e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2156])
  in_scale: 6.26912e-09f /* ty=float32 */
  out_scale: 0.0984178f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2162])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00435483f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2168])
  in_scale: 0.000428593f /* ty=float32 */
  out_scale: 5.721e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2174])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.721e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2180])
  in_scale: 5.721e-09f /* ty=float32 */
  out_scale: 5.721e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2186])
  in_scale: 5.721e-09f /* ty=float32 */
  out_scale: 0.095106f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2192])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00510192f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2198])
  in_scale: 0.000485223f /* ty=float32 */
  out_scale: 6.05562e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2204])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.05562e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2210])
  in_scale: 6.05562e-09f /* ty=float32 */
  out_scale: 6.05562e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2216])
  in_scale: 6.05562e-09f /* ty=float32 */
  out_scale: 0.101932f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2222])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00319643f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2228])
  in_scale: 0.000325819f /* ty=float32 */
  out_scale: 6.22569e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2234])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.22569e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2240])
  in_scale: 6.22569e-09f /* ty=float32 */
  out_scale: 6.22569e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2246])
  in_scale: 6.22569e-09f /* ty=float32 */
  out_scale: 0.108909f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2252])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00268704f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2258])
  in_scale: 0.000292643f /* ty=float32 */
  out_scale: 4.05702e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2264])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.05702e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2270])
  in_scale: 4.05702e-09f /* ty=float32 */
  out_scale: 4.05702e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2276])
  in_scale: 4.05702e-09f /* ty=float32 */
  out_scale: 7.19665e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%1996])
  in_scale: 6.43986e-09f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%2287])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0509987f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2293])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00717943f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2299])
  in_scale: 0.000366142f /* ty=float32 */
  out_scale: 3.37533e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2305])
  in_scale: 1f /* ty=float32 */
  out_scale: 3.37533e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2311])
  in_scale: 3.37533e-09f /* ty=float32 */
  out_scale: 3.37533e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2317])
  in_scale: 3.37533e-09f /* ty=float32 */
  out_scale: 7.19665e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2323])
  in_scale: 7.19665e-09f /* ty=float32 */
  out_scale: 0.0695813f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2329])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00696009f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2335])
  in_scale: 0.000484292f /* ty=float32 */
  out_scale: 6.74937e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2341])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.74937e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2347])
  in_scale: 6.74937e-09f /* ty=float32 */
  out_scale: 6.74937e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2353])
  in_scale: 6.74937e-09f /* ty=float32 */
  out_scale: 0.0927286f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2359])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00493863f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2365])
  in_scale: 0.000457952f /* ty=float32 */
  out_scale: 7.76225e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2371])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.76225e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2377])
  in_scale: 7.76225e-09f /* ty=float32 */
  out_scale: 7.76225e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2383])
  in_scale: 7.76225e-09f /* ty=float32 */
  out_scale: 7.76225e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2323])
  in_scale: 7.19665e-09f /* ty=float32 */
  out_scale: 0.0695813f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2394])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00976139f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2400])
  in_scale: 0.000679211f /* ty=float32 */
  out_scale: 6.75832e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2406])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.75832e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2412])
  in_scale: 6.75832e-09f /* ty=float32 */
  out_scale: 6.75832e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2418])
  in_scale: 6.75832e-09f /* ty=float32 */
  out_scale: 0.0941049f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2424])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00384012f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2430])
  in_scale: 0.000361374f /* ty=float32 */
  out_scale: 6.27892e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2436])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.27892e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2442])
  in_scale: 6.27892e-09f /* ty=float32 */
  out_scale: 6.27892e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2448])
  in_scale: 6.27892e-09f /* ty=float32 */
  out_scale: 0.106464f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2454])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.003142f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2460])
  in_scale: 0.000334511f /* ty=float32 */
  out_scale: 4.70926e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2466])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.70926e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2472])
  in_scale: 4.70926e-09f /* ty=float32 */
  out_scale: 4.70926e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2478])
  in_scale: 4.70926e-09f /* ty=float32 */
  out_scale: 0.0758742f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2484])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00747768f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2490])
  in_scale: 0.000567363f /* ty=float32 */
  out_scale: 5.06169e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2496])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.06169e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2502])
  in_scale: 5.06169e-09f /* ty=float32 */
  out_scale: 5.06169e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2508])
  in_scale: 5.06169e-09f /* ty=float32 */
  out_scale: 7.76225e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2323])
  in_scale: 7.19665e-09f /* ty=float32 */
  out_scale: 4.14737e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.max_pool2d[%2519])
  in_scale: 4.14737e-09f /* ty=float32 */
  out_scale: 7.76225e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2525])
  in_scale: 7.76225e-09f /* ty=float32 */
  out_scale: 0.129485f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2531])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0073916f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2537])
  in_scale: 0.000957103f /* ty=float32 */
  out_scale: 4.63508e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2543])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.63508e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2549])
  in_scale: 4.63508e-09f /* ty=float32 */
  out_scale: 4.63508e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2555])
  in_scale: 4.63508e-09f /* ty=float32 */
  out_scale: 8.75088e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2525])
  in_scale: 7.76225e-09f /* ty=float32 */
  out_scale: 0.129485f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2566])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00610927f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2572])
  in_scale: 0.00079106f /* ty=float32 */
  out_scale: 4.72628e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2578])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.72628e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2584])
  in_scale: 4.72628e-09f /* ty=float32 */
  out_scale: 4.72628e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2590])
  in_scale: 4.72628e-09f /* ty=float32 */
  out_scale: 0.0767814f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2596])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00507655f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2602])
  in_scale: 0.000389785f /* ty=float32 */
  out_scale: 4.80139e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2608])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.80139e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2614])
  in_scale: 4.80139e-09f /* ty=float32 */
  out_scale: 4.80139e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2620])
  in_scale: 4.80139e-09f /* ty=float32 */
  out_scale: 8.75088e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2590])
  in_scale: 4.72628e-09f /* ty=float32 */
  out_scale: 0.0767814f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2631])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00815033f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2637])
  in_scale: 0.000625794f /* ty=float32 */
  out_scale: 8.75088e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2643])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.75088e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2649])
  in_scale: 8.75088e-09f /* ty=float32 */
  out_scale: 8.75088e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2655])
  in_scale: 8.75088e-09f /* ty=float32 */
  out_scale: 8.75088e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2661])
  in_scale: 8.75088e-09f /* ty=float32 */
  out_scale: 8.75088e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2525])
  in_scale: 7.76225e-09f /* ty=float32 */
  out_scale: 0.129485f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2672])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00571918f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2678])
  in_scale: 0.000740549f /* ty=float32 */
  out_scale: 4.25375e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2684])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.25375e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2690])
  in_scale: 4.25375e-09f /* ty=float32 */
  out_scale: 4.25375e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2696])
  in_scale: 4.25375e-09f /* ty=float32 */
  out_scale: 0.0580821f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2702])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00254263f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2708])
  in_scale: 0.000147681f /* ty=float32 */
  out_scale: 3.88767e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2714])
  in_scale: 1f /* ty=float32 */
  out_scale: 3.88767e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2720])
  in_scale: 3.88767e-09f /* ty=float32 */
  out_scale: 3.88767e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2726])
  in_scale: 3.88767e-09f /* ty=float32 */
  out_scale: 0.0542847f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2732])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00525039f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2738])
  in_scale: 0.000285016f /* ty=float32 */
  out_scale: 4.25846e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2744])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.25846e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2750])
  in_scale: 4.25846e-09f /* ty=float32 */
  out_scale: 4.25846e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2756])
  in_scale: 4.25846e-09f /* ty=float32 */
  out_scale: 4.41786e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2726])
  in_scale: 3.88767e-09f /* ty=float32 */
  out_scale: 0.0542847f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2767])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00360351f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2773])
  in_scale: 0.000195615f /* ty=float32 */
  out_scale: 4.41786e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2779])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.41786e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2785])
  in_scale: 4.41786e-09f /* ty=float32 */
  out_scale: 4.41786e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2791])
  in_scale: 4.41786e-09f /* ty=float32 */
  out_scale: 4.41786e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2797])
  in_scale: 4.41786e-09f /* ty=float32 */
  out_scale: 8.75088e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2525])
  in_scale: 7.76225e-09f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%2808])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0628236f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2814])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00902899f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2820])
  in_scale: 0.000567233f /* ty=float32 */
  out_scale: 3.7089e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2826])
  in_scale: 1f /* ty=float32 */
  out_scale: 3.7089e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2832])
  in_scale: 3.7089e-09f /* ty=float32 */
  out_scale: 3.7089e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2838])
  in_scale: 3.7089e-09f /* ty=float32 */
  out_scale: 8.75088e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2844])
  in_scale: 8.75088e-09f /* ty=float32 */
  out_scale: 0.1469f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2850])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0190236f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2856])
  in_scale: 0.00279457f /* ty=float32 */
  out_scale: 6.0167e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2862])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.0167e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2868])
  in_scale: 6.0167e-09f /* ty=float32 */
  out_scale: 6.0167e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2874])
  in_scale: 6.0167e-09f /* ty=float32 */
  out_scale: 1.62613e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2844])
  in_scale: 8.75088e-09f /* ty=float32 */
  out_scale: 0.1469f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2885])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00848985f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2891])
  in_scale: 0.00124716f /* ty=float32 */
  out_scale: 7.30744e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2897])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.30744e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2903])
  in_scale: 7.30744e-09f /* ty=float32 */
  out_scale: 7.30744e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2909])
  in_scale: 7.30744e-09f /* ty=float32 */
  out_scale: 0.121899f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2915])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0151868f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2921])
  in_scale: 0.00185125f /* ty=float32 */
  out_scale: 1.61283e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2927])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.61283e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2933])
  in_scale: 1.61283e-08f /* ty=float32 */
  out_scale: 1.61283e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2939])
  in_scale: 1.61283e-08f /* ty=float32 */
  out_scale: 1.62613e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2909])
  in_scale: 7.30744e-09f /* ty=float32 */
  out_scale: 0.121899f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2950])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0160716f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2956])
  in_scale: 0.0019591f /* ty=float32 */
  out_scale: 1.62613e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2962])
  in_scale: 1f /* ty=float32 */
  out_scale: 1.62613e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%2968])
  in_scale: 1.62613e-08f /* ty=float32 */
  out_scale: 1.62613e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%2974])
  in_scale: 1.62613e-08f /* ty=float32 */
  out_scale: 1.62613e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2980])
  in_scale: 1.62613e-08f /* ty=float32 */
  out_scale: 1.62613e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2844])
  in_scale: 8.75088e-09f /* ty=float32 */
  out_scale: 0.1469f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%2991])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00626862f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%2997])
  in_scale: 0.000920861f /* ty=float32 */
  out_scale: 5.87778e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3003])
  in_scale: 1f /* ty=float32 */
  out_scale: 5.87778e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%3009])
  in_scale: 5.87778e-09f /* ty=float32 */
  out_scale: 5.87778e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%3015])
  in_scale: 5.87778e-09f /* ty=float32 */
  out_scale: 0.0818889f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3021])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00502423f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%3027])
  in_scale: 0.000411429f /* ty=float32 */
  out_scale: 8.56509e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3033])
  in_scale: 1f /* ty=float32 */
  out_scale: 8.56509e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%3039])
  in_scale: 8.56509e-09f /* ty=float32 */
  out_scale: 8.56509e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%3045])
  in_scale: 8.56509e-09f /* ty=float32 */
  out_scale: 0.145289f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3051])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00715138f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%3057])
  in_scale: 0.00103902f /* ty=float32 */
  out_scale: 7.84438e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3063])
  in_scale: 1f /* ty=float32 */
  out_scale: 7.84438e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%3069])
  in_scale: 7.84438e-09f /* ty=float32 */
  out_scale: 7.84438e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%3075])
  in_scale: 7.84438e-09f /* ty=float32 */
  out_scale: 7.84438e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%3045])
  in_scale: 8.56509e-09f /* ty=float32 */
  out_scale: 0.145289f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3086])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.00734243f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%3092])
  in_scale: 0.00106678f /* ty=float32 */
  out_scale: 6.81761e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3098])
  in_scale: 1f /* ty=float32 */
  out_scale: 6.81761e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%3104])
  in_scale: 6.81761e-09f /* ty=float32 */
  out_scale: 6.81761e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%3110])
  in_scale: 6.81761e-09f /* ty=float32 */
  out_scale: 7.84438e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%3116])
  in_scale: 7.84438e-09f /* ty=float32 */
  out_scale: 1.62613e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%2844])
  in_scale: 8.75088e-09f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%3127])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.078022f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3133])
  in_scale: 1f /* ty=float32 */
  out_scale: 0.0223402f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.conv2d[%3139])
  in_scale: 0.00174302f /* ty=float32 */
  out_scale: 4.03363e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3145])
  in_scale: 1f /* ty=float32 */
  out_scale: 4.03363e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%3151])
  in_scale: 4.03363e-09f /* ty=float32 */
  out_scale: 4.03363e-09f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.relu[%3157])
  in_scale: 4.03363e-09f /* ty=float32 */
  out_scale: 1.62613e-08f /* ty=float32 */
  axis: -1
---------
simulated_quantize(concatenate[%3163])
  in_scale: 1.62613e-08f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.avg_pool2d[%3169])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.batch_flatten[%3175])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3181])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(nn.dense[%3187])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(constant[%3193])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
---------
simulated_quantize(add[%3199])
  in_scale: 1f /* ty=float32 */
  out_scale: 1f /* ty=float32 */
  axis: -1
DEBUG:root:simulated graph
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 299, 299), float32]) -> Tensor[(32, 1000), float32] {
  %0 = nn.simulated_quantize(%data, 1f /* ty=float32 */, 0.0205693f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 3, 299, 299), float32] */;
  %1 = nn.simulated_quantize(meta[relay.Constant][0] /* ty=Tensor[(32, 3, 3, 3), float32] */ /* ty=Tensor[(32, 3, 3, 3), float32] */, 1f /* ty=float32 */, 0.0184393f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 3, 3, 3), float32] */;
  %2 = nn.conv2d(%0, %1, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %3 = nn.simulated_quantize(%2, 0.000379284f /* ty=float32 */, 5.61859e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %4 = nn.simulated_quantize(meta[relay.Constant][1] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */, 1f /* ty=float32 */, 5.61859e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1, 1), float32] */;
  %5 = add(%3, %4) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %6 = nn.simulated_quantize(%5, 5.61859e-09f /* ty=float32 */, 5.61859e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %8 = nn.simulated_quantize(%7, 5.61859e-09f /* ty=float32 */, 0.101719f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 32, 149, 149), float32] */;
  %9 = nn.simulated_quantize(meta[relay.Constant][2] /* ty=Tensor[(32, 32, 3, 3), float32] */ /* ty=Tensor[(32, 32, 3, 3), float32] */, 1f /* ty=float32 */, 0.015331f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 32, 3, 3), float32] */;
  %10 = nn.conv2d(%8, %9, padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %11 = nn.simulated_quantize(%10, 0.00155945f /* ty=float32 */, 7.71852e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %12 = nn.simulated_quantize(meta[relay.Constant][3] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */, 1f /* ty=float32 */, 7.71852e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1, 1), float32] */;
  %13 = add(%11, %12) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %14 = nn.simulated_quantize(%13, 7.71852e-09f /* ty=float32 */, 7.71852e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %15 = nn.relu(%14) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %16 = nn.simulated_quantize(%15, 7.71852e-09f /* ty=float32 */, 0.126736f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 32, 147, 147), float32] */;
  %17 = nn.simulated_quantize(meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */ /* ty=Tensor[(64, 32, 3, 3), float32] */, 1f /* ty=float32 */, 0.00999235f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 32, 3, 3), float32] */;
  %18 = nn.conv2d(%16, %17, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %19 = nn.simulated_quantize(%18, 0.00126639f /* ty=float32 */, 8.47564e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %20 = nn.simulated_quantize(meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 8.47564e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %21 = add(%19, %20) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %22 = nn.simulated_quantize(%21, 8.47564e-09f /* ty=float32 */, 8.47564e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %23 = nn.relu(%22) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %24 = nn.simulated_quantize(%23, 8.47564e-09f /* ty=float32 */, 6.4983e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 147, 147), float32] */;
  %25 = nn.max_pool2d(%24, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 64, 73, 73), float32] */;
  %26 = nn.simulated_quantize(%25, 6.4983e-09f /* ty=float32 */, 0.109023f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 73, 73), float32] */;
  %27 = nn.simulated_quantize(meta[relay.Constant][6] /* ty=Tensor[(80, 64, 1, 1), float32] */ /* ty=Tensor[(80, 64, 1, 1), float32] */, 1f /* ty=float32 */, 0.0102552f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(80, 64, 1, 1), float32] */;
  %28 = nn.conv2d(%26, %27, padding=[0, 0, 0, 0], channels=80, kernel_size=[1, 1]) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %29 = nn.simulated_quantize(%28, 0.00111806f /* ty=float32 */, 7.58872e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %30 = nn.simulated_quantize(meta[relay.Constant][7] /* ty=Tensor[(80, 1, 1), float32] */ /* ty=Tensor[(80, 1, 1), float32] */, 1f /* ty=float32 */, 7.58872e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(80, 1, 1), float32] */;
  %31 = add(%29, %30) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %32 = nn.simulated_quantize(%31, 7.58872e-09f /* ty=float32 */, 7.58872e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %34 = nn.simulated_quantize(%33, 7.58872e-09f /* ty=float32 */, 0.117261f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 80, 73, 73), float32] */;
  %35 = nn.simulated_quantize(meta[relay.Constant][8] /* ty=Tensor[(192, 80, 3, 3), float32] */ /* ty=Tensor[(192, 80, 3, 3), float32] */, 1f /* ty=float32 */, 0.00574007f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 80, 3, 3), float32] */;
  %36 = nn.conv2d(%34, %35, padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %37 = nn.simulated_quantize(%36, 0.000673084f /* ty=float32 */, 5.89513e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %38 = nn.simulated_quantize(meta[relay.Constant][9] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 5.89513e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %39 = add(%37, %38) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %40 = nn.simulated_quantize(%39, 5.89513e-09f /* ty=float32 */, 5.89513e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %42 = nn.simulated_quantize(%41, 5.89513e-09f /* ty=float32 */, 5.84559e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 71, 71), float32] */;
  %43 = nn.max_pool2d(%42, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %44 = nn.simulated_quantize(%43, 5.84559e-09f /* ty=float32 */, 0.0980727f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %45 = nn.simulated_quantize(meta[relay.Constant][10] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, 1f /* ty=float32 */, 0.00433901f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 192, 1, 1), float32] */;
  %46 = nn.conv2d(%44, %45, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %47 = nn.simulated_quantize(%46, 0.000425539f /* ty=float32 */, 4.94103e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %48 = nn.simulated_quantize(meta[relay.Constant][11] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 4.94103e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %49 = add(%47, %48) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %50 = nn.simulated_quantize(%49, 4.94103e-09f /* ty=float32 */, 4.94103e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %51 = nn.relu(%50) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %52 = nn.simulated_quantize(%51, 4.94103e-09f /* ty=float32 */, 9.00128e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %53 = nn.simulated_quantize(%43, 5.84559e-09f /* ty=float32 */, 0.0980727f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %54 = nn.simulated_quantize(meta[relay.Constant][12] /* ty=Tensor[(48, 192, 1, 1), float32] */ /* ty=Tensor[(48, 192, 1, 1), float32] */, 1f /* ty=float32 */, 0.00401616f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(48, 192, 1, 1), float32] */;
  %55 = nn.conv2d(%53, %54, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %56 = nn.simulated_quantize(%55, 0.000393875f /* ty=float32 */, 5.00831e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %57 = nn.simulated_quantize(meta[relay.Constant][13] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */, 1f /* ty=float32 */, 5.00831e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(48, 1, 1), float32] */;
  %58 = add(%56, %57) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %59 = nn.simulated_quantize(%58, 5.00831e-09f /* ty=float32 */, 5.00831e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %60 = nn.relu(%59) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %61 = nn.simulated_quantize(%60, 5.00831e-09f /* ty=float32 */, 0.0731393f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %62 = nn.simulated_quantize(meta[relay.Constant][14] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, 1f /* ty=float32 */, 0.00522591f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 48, 5, 5), float32] */;
  %63 = nn.conv2d(%61, %62, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %64 = nn.simulated_quantize(%63, 0.000382219f /* ty=float32 */, 4.36374e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %65 = nn.simulated_quantize(meta[relay.Constant][15] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 4.36374e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %66 = add(%64, %65) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %67 = nn.simulated_quantize(%66, 4.36374e-09f /* ty=float32 */, 4.36374e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %68 = nn.relu(%67) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %69 = nn.simulated_quantize(%68, 4.36374e-09f /* ty=float32 */, 9.00128e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %70 = nn.simulated_quantize(%43, 5.84559e-09f /* ty=float32 */, 0.0980727f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %71 = nn.simulated_quantize(meta[relay.Constant][16] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, 1f /* ty=float32 */, 0.00706938f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 192, 1, 1), float32] */;
  %72 = nn.conv2d(%70, %71, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %73 = nn.simulated_quantize(%72, 0.000693313f /* ty=float32 */, 6.26184e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %74 = nn.simulated_quantize(meta[relay.Constant][17] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 6.26184e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %75 = add(%73, %74) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %76 = nn.simulated_quantize(%75, 6.26184e-09f /* ty=float32 */, 6.26184e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %77 = nn.relu(%76) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %78 = nn.simulated_quantize(%77, 6.26184e-09f /* ty=float32 */, 0.0858584f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %79 = nn.simulated_quantize(meta[relay.Constant][18] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00603217f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 64, 3, 3), float32] */;
  %80 = nn.conv2d(%78, %79, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %81 = nn.simulated_quantize(%80, 0.000517912f /* ty=float32 */, 5.63654e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %82 = nn.simulated_quantize(meta[relay.Constant][19] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 1f /* ty=float32 */, 5.63654e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %83 = add(%81, %82) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %84 = nn.simulated_quantize(%83, 5.63654e-09f /* ty=float32 */, 5.63654e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %85 = nn.relu(%84) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %86 = nn.simulated_quantize(%85, 5.63654e-09f /* ty=float32 */, 0.102477f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %87 = nn.simulated_quantize(meta[relay.Constant][20] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, 1f /* ty=float32 */, 0.00584231f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 96, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %89 = nn.simulated_quantize(%88, 0.000598702f /* ty=float32 */, 9.00128e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %90 = nn.simulated_quantize(meta[relay.Constant][21] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 1f /* ty=float32 */, 9.00128e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %91 = add(%89, %90) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %92 = nn.simulated_quantize(%91, 9.00128e-09f /* ty=float32 */, 9.00128e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %93 = nn.relu(%92) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %94 = nn.simulated_quantize(%93, 9.00128e-09f /* ty=float32 */, 9.00128e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %95 = nn.simulated_quantize(%43, 5.84559e-09f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %96 = nn.avg_pool2d(%95, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %97 = nn.simulated_quantize(%96, 1f /* ty=float32 */, 0.0618268f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %98 = nn.simulated_quantize(meta[relay.Constant][22] /* ty=Tensor[(32, 192, 1, 1), float32] */ /* ty=Tensor[(32, 192, 1, 1), float32] */, 1f /* ty=float32 */, 0.011058f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 1, 1), float32] */;
  %99 = nn.conv2d(%97, %98, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %100 = nn.simulated_quantize(%99, 0.000683679f /* ty=float32 */, 5.49927e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %101 = nn.simulated_quantize(meta[relay.Constant][23] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */, 1f /* ty=float32 */, 5.49927e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 1, 1), float32] */;
  %102 = add(%100, %101) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %103 = nn.simulated_quantize(%102, 5.49927e-09f /* ty=float32 */, 5.49927e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %104 = nn.relu(%103) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %105 = nn.simulated_quantize(%104, 5.49927e-09f /* ty=float32 */, 9.00128e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 32, 35, 35), float32] */;
  %106 = (%52, %69, %94, %105);
  %107 = concatenate(%106, axis=1) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %108 = nn.simulated_quantize(%107, 9.00128e-09f /* ty=float32 */, 0.144319f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %109 = nn.simulated_quantize(meta[relay.Constant][24] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00584852f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %110 = nn.conv2d(%108, %109, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %111 = nn.simulated_quantize(%110, 0.000844052f /* ty=float32 */, 6.7476e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %112 = nn.simulated_quantize(meta[relay.Constant][25] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 6.7476e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %113 = add(%111, %112) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %114 = nn.simulated_quantize(%113, 6.7476e-09f /* ty=float32 */, 6.7476e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %115 = nn.relu(%114) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %116 = nn.simulated_quantize(%115, 6.7476e-09f /* ty=float32 */, 8.0604e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %117 = nn.simulated_quantize(%107, 9.00128e-09f /* ty=float32 */, 0.144319f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %118 = nn.simulated_quantize(meta[relay.Constant][26] /* ty=Tensor[(48, 256, 1, 1), float32] */ /* ty=Tensor[(48, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00465262f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(48, 256, 1, 1), float32] */;
  %119 = nn.conv2d(%117, %118, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %120 = nn.simulated_quantize(%119, 0.000671461f /* ty=float32 */, 6.83224e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %121 = nn.simulated_quantize(meta[relay.Constant][27] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */, 1f /* ty=float32 */, 6.83224e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(48, 1, 1), float32] */;
  %122 = add(%120, %121) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %123 = nn.simulated_quantize(%122, 6.83224e-09f /* ty=float32 */, 6.83224e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %125 = nn.simulated_quantize(%124, 6.83224e-09f /* ty=float32 */, 0.104371f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %126 = nn.simulated_quantize(meta[relay.Constant][28] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, 1f /* ty=float32 */, 0.00393846f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 48, 5, 5), float32] */;
  %127 = nn.conv2d(%125, %126, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %128 = nn.simulated_quantize(%127, 0.000411061f /* ty=float32 */, 5.49525e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %129 = nn.simulated_quantize(meta[relay.Constant][29] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 5.49525e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %130 = add(%128, %129) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %131 = nn.simulated_quantize(%130, 5.49525e-09f /* ty=float32 */, 5.49525e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %132 = nn.relu(%131) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %133 = nn.simulated_quantize(%132, 5.49525e-09f /* ty=float32 */, 8.0604e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %134 = nn.simulated_quantize(%107, 9.00128e-09f /* ty=float32 */, 0.144319f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %135 = nn.simulated_quantize(meta[relay.Constant][30] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00536366f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %136 = nn.conv2d(%134, %135, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %137 = nn.simulated_quantize(%136, 0.000774077f /* ty=float32 */, 5.6246e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %138 = nn.simulated_quantize(meta[relay.Constant][31] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 5.6246e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %139 = add(%137, %138) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %140 = nn.simulated_quantize(%139, 5.6246e-09f /* ty=float32 */, 5.6246e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %141 = nn.relu(%140) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %142 = nn.simulated_quantize(%141, 5.6246e-09f /* ty=float32 */, 0.0654255f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %143 = nn.simulated_quantize(meta[relay.Constant][32] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00558279f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 64, 3, 3), float32] */;
  %144 = nn.conv2d(%142, %143, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %145 = nn.simulated_quantize(%144, 0.000365257f /* ty=float32 */, 4.72001e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %146 = nn.simulated_quantize(meta[relay.Constant][33] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 1f /* ty=float32 */, 4.72001e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %147 = add(%145, %146) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %148 = nn.simulated_quantize(%147, 4.72001e-09f /* ty=float32 */, 4.72001e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %149 = nn.relu(%148) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %150 = nn.simulated_quantize(%149, 4.72001e-09f /* ty=float32 */, 0.0737892f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %151 = nn.simulated_quantize(meta[relay.Constant][34] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, 1f /* ty=float32 */, 0.00928544f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 96, 3, 3), float32] */;
  %152 = nn.conv2d(%150, %151, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %153 = nn.simulated_quantize(%152, 0.000685166f /* ty=float32 */, 8.0604e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %154 = nn.simulated_quantize(meta[relay.Constant][35] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 1f /* ty=float32 */, 8.0604e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %155 = add(%153, %154) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %156 = nn.simulated_quantize(%155, 8.0604e-09f /* ty=float32 */, 8.0604e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %158 = nn.simulated_quantize(%157, 8.0604e-09f /* ty=float32 */, 8.0604e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %159 = nn.simulated_quantize(%107, 9.00128e-09f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %160 = nn.avg_pool2d(%159, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %161 = nn.simulated_quantize(%160, 1f /* ty=float32 */, 0.0866785f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %162 = nn.simulated_quantize(meta[relay.Constant][36] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 1f /* ty=float32 */, 0.00933783f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 256, 1, 1), float32] */;
  %163 = nn.conv2d(%161, %162, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %164 = nn.simulated_quantize(%163, 0.00080939f /* ty=float32 */, 7.18542e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %165 = nn.simulated_quantize(meta[relay.Constant][37] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 7.18542e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %166 = add(%164, %165) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %167 = nn.simulated_quantize(%166, 7.18542e-09f /* ty=float32 */, 7.18542e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %168 = nn.relu(%167) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %169 = nn.simulated_quantize(%168, 7.18542e-09f /* ty=float32 */, 8.0604e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %170 = (%116, %133, %158, %169);
  %171 = concatenate(%170, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %172 = nn.simulated_quantize(%171, 8.0604e-09f /* ty=float32 */, 0.13219f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %173 = nn.simulated_quantize(meta[relay.Constant][38] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, 1f /* ty=float32 */, 0.00838885f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 288, 1, 1), float32] */;
  %174 = nn.conv2d(%172, %173, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %175 = nn.simulated_quantize(%174, 0.00110892f /* ty=float32 */, 7.61068e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %176 = nn.simulated_quantize(meta[relay.Constant][39] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 7.61068e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %177 = add(%175, %176) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %178 = nn.simulated_quantize(%177, 7.61068e-09f /* ty=float32 */, 7.61068e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %180 = nn.simulated_quantize(%179, 7.61068e-09f /* ty=float32 */, 7.61068e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %181 = nn.simulated_quantize(%171, 8.0604e-09f /* ty=float32 */, 0.13219f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %182 = nn.simulated_quantize(meta[relay.Constant][40] /* ty=Tensor[(48, 288, 1, 1), float32] */ /* ty=Tensor[(48, 288, 1, 1), float32] */, 1f /* ty=float32 */, 0.00501439f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(48, 288, 1, 1), float32] */;
  %183 = nn.conv2d(%181, %182, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1]) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %184 = nn.simulated_quantize(%183, 0.000662853f /* ty=float32 */, 5.41504e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %185 = nn.simulated_quantize(meta[relay.Constant][41] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */, 1f /* ty=float32 */, 5.41504e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(48, 1, 1), float32] */;
  %186 = add(%184, %185) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %187 = nn.simulated_quantize(%186, 5.41504e-09f /* ty=float32 */, 5.41504e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %188 = nn.relu(%187) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %189 = nn.simulated_quantize(%188, 5.41504e-09f /* ty=float32 */, 0.0840957f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 48, 35, 35), float32] */;
  %190 = nn.simulated_quantize(meta[relay.Constant][42] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, 1f /* ty=float32 */, 0.00286836f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 48, 5, 5), float32] */;
  %191 = nn.conv2d(%189, %190, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %192 = nn.simulated_quantize(%191, 0.000241217f /* ty=float32 */, 4.46515e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %193 = nn.simulated_quantize(meta[relay.Constant][43] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 4.46515e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %194 = add(%192, %193) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %195 = nn.simulated_quantize(%194, 4.46515e-09f /* ty=float32 */, 4.46515e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %196 = nn.relu(%195) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %197 = nn.simulated_quantize(%196, 4.46515e-09f /* ty=float32 */, 7.61068e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %198 = nn.simulated_quantize(%171, 8.0604e-09f /* ty=float32 */, 0.13219f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %199 = nn.simulated_quantize(meta[relay.Constant][44] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, 1f /* ty=float32 */, 0.00695815f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 288, 1, 1), float32] */;
  %200 = nn.conv2d(%198, %199, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %201 = nn.simulated_quantize(%200, 0.0009198f /* ty=float32 */, 6.54673e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %202 = nn.simulated_quantize(meta[relay.Constant][45] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 6.54673e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %203 = add(%201, %202) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %204 = nn.simulated_quantize(%203, 6.54673e-09f /* ty=float32 */, 6.54673e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %206 = nn.simulated_quantize(%205, 6.54673e-09f /* ty=float32 */, 0.100167f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %207 = nn.simulated_quantize(meta[relay.Constant][46] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00469817f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 64, 3, 3), float32] */;
  %208 = nn.conv2d(%206, %207, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %209 = nn.simulated_quantize(%208, 0.0004706f /* ty=float32 */, 5.32158e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %210 = nn.simulated_quantize(meta[relay.Constant][47] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 1f /* ty=float32 */, 5.32158e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %211 = add(%209, %210) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %212 = nn.simulated_quantize(%211, 5.32158e-09f /* ty=float32 */, 5.32158e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %213 = nn.relu(%212) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %214 = nn.simulated_quantize(%213, 5.32158e-09f /* ty=float32 */, 0.0884136f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %215 = nn.simulated_quantize(meta[relay.Constant][48] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, 1f /* ty=float32 */, 0.00468934f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 96, 3, 3), float32] */;
  %216 = nn.conv2d(%214, %215, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %217 = nn.simulated_quantize(%216, 0.000414602f /* ty=float32 */, 4.12971e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %218 = nn.simulated_quantize(meta[relay.Constant][49] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 1f /* ty=float32 */, 4.12971e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %219 = add(%217, %218) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %220 = nn.simulated_quantize(%219, 4.12971e-09f /* ty=float32 */, 4.12971e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %221 = nn.relu(%220) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %222 = nn.simulated_quantize(%221, 4.12971e-09f /* ty=float32 */, 7.61068e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %223 = nn.simulated_quantize(%171, 8.0604e-09f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %224 = nn.avg_pool2d(%223, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %225 = nn.simulated_quantize(%224, 1f /* ty=float32 */, 0.0766314f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %226 = nn.simulated_quantize(meta[relay.Constant][50] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, 1f /* ty=float32 */, 0.0113221f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 288, 1, 1), float32] */;
  %227 = nn.conv2d(%225, %226, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %228 = nn.simulated_quantize(%227, 0.000867631f /* ty=float32 */, 5.5098e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %229 = nn.simulated_quantize(meta[relay.Constant][51] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 5.5098e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %230 = add(%228, %229) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %231 = nn.simulated_quantize(%230, 5.5098e-09f /* ty=float32 */, 5.5098e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %232 = nn.relu(%231) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %233 = nn.simulated_quantize(%232, 5.5098e-09f /* ty=float32 */, 7.61068e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %234 = (%180, %197, %222, %233);
  %235 = concatenate(%234, axis=1) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %236 = nn.simulated_quantize(%235, 7.61068e-09f /* ty=float32 */, 0.0827759f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %237 = nn.simulated_quantize(meta[relay.Constant][52] /* ty=Tensor[(384, 288, 3, 3), float32] */ /* ty=Tensor[(384, 288, 3, 3), float32] */, 1f /* ty=float32 */, 0.00398437f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 288, 3, 3), float32] */;
  %238 = nn.conv2d(%236, %237, strides=[2, 2], padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %239 = nn.simulated_quantize(%238, 0.00032981f /* ty=float32 */, 4.90954e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %240 = nn.simulated_quantize(meta[relay.Constant][53] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 4.90954e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %241 = add(%239, %240) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %242 = nn.simulated_quantize(%241, 4.90954e-09f /* ty=float32 */, 4.90954e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %243 = nn.relu(%242) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %244 = nn.simulated_quantize(%243, 4.90954e-09f /* ty=float32 */, 5.87421e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 17, 17), float32] */;
  %245 = nn.simulated_quantize(%235, 7.61068e-09f /* ty=float32 */, 0.0827759f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %246 = nn.simulated_quantize(meta[relay.Constant][54] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, 1f /* ty=float32 */, 0.00519745f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(64, 288, 1, 1), float32] */;
  %247 = nn.conv2d(%245, %246, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %248 = nn.simulated_quantize(%247, 0.000430224f /* ty=float32 */, 5.60844e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %249 = nn.simulated_quantize(meta[relay.Constant][55] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 1f /* ty=float32 */, 5.60844e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(64, 1, 1), float32] */;
  %250 = add(%248, %249) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %251 = nn.simulated_quantize(%250, 5.60844e-09f /* ty=float32 */, 5.60844e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %252 = nn.relu(%251) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %253 = nn.simulated_quantize(%252, 5.60844e-09f /* ty=float32 */, 0.0798705f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 64, 35, 35), float32] */;
  %254 = nn.simulated_quantize(meta[relay.Constant][56] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, 1f /* ty=float32 */, 0.00464639f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 64, 3, 3), float32] */;
  %255 = nn.conv2d(%253, %254, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %256 = nn.simulated_quantize(%255, 0.00037111f /* ty=float32 */, 4.99602e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %257 = nn.simulated_quantize(meta[relay.Constant][57] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 1f /* ty=float32 */, 4.99602e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %258 = add(%256, %257) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %259 = nn.simulated_quantize(%258, 4.99602e-09f /* ty=float32 */, 4.99602e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %260 = nn.relu(%259) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %261 = nn.simulated_quantize(%260, 4.99602e-09f /* ty=float32 */, 0.0783957f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 96, 35, 35), float32] */;
  %262 = nn.simulated_quantize(meta[relay.Constant][58] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, 1f /* ty=float32 */, 0.00440968f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(96, 96, 3, 3), float32] */;
  %263 = nn.conv2d(%261, %262, strides=[2, 2], padding=[0, 0, 0, 0], channels=96, kernel_size=[3, 3]) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %264 = nn.simulated_quantize(%263, 0.0003457f /* ty=float32 */, 5.87421e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %265 = nn.simulated_quantize(meta[relay.Constant][59] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 1f /* ty=float32 */, 5.87421e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(96, 1, 1), float32] */;
  %266 = add(%264, %265) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %267 = nn.simulated_quantize(%266, 5.87421e-09f /* ty=float32 */, 5.87421e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %268 = nn.relu(%267) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %269 = nn.simulated_quantize(%268, 5.87421e-09f /* ty=float32 */, 5.87421e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 96, 17, 17), float32] */;
  %270 = nn.simulated_quantize(%235, 7.61068e-09f /* ty=float32 */, 4.93383e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %271 = nn.max_pool2d(%270, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 288, 17, 17), float32] */;
  %272 = nn.simulated_quantize(%271, 4.93383e-09f /* ty=float32 */, 5.87421e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 288, 17, 17), float32] */;
  %273 = (%244, %269, %272);
  %274 = concatenate(%273, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %275 = nn.simulated_quantize(%274, 5.87421e-09f /* ty=float32 */, 0.0958396f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %276 = nn.simulated_quantize(meta[relay.Constant][60] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00537618f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %277 = nn.conv2d(%275, %276, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %278 = nn.simulated_quantize(%277, 0.000515251f /* ty=float32 */, 4.88805e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %279 = nn.simulated_quantize(meta[relay.Constant][61] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 4.88805e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %280 = add(%278, %279) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %281 = nn.simulated_quantize(%280, 4.88805e-09f /* ty=float32 */, 4.88805e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %282 = nn.relu(%281) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %283 = nn.simulated_quantize(%282, 4.88805e-09f /* ty=float32 */, 8.9195e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %284 = nn.simulated_quantize(%274, 5.87421e-09f /* ty=float32 */, 0.0958396f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %285 = nn.simulated_quantize(meta[relay.Constant][62] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00610288f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 768, 1, 1), float32] */;
  %286 = nn.conv2d(%284, %285, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %287 = nn.simulated_quantize(%286, 0.000584898f /* ty=float32 */, 5.25398e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %288 = nn.simulated_quantize(meta[relay.Constant][63] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 5.25398e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %289 = add(%287, %288) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %290 = nn.simulated_quantize(%289, 5.25398e-09f /* ty=float32 */, 5.25398e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %291 = nn.relu(%290) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %292 = nn.simulated_quantize(%291, 5.25398e-09f /* ty=float32 */, 0.0796376f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %293 = nn.simulated_quantize(meta[relay.Constant][64] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, 1f /* ty=float32 */, 0.00749939f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 1, 7), float32] */;
  %294 = nn.conv2d(%292, %293, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %295 = nn.simulated_quantize(%294, 0.000597234f /* ty=float32 */, 5.06826e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %296 = nn.simulated_quantize(meta[relay.Constant][65] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 5.06826e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %297 = add(%295, %296) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %298 = nn.simulated_quantize(%297, 5.06826e-09f /* ty=float32 */, 5.06826e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %299 = nn.relu(%298) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %300 = nn.simulated_quantize(%299, 5.06826e-09f /* ty=float32 */, 0.0838972f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %301 = nn.simulated_quantize(meta[relay.Constant][66] /* ty=Tensor[(192, 128, 7, 1), float32] */ /* ty=Tensor[(192, 128, 7, 1), float32] */, 1f /* ty=float32 */, 0.00617447f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 128, 7, 1), float32] */;
  %302 = nn.conv2d(%300, %301, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %303 = nn.simulated_quantize(%302, 0.000518021f /* ty=float32 */, 4.83569e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %304 = nn.simulated_quantize(meta[relay.Constant][67] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 4.83569e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %305 = add(%303, %304) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %306 = nn.simulated_quantize(%305, 4.83569e-09f /* ty=float32 */, 4.83569e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %307 = nn.relu(%306) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %308 = nn.simulated_quantize(%307, 4.83569e-09f /* ty=float32 */, 8.9195e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %309 = nn.simulated_quantize(%274, 5.87421e-09f /* ty=float32 */, 0.0958396f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %310 = nn.simulated_quantize(meta[relay.Constant][68] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00469163f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 768, 1, 1), float32] */;
  %311 = nn.conv2d(%309, %310, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %312 = nn.simulated_quantize(%311, 0.000449644f /* ty=float32 */, 4.69498e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %313 = nn.simulated_quantize(meta[relay.Constant][69] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 4.69498e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %314 = add(%312, %313) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %315 = nn.simulated_quantize(%314, 4.69498e-09f /* ty=float32 */, 4.69498e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %316 = nn.relu(%315) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %317 = nn.simulated_quantize(%316, 4.69498e-09f /* ty=float32 */, 0.0646086f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %318 = nn.simulated_quantize(meta[relay.Constant][70] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, 1f /* ty=float32 */, 0.00493314f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 7, 1), float32] */;
  %319 = nn.conv2d(%317, %318, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %320 = nn.simulated_quantize(%319, 0.000318723f /* ty=float32 */, 3.92667e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %321 = nn.simulated_quantize(meta[relay.Constant][71] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 3.92667e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %322 = add(%320, %321) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %323 = nn.simulated_quantize(%322, 3.92667e-09f /* ty=float32 */, 3.92667e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %324 = nn.relu(%323) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %325 = nn.simulated_quantize(%324, 3.92667e-09f /* ty=float32 */, 0.0552085f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %326 = nn.simulated_quantize(meta[relay.Constant][72] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, 1f /* ty=float32 */, 0.00597663f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 1, 7), float32] */;
  %327 = nn.conv2d(%325, %326, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %328 = nn.simulated_quantize(%327, 0.000329961f /* ty=float32 */, 4.00599e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %329 = nn.simulated_quantize(meta[relay.Constant][73] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 4.00599e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %330 = add(%328, %329) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %331 = nn.simulated_quantize(%330, 4.00599e-09f /* ty=float32 */, 4.00599e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %332 = nn.relu(%331) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %333 = nn.simulated_quantize(%332, 4.00599e-09f /* ty=float32 */, 0.0631221f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %334 = nn.simulated_quantize(meta[relay.Constant][74] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, 1f /* ty=float32 */, 0.00586756f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(128, 128, 7, 1), float32] */;
  %335 = nn.conv2d(%333, %334, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1]) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %336 = nn.simulated_quantize(%335, 0.000370372f /* ty=float32 */, 5.84374e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %337 = nn.simulated_quantize(meta[relay.Constant][75] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 1f /* ty=float32 */, 5.84374e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(128, 1, 1), float32] */;
  %338 = add(%336, %337) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %339 = nn.simulated_quantize(%338, 5.84374e-09f /* ty=float32 */, 5.84374e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %340 = nn.relu(%339) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %341 = nn.simulated_quantize(%340, 5.84374e-09f /* ty=float32 */, 0.0963442f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 128, 17, 17), float32] */;
  %342 = nn.simulated_quantize(meta[relay.Constant][76] /* ty=Tensor[(192, 128, 1, 7), float32] */ /* ty=Tensor[(192, 128, 1, 7), float32] */, 1f /* ty=float32 */, 0.00404097f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 128, 1, 7), float32] */;
  %343 = nn.conv2d(%341, %342, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %344 = nn.simulated_quantize(%343, 0.000389324f /* ty=float32 */, 8.9195e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %345 = nn.simulated_quantize(meta[relay.Constant][77] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 8.9195e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %346 = add(%344, %345) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %347 = nn.simulated_quantize(%346, 8.9195e-09f /* ty=float32 */, 8.9195e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %348 = nn.relu(%347) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %349 = nn.simulated_quantize(%348, 8.9195e-09f /* ty=float32 */, 8.9195e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %350 = nn.simulated_quantize(%274, 5.87421e-09f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %351 = nn.avg_pool2d(%350, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %352 = nn.simulated_quantize(%351, 1f /* ty=float32 */, 0.057475f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %353 = nn.simulated_quantize(meta[relay.Constant][78] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.01131f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %354 = nn.conv2d(%352, %353, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %355 = nn.simulated_quantize(%354, 0.000650043f /* ty=float32 */, 4.57761e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %356 = nn.simulated_quantize(meta[relay.Constant][79] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 4.57761e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %357 = add(%355, %356) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %358 = nn.simulated_quantize(%357, 4.57761e-09f /* ty=float32 */, 4.57761e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %359 = nn.relu(%358) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %360 = nn.simulated_quantize(%359, 4.57761e-09f /* ty=float32 */, 8.9195e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %361 = (%283, %308, %349, %360);
  %362 = concatenate(%361, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %363 = nn.simulated_quantize(%362, 8.9195e-09f /* ty=float32 */, 0.149103f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %364 = nn.simulated_quantize(meta[relay.Constant][80] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00530625f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %365 = nn.conv2d(%363, %364, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %366 = nn.simulated_quantize(%365, 0.00079118f /* ty=float32 */, 5.71017e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %367 = nn.simulated_quantize(meta[relay.Constant][81] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 5.71017e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %368 = add(%366, %367) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %369 = nn.simulated_quantize(%368, 5.71017e-09f /* ty=float32 */, 5.71017e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %370 = nn.relu(%369) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %371 = nn.simulated_quantize(%370, 5.71017e-09f /* ty=float32 */, 1.05709e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %372 = nn.simulated_quantize(%362, 8.9195e-09f /* ty=float32 */, 0.149103f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %373 = nn.simulated_quantize(meta[relay.Constant][82] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00749864f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 768, 1, 1), float32] */;
  %374 = nn.conv2d(%372, %373, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %375 = nn.simulated_quantize(%374, 0.00111807f /* ty=float32 */, 6.19932e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %376 = nn.simulated_quantize(meta[relay.Constant][83] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 6.19932e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %377 = add(%375, %376) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %378 = nn.simulated_quantize(%377, 6.19932e-09f /* ty=float32 */, 6.19932e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %379 = nn.relu(%378) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %380 = nn.simulated_quantize(%379, 6.19932e-09f /* ty=float32 */, 0.103404f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %381 = nn.simulated_quantize(meta[relay.Constant][84] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, 1f /* ty=float32 */, 0.00484023f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 1, 7), float32] */;
  %382 = nn.conv2d(%380, %381, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %383 = nn.simulated_quantize(%382, 0.000500498f /* ty=float32 */, 4.41672e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %384 = nn.simulated_quantize(meta[relay.Constant][85] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 4.41672e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %385 = add(%383, %384) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %386 = nn.simulated_quantize(%385, 4.41672e-09f /* ty=float32 */, 4.41672e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %387 = nn.relu(%386) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %388 = nn.simulated_quantize(%387, 4.41672e-09f /* ty=float32 */, 0.0772715f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %389 = nn.simulated_quantize(meta[relay.Constant][86] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, 1f /* ty=float32 */, 0.00553171f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 160, 7, 1), float32] */;
  %390 = nn.conv2d(%388, %389, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %391 = nn.simulated_quantize(%390, 0.000427443f /* ty=float32 */, 4.53216e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %392 = nn.simulated_quantize(meta[relay.Constant][87] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 4.53216e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %393 = add(%391, %392) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %394 = nn.simulated_quantize(%393, 4.53216e-09f /* ty=float32 */, 4.53216e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %395 = nn.relu(%394) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %396 = nn.simulated_quantize(%395, 4.53216e-09f /* ty=float32 */, 1.05709e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %397 = nn.simulated_quantize(%362, 8.9195e-09f /* ty=float32 */, 0.149103f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %398 = nn.simulated_quantize(meta[relay.Constant][88] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00691003f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 768, 1, 1), float32] */;
  %399 = nn.conv2d(%397, %398, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %400 = nn.simulated_quantize(%399, 0.00103031f /* ty=float32 */, 5.28767e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %401 = nn.simulated_quantize(meta[relay.Constant][89] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 5.28767e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %402 = add(%400, %401) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %403 = nn.simulated_quantize(%402, 5.28767e-09f /* ty=float32 */, 5.28767e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %404 = nn.relu(%403) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %405 = nn.simulated_quantize(%404, 5.28767e-09f /* ty=float32 */, 0.0807742f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %406 = nn.simulated_quantize(meta[relay.Constant][90] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, 1f /* ty=float32 */, 0.0040512f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 7, 1), float32] */;
  %407 = nn.conv2d(%405, %406, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %408 = nn.simulated_quantize(%407, 0.000327232f /* ty=float32 */, 5.3636e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %409 = nn.simulated_quantize(meta[relay.Constant][91] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 5.3636e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %410 = add(%408, %409) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %411 = nn.simulated_quantize(%410, 5.3636e-09f /* ty=float32 */, 5.3636e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %412 = nn.relu(%411) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %413 = nn.simulated_quantize(%412, 5.3636e-09f /* ty=float32 */, 0.0894094f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %414 = nn.simulated_quantize(meta[relay.Constant][92] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, 1f /* ty=float32 */, 0.00555295f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 1, 7), float32] */;
  %415 = nn.conv2d(%413, %414, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %416 = nn.simulated_quantize(%415, 0.000496486f /* ty=float32 */, 7.19698e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %417 = nn.simulated_quantize(meta[relay.Constant][93] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 7.19698e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %418 = add(%416, %417) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %419 = nn.simulated_quantize(%418, 7.19698e-09f /* ty=float32 */, 7.19698e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %420 = nn.relu(%419) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %421 = nn.simulated_quantize(%420, 7.19698e-09f /* ty=float32 */, 0.111239f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %422 = nn.simulated_quantize(meta[relay.Constant][94] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, 1f /* ty=float32 */, 0.00499566f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 7, 1), float32] */;
  %423 = nn.conv2d(%421, %422, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %424 = nn.simulated_quantize(%423, 0.000555714f /* ty=float32 */, 1.00332e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %425 = nn.simulated_quantize(meta[relay.Constant][95] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 1.00332e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %426 = add(%424, %425) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %427 = nn.simulated_quantize(%426, 1.00332e-08f /* ty=float32 */, 1.00332e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %428 = nn.relu(%427) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %429 = nn.simulated_quantize(%428, 1.00332e-08f /* ty=float32 */, 0.157102f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %430 = nn.simulated_quantize(meta[relay.Constant][96] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, 1f /* ty=float32 */, 0.00521494f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 160, 1, 7), float32] */;
  %431 = nn.conv2d(%429, %430, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %432 = nn.simulated_quantize(%431, 0.000819276f /* ty=float32 */, 1.05709e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %433 = nn.simulated_quantize(meta[relay.Constant][97] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 1.05709e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %434 = add(%432, %433) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %435 = nn.simulated_quantize(%434, 1.05709e-08f /* ty=float32 */, 1.05709e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %436 = nn.relu(%435) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %437 = nn.simulated_quantize(%436, 1.05709e-08f /* ty=float32 */, 1.05709e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %438 = nn.simulated_quantize(%362, 8.9195e-09f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %439 = nn.avg_pool2d(%438, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %440 = nn.simulated_quantize(%439, 1f /* ty=float32 */, 0.0451576f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %441 = nn.simulated_quantize(meta[relay.Constant][98] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00751682f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %442 = nn.conv2d(%440, %441, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %443 = nn.simulated_quantize(%442, 0.000339442f /* ty=float32 */, 3.82568e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %444 = nn.simulated_quantize(meta[relay.Constant][99] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 3.82568e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %445 = add(%443, %444) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %446 = nn.simulated_quantize(%445, 3.82568e-09f /* ty=float32 */, 3.82568e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %447 = nn.relu(%446) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %448 = nn.simulated_quantize(%447, 3.82568e-09f /* ty=float32 */, 1.05709e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %449 = (%371, %396, %437, %448);
  %450 = concatenate(%449, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %451 = nn.simulated_quantize(%450, 1.05709e-08f /* ty=float32 */, 0.17452f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %452 = nn.simulated_quantize(meta[relay.Constant][100] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00918641f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %453 = nn.conv2d(%451, %452, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %454 = nn.simulated_quantize(%453, 0.00160321f /* ty=float32 */, 6.29081e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %455 = nn.simulated_quantize(meta[relay.Constant][101] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 6.29081e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %456 = add(%454, %455) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %457 = nn.simulated_quantize(%456, 6.29081e-09f /* ty=float32 */, 6.29081e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %458 = nn.relu(%457) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %459 = nn.simulated_quantize(%458, 6.29081e-09f /* ty=float32 */, 6.43986e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %460 = nn.simulated_quantize(%450, 1.05709e-08f /* ty=float32 */, 0.17452f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %461 = nn.simulated_quantize(meta[relay.Constant][102] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00456683f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 768, 1, 1), float32] */;
  %462 = nn.conv2d(%460, %461, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %463 = nn.simulated_quantize(%462, 0.000797002f /* ty=float32 */, 6.89674e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %464 = nn.simulated_quantize(meta[relay.Constant][103] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 6.89674e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %465 = add(%463, %464) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %466 = nn.simulated_quantize(%465, 6.89674e-09f /* ty=float32 */, 6.89674e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %467 = nn.relu(%466) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %468 = nn.simulated_quantize(%467, 6.89674e-09f /* ty=float32 */, 0.100028f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %469 = nn.simulated_quantize(meta[relay.Constant][104] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, 1f /* ty=float32 */, 0.00880511f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 1, 7), float32] */;
  %470 = nn.conv2d(%468, %469, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %471 = nn.simulated_quantize(%470, 0.000880759f /* ty=float32 */, 7.31816e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %472 = nn.simulated_quantize(meta[relay.Constant][105] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 7.31816e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %473 = add(%471, %472) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %474 = nn.simulated_quantize(%473, 7.31816e-09f /* ty=float32 */, 7.31816e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %475 = nn.relu(%474) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %476 = nn.simulated_quantize(%475, 7.31816e-09f /* ty=float32 */, 0.120732f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %477 = nn.simulated_quantize(meta[relay.Constant][106] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, 1f /* ty=float32 */, 0.00519522f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 160, 7, 1), float32] */;
  %478 = nn.conv2d(%476, %477, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %479 = nn.simulated_quantize(%478, 0.000627232f /* ty=float32 */, 6.16979e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %480 = nn.simulated_quantize(meta[relay.Constant][107] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 6.16979e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %481 = add(%479, %480) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %482 = nn.simulated_quantize(%481, 6.16979e-09f /* ty=float32 */, 6.16979e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %483 = nn.relu(%482) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %484 = nn.simulated_quantize(%483, 6.16979e-09f /* ty=float32 */, 6.43986e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %485 = nn.simulated_quantize(%450, 1.05709e-08f /* ty=float32 */, 0.17452f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %486 = nn.simulated_quantize(meta[relay.Constant][108] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00663011f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 768, 1, 1), float32] */;
  %487 = nn.conv2d(%485, %486, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %488 = nn.simulated_quantize(%487, 0.00115709f /* ty=float32 */, 7.91671e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %489 = nn.simulated_quantize(meta[relay.Constant][109] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 7.91671e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %490 = add(%488, %489) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %491 = nn.simulated_quantize(%490, 7.91671e-09f /* ty=float32 */, 7.91671e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %492 = nn.relu(%491) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %493 = nn.simulated_quantize(%492, 7.91671e-09f /* ty=float32 */, 0.12414f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %494 = nn.simulated_quantize(meta[relay.Constant][110] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, 1f /* ty=float32 */, 0.00737073f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 7, 1), float32] */;
  %495 = nn.conv2d(%493, %494, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %496 = nn.simulated_quantize(%495, 0.000915f /* ty=float32 */, 7.64215e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %497 = nn.simulated_quantize(meta[relay.Constant][111] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 7.64215e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %498 = add(%496, %497) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %499 = nn.simulated_quantize(%498, 7.64215e-09f /* ty=float32 */, 7.64215e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %500 = nn.relu(%499) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %501 = nn.simulated_quantize(%500, 7.64215e-09f /* ty=float32 */, 0.130681f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %502 = nn.simulated_quantize(meta[relay.Constant][112] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, 1f /* ty=float32 */, 0.00491007f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 1, 7), float32] */;
  %503 = nn.conv2d(%501, %502, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %504 = nn.simulated_quantize(%503, 0.00064165f /* ty=float32 */, 5.3899e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %505 = nn.simulated_quantize(meta[relay.Constant][113] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 5.3899e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %506 = add(%504, %505) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %507 = nn.simulated_quantize(%506, 5.3899e-09f /* ty=float32 */, 5.3899e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %508 = nn.relu(%507) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %509 = nn.simulated_quantize(%508, 5.3899e-09f /* ty=float32 */, 0.0883848f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %510 = nn.simulated_quantize(meta[relay.Constant][114] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, 1f /* ty=float32 */, 0.00747773f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(160, 160, 7, 1), float32] */;
  %511 = nn.conv2d(%509, %510, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1]) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %512 = nn.simulated_quantize(%511, 0.000660917f /* ty=float32 */, 7.24282e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %513 = nn.simulated_quantize(meta[relay.Constant][115] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1f /* ty=float32 */, 7.24282e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(160, 1, 1), float32] */;
  %514 = add(%512, %513) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %515 = nn.simulated_quantize(%514, 7.24282e-09f /* ty=float32 */, 7.24282e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %516 = nn.relu(%515) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %517 = nn.simulated_quantize(%516, 7.24282e-09f /* ty=float32 */, 0.0920394f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 160, 17, 17), float32] */;
  %518 = nn.simulated_quantize(meta[relay.Constant][116] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, 1f /* ty=float32 */, 0.00741879f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 160, 1, 7), float32] */;
  %519 = nn.conv2d(%517, %518, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %520 = nn.simulated_quantize(%519, 0.000682821f /* ty=float32 */, 5.33132e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %521 = nn.simulated_quantize(meta[relay.Constant][117] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 5.33132e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %522 = add(%520, %521) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %523 = nn.simulated_quantize(%522, 5.33132e-09f /* ty=float32 */, 5.33132e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %524 = nn.relu(%523) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %525 = nn.simulated_quantize(%524, 5.33132e-09f /* ty=float32 */, 6.43986e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %526 = nn.simulated_quantize(%450, 1.05709e-08f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %527 = nn.avg_pool2d(%526, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %528 = nn.simulated_quantize(%527, 1f /* ty=float32 */, 0.0481846f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %529 = nn.simulated_quantize(meta[relay.Constant][118] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.0139921f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %530 = nn.conv2d(%528, %529, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %531 = nn.simulated_quantize(%530, 0.000674205f /* ty=float32 */, 3.8614e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %532 = nn.simulated_quantize(meta[relay.Constant][119] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 3.8614e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %533 = add(%531, %532) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %534 = nn.simulated_quantize(%533, 3.8614e-09f /* ty=float32 */, 3.8614e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %535 = nn.relu(%534) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %536 = nn.simulated_quantize(%535, 3.8614e-09f /* ty=float32 */, 6.43986e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %537 = (%459, %484, %525, %536);
  %538 = concatenate(%537, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %539 = nn.simulated_quantize(%538, 6.43986e-09f /* ty=float32 */, 0.108856f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %540 = nn.simulated_quantize(meta[relay.Constant][120] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00770918f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %541 = nn.conv2d(%539, %540, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %542 = nn.simulated_quantize(%541, 0.000839188f /* ty=float32 */, 7.19665e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %543 = nn.simulated_quantize(meta[relay.Constant][121] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 7.19665e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %544 = add(%542, %543) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %545 = nn.simulated_quantize(%544, 7.19665e-09f /* ty=float32 */, 7.19665e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %546 = nn.relu(%545) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %547 = nn.simulated_quantize(%546, 7.19665e-09f /* ty=float32 */, 7.19665e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %548 = nn.simulated_quantize(%538, 6.43986e-09f /* ty=float32 */, 0.108856f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %549 = nn.simulated_quantize(meta[relay.Constant][122] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00773977f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %550 = nn.conv2d(%548, %549, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %551 = nn.simulated_quantize(%550, 0.000842519f /* ty=float32 */, 5.59384e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %552 = nn.simulated_quantize(meta[relay.Constant][123] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 5.59384e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %553 = add(%551, %552) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %554 = nn.simulated_quantize(%553, 5.59384e-09f /* ty=float32 */, 5.59384e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %555 = nn.relu(%554) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %556 = nn.simulated_quantize(%555, 5.59384e-09f /* ty=float32 */, 0.0909263f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %557 = nn.simulated_quantize(meta[relay.Constant][124] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, 1f /* ty=float32 */, 0.00848777f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 1, 7), float32] */;
  %558 = nn.conv2d(%556, %557, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %559 = nn.simulated_quantize(%558, 0.000771762f /* ty=float32 */, 6.45887e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %560 = nn.simulated_quantize(meta[relay.Constant][125] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 6.45887e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %561 = add(%559, %560) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %562 = nn.simulated_quantize(%561, 6.45887e-09f /* ty=float32 */, 6.45887e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %563 = nn.relu(%562) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %564 = nn.simulated_quantize(%563, 6.45887e-09f /* ty=float32 */, 0.109682f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %565 = nn.simulated_quantize(meta[relay.Constant][126] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, 1f /* ty=float32 */, 0.0026531f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 7, 1), float32] */;
  %566 = nn.conv2d(%564, %565, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %567 = nn.simulated_quantize(%566, 0.000290997f /* ty=float32 */, 4.6028e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %568 = nn.simulated_quantize(meta[relay.Constant][127] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 4.6028e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %569 = add(%567, %568) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %570 = nn.simulated_quantize(%569, 4.6028e-09f /* ty=float32 */, 4.6028e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %571 = nn.relu(%570) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %572 = nn.simulated_quantize(%571, 4.6028e-09f /* ty=float32 */, 7.19665e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %573 = nn.simulated_quantize(%538, 6.43986e-09f /* ty=float32 */, 0.108856f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %574 = nn.simulated_quantize(meta[relay.Constant][128] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00476074f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %575 = nn.conv2d(%573, %574, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %576 = nn.simulated_quantize(%575, 0.000518234f /* ty=float32 */, 6.26912e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %577 = nn.simulated_quantize(meta[relay.Constant][129] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 6.26912e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %578 = add(%576, %577) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %579 = nn.simulated_quantize(%578, 6.26912e-09f /* ty=float32 */, 6.26912e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %580 = nn.relu(%579) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %581 = nn.simulated_quantize(%580, 6.26912e-09f /* ty=float32 */, 0.0984178f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %582 = nn.simulated_quantize(meta[relay.Constant][130] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, 1f /* ty=float32 */, 0.00435483f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 7, 1), float32] */;
  %583 = nn.conv2d(%581, %582, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %584 = nn.simulated_quantize(%583, 0.000428593f /* ty=float32 */, 5.721e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %585 = nn.simulated_quantize(meta[relay.Constant][131] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 5.721e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %586 = add(%584, %585) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %587 = nn.simulated_quantize(%586, 5.721e-09f /* ty=float32 */, 5.721e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %588 = nn.relu(%587) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %589 = nn.simulated_quantize(%588, 5.721e-09f /* ty=float32 */, 0.095106f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %590 = nn.simulated_quantize(meta[relay.Constant][132] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, 1f /* ty=float32 */, 0.00510192f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 1, 7), float32] */;
  %591 = nn.conv2d(%589, %590, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %592 = nn.simulated_quantize(%591, 0.000485223f /* ty=float32 */, 6.05562e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %593 = nn.simulated_quantize(meta[relay.Constant][133] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 6.05562e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %594 = add(%592, %593) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %595 = nn.simulated_quantize(%594, 6.05562e-09f /* ty=float32 */, 6.05562e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %596 = nn.relu(%595) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %597 = nn.simulated_quantize(%596, 6.05562e-09f /* ty=float32 */, 0.101932f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %598 = nn.simulated_quantize(meta[relay.Constant][134] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, 1f /* ty=float32 */, 0.00319643f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 7, 1), float32] */;
  %599 = nn.conv2d(%597, %598, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %600 = nn.simulated_quantize(%599, 0.000325819f /* ty=float32 */, 6.22569e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %601 = nn.simulated_quantize(meta[relay.Constant][135] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 6.22569e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %602 = add(%600, %601) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %603 = nn.simulated_quantize(%602, 6.22569e-09f /* ty=float32 */, 6.22569e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %604 = nn.relu(%603) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %605 = nn.simulated_quantize(%604, 6.22569e-09f /* ty=float32 */, 0.108909f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %606 = nn.simulated_quantize(meta[relay.Constant][136] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, 1f /* ty=float32 */, 0.00268704f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 1, 7), float32] */;
  %607 = nn.conv2d(%605, %606, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %608 = nn.simulated_quantize(%607, 0.000292643f /* ty=float32 */, 4.05702e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %609 = nn.simulated_quantize(meta[relay.Constant][137] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 4.05702e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %610 = add(%608, %609) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %611 = nn.simulated_quantize(%610, 4.05702e-09f /* ty=float32 */, 4.05702e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %612 = nn.relu(%611) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %613 = nn.simulated_quantize(%612, 4.05702e-09f /* ty=float32 */, 7.19665e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %614 = nn.simulated_quantize(%538, 6.43986e-09f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %615 = nn.avg_pool2d(%614, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %616 = nn.simulated_quantize(%615, 1f /* ty=float32 */, 0.0509987f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %617 = nn.simulated_quantize(meta[relay.Constant][138] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00717943f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %618 = nn.conv2d(%616, %617, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %619 = nn.simulated_quantize(%618, 0.000366142f /* ty=float32 */, 3.37533e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %620 = nn.simulated_quantize(meta[relay.Constant][139] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 3.37533e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %621 = add(%619, %620) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %622 = nn.simulated_quantize(%621, 3.37533e-09f /* ty=float32 */, 3.37533e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %623 = nn.relu(%622) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %624 = nn.simulated_quantize(%623, 3.37533e-09f /* ty=float32 */, 7.19665e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %625 = (%547, %572, %613, %624);
  %626 = concatenate(%625, axis=1) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %627 = nn.simulated_quantize(%626, 7.19665e-09f /* ty=float32 */, 0.0695813f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %628 = nn.simulated_quantize(meta[relay.Constant][140] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00696009f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %629 = nn.conv2d(%627, %628, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %630 = nn.simulated_quantize(%629, 0.000484292f /* ty=float32 */, 6.74937e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %631 = nn.simulated_quantize(meta[relay.Constant][141] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 6.74937e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %632 = add(%630, %631) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %633 = nn.simulated_quantize(%632, 6.74937e-09f /* ty=float32 */, 6.74937e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %634 = nn.relu(%633) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %635 = nn.simulated_quantize(%634, 6.74937e-09f /* ty=float32 */, 0.0927286f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %636 = nn.simulated_quantize(meta[relay.Constant][142] /* ty=Tensor[(320, 192, 3, 3), float32] */ /* ty=Tensor[(320, 192, 3, 3), float32] */, 1f /* ty=float32 */, 0.00493863f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(320, 192, 3, 3), float32] */;
  %637 = nn.conv2d(%635, %636, strides=[2, 2], padding=[0, 0, 0, 0], channels=320, kernel_size=[3, 3]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %638 = nn.simulated_quantize(%637, 0.000457952f /* ty=float32 */, 7.76225e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %639 = nn.simulated_quantize(meta[relay.Constant][143] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */, 1f /* ty=float32 */, 7.76225e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(320, 1, 1), float32] */;
  %640 = add(%638, %639) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %641 = nn.simulated_quantize(%640, 7.76225e-09f /* ty=float32 */, 7.76225e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %642 = nn.relu(%641) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %643 = nn.simulated_quantize(%642, 7.76225e-09f /* ty=float32 */, 7.76225e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %644 = nn.simulated_quantize(%626, 7.19665e-09f /* ty=float32 */, 0.0695813f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %645 = nn.simulated_quantize(meta[relay.Constant][144] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 1f /* ty=float32 */, 0.00976139f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 768, 1, 1), float32] */;
  %646 = nn.conv2d(%644, %645, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %647 = nn.simulated_quantize(%646, 0.000679211f /* ty=float32 */, 6.75832e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %648 = nn.simulated_quantize(meta[relay.Constant][145] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 6.75832e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %649 = add(%647, %648) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %650 = nn.simulated_quantize(%649, 6.75832e-09f /* ty=float32 */, 6.75832e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %651 = nn.relu(%650) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %652 = nn.simulated_quantize(%651, 6.75832e-09f /* ty=float32 */, 0.0941049f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %653 = nn.simulated_quantize(meta[relay.Constant][146] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, 1f /* ty=float32 */, 0.00384012f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 1, 7), float32] */;
  %654 = nn.conv2d(%652, %653, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %655 = nn.simulated_quantize(%654, 0.000361374f /* ty=float32 */, 6.27892e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %656 = nn.simulated_quantize(meta[relay.Constant][147] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 6.27892e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %657 = add(%655, %656) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %658 = nn.simulated_quantize(%657, 6.27892e-09f /* ty=float32 */, 6.27892e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %659 = nn.relu(%658) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %660 = nn.simulated_quantize(%659, 6.27892e-09f /* ty=float32 */, 0.106464f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %661 = nn.simulated_quantize(meta[relay.Constant][148] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, 1f /* ty=float32 */, 0.003142f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 7, 1), float32] */;
  %662 = nn.conv2d(%660, %661, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1]) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %663 = nn.simulated_quantize(%662, 0.000334511f /* ty=float32 */, 4.70926e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %664 = nn.simulated_quantize(meta[relay.Constant][149] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 4.70926e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %665 = add(%663, %664) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %666 = nn.simulated_quantize(%665, 4.70926e-09f /* ty=float32 */, 4.70926e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %667 = nn.relu(%666) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %668 = nn.simulated_quantize(%667, 4.70926e-09f /* ty=float32 */, 0.0758742f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 192, 17, 17), float32] */;
  %669 = nn.simulated_quantize(meta[relay.Constant][150] /* ty=Tensor[(192, 192, 3, 3), float32] */ /* ty=Tensor[(192, 192, 3, 3), float32] */, 1f /* ty=float32 */, 0.00747768f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 192, 3, 3), float32] */;
  %670 = nn.conv2d(%668, %669, strides=[2, 2], padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %671 = nn.simulated_quantize(%670, 0.000567363f /* ty=float32 */, 5.06169e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %672 = nn.simulated_quantize(meta[relay.Constant][151] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 5.06169e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %673 = add(%671, %672) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %674 = nn.simulated_quantize(%673, 5.06169e-09f /* ty=float32 */, 5.06169e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %675 = nn.relu(%674) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %676 = nn.simulated_quantize(%675, 5.06169e-09f /* ty=float32 */, 7.76225e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %677 = nn.simulated_quantize(%626, 7.19665e-09f /* ty=float32 */, 4.14737e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %678 = nn.max_pool2d(%677, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %679 = nn.simulated_quantize(%678, 4.14737e-09f /* ty=float32 */, 7.76225e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %680 = (%643, %676, %679);
  %681 = concatenate(%680, axis=1) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %682 = nn.simulated_quantize(%681, 7.76225e-09f /* ty=float32 */, 0.129485f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %683 = nn.simulated_quantize(meta[relay.Constant][152] /* ty=Tensor[(320, 1280, 1, 1), float32] */ /* ty=Tensor[(320, 1280, 1, 1), float32] */, 1f /* ty=float32 */, 0.0073916f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(320, 1280, 1, 1), float32] */;
  %684 = nn.conv2d(%682, %683, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %685 = nn.simulated_quantize(%684, 0.000957103f /* ty=float32 */, 4.63508e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %686 = nn.simulated_quantize(meta[relay.Constant][153] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */, 1f /* ty=float32 */, 4.63508e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(320, 1, 1), float32] */;
  %687 = add(%685, %686) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %688 = nn.simulated_quantize(%687, 4.63508e-09f /* ty=float32 */, 4.63508e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %689 = nn.relu(%688) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %690 = nn.simulated_quantize(%689, 4.63508e-09f /* ty=float32 */, 8.75088e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %691 = nn.simulated_quantize(%681, 7.76225e-09f /* ty=float32 */, 0.129485f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %692 = nn.simulated_quantize(meta[relay.Constant][154] /* ty=Tensor[(384, 1280, 1, 1), float32] */ /* ty=Tensor[(384, 1280, 1, 1), float32] */, 1f /* ty=float32 */, 0.00610927f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 1280, 1, 1), float32] */;
  %693 = nn.conv2d(%691, %692, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %694 = nn.simulated_quantize(%693, 0.00079106f /* ty=float32 */, 4.72628e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %695 = nn.simulated_quantize(meta[relay.Constant][155] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 4.72628e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %696 = add(%694, %695) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %697 = nn.simulated_quantize(%696, 4.72628e-09f /* ty=float32 */, 4.72628e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %698 = nn.relu(%697) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %699 = nn.simulated_quantize(%698, 4.72628e-09f /* ty=float32 */, 0.0767814f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %700 = nn.simulated_quantize(meta[relay.Constant][156] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, 1f /* ty=float32 */, 0.00507655f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 1, 3), float32] */;
  %701 = nn.conv2d(%699, %700, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %702 = nn.simulated_quantize(%701, 0.000389785f /* ty=float32 */, 4.80139e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %703 = nn.simulated_quantize(meta[relay.Constant][157] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 4.80139e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %704 = add(%702, %703) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %705 = nn.simulated_quantize(%704, 4.80139e-09f /* ty=float32 */, 4.80139e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %706 = nn.relu(%705) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %707 = nn.simulated_quantize(%706, 4.80139e-09f /* ty=float32 */, 8.75088e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %708 = nn.simulated_quantize(%698, 4.72628e-09f /* ty=float32 */, 0.0767814f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %709 = nn.simulated_quantize(meta[relay.Constant][158] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, 1f /* ty=float32 */, 0.00815033f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 3, 1), float32] */;
  %710 = nn.conv2d(%708, %709, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %711 = nn.simulated_quantize(%710, 0.000625794f /* ty=float32 */, 8.75088e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %712 = nn.simulated_quantize(meta[relay.Constant][159] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 8.75088e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %713 = add(%711, %712) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %714 = nn.simulated_quantize(%713, 8.75088e-09f /* ty=float32 */, 8.75088e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %715 = nn.relu(%714) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %716 = nn.simulated_quantize(%715, 8.75088e-09f /* ty=float32 */, 8.75088e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %717 = (%707, %716);
  %718 = concatenate(%717, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %719 = nn.simulated_quantize(%718, 8.75088e-09f /* ty=float32 */, 8.75088e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %720 = nn.simulated_quantize(%681, 7.76225e-09f /* ty=float32 */, 0.129485f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %721 = nn.simulated_quantize(meta[relay.Constant][160] /* ty=Tensor[(448, 1280, 1, 1), float32] */ /* ty=Tensor[(448, 1280, 1, 1), float32] */, 1f /* ty=float32 */, 0.00571918f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(448, 1280, 1, 1), float32] */;
  %722 = nn.conv2d(%720, %721, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %723 = nn.simulated_quantize(%722, 0.000740549f /* ty=float32 */, 4.25375e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %724 = nn.simulated_quantize(meta[relay.Constant][161] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */, 1f /* ty=float32 */, 4.25375e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(448, 1, 1), float32] */;
  %725 = add(%723, %724) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %726 = nn.simulated_quantize(%725, 4.25375e-09f /* ty=float32 */, 4.25375e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %727 = nn.relu(%726) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %728 = nn.simulated_quantize(%727, 4.25375e-09f /* ty=float32 */, 0.0580821f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %729 = nn.simulated_quantize(meta[relay.Constant][162] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, 1f /* ty=float32 */, 0.00254263f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 448, 3, 3), float32] */;
  %730 = nn.conv2d(%728, %729, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %731 = nn.simulated_quantize(%730, 0.000147681f /* ty=float32 */, 3.88767e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %732 = nn.simulated_quantize(meta[relay.Constant][163] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 3.88767e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %733 = add(%731, %732) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %734 = nn.simulated_quantize(%733, 3.88767e-09f /* ty=float32 */, 3.88767e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %735 = nn.relu(%734) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %736 = nn.simulated_quantize(%735, 3.88767e-09f /* ty=float32 */, 0.0542847f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %737 = nn.simulated_quantize(meta[relay.Constant][164] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, 1f /* ty=float32 */, 0.00525039f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 1, 3), float32] */;
  %738 = nn.conv2d(%736, %737, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %739 = nn.simulated_quantize(%738, 0.000285016f /* ty=float32 */, 4.25846e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %740 = nn.simulated_quantize(meta[relay.Constant][165] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 4.25846e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %741 = add(%739, %740) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %742 = nn.simulated_quantize(%741, 4.25846e-09f /* ty=float32 */, 4.25846e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %743 = nn.relu(%742) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %744 = nn.simulated_quantize(%743, 4.25846e-09f /* ty=float32 */, 4.41786e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %745 = nn.simulated_quantize(%735, 3.88767e-09f /* ty=float32 */, 0.0542847f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %746 = nn.simulated_quantize(meta[relay.Constant][166] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, 1f /* ty=float32 */, 0.00360351f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 3, 1), float32] */;
  %747 = nn.conv2d(%745, %746, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %748 = nn.simulated_quantize(%747, 0.000195615f /* ty=float32 */, 4.41786e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %749 = nn.simulated_quantize(meta[relay.Constant][167] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 4.41786e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %750 = add(%748, %749) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %751 = nn.simulated_quantize(%750, 4.41786e-09f /* ty=float32 */, 4.41786e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %752 = nn.relu(%751) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %753 = nn.simulated_quantize(%752, 4.41786e-09f /* ty=float32 */, 4.41786e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %754 = (%744, %753);
  %755 = concatenate(%754, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %756 = nn.simulated_quantize(%755, 4.41786e-09f /* ty=float32 */, 8.75088e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %757 = nn.simulated_quantize(%681, 7.76225e-09f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %758 = nn.avg_pool2d(%757, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %759 = nn.simulated_quantize(%758, 1f /* ty=float32 */, 0.0628236f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %760 = nn.simulated_quantize(meta[relay.Constant][168] /* ty=Tensor[(192, 1280, 1, 1), float32] */ /* ty=Tensor[(192, 1280, 1, 1), float32] */, 1f /* ty=float32 */, 0.00902899f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 1280, 1, 1), float32] */;
  %761 = nn.conv2d(%759, %760, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %762 = nn.simulated_quantize(%761, 0.000567233f /* ty=float32 */, 3.7089e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %763 = nn.simulated_quantize(meta[relay.Constant][169] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 3.7089e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %764 = add(%762, %763) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %765 = nn.simulated_quantize(%764, 3.7089e-09f /* ty=float32 */, 3.7089e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %766 = nn.relu(%765) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %767 = nn.simulated_quantize(%766, 3.7089e-09f /* ty=float32 */, 8.75088e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %768 = (%690, %719, %756, %767);
  %769 = concatenate(%768, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %770 = nn.simulated_quantize(%769, 8.75088e-09f /* ty=float32 */, 0.1469f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %771 = nn.simulated_quantize(meta[relay.Constant][170] /* ty=Tensor[(320, 2048, 1, 1), float32] */ /* ty=Tensor[(320, 2048, 1, 1), float32] */, 1f /* ty=float32 */, 0.0190236f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(320, 2048, 1, 1), float32] */;
  %772 = nn.conv2d(%770, %771, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1]) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %773 = nn.simulated_quantize(%772, 0.00279457f /* ty=float32 */, 6.0167e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %774 = nn.simulated_quantize(meta[relay.Constant][171] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */, 1f /* ty=float32 */, 6.0167e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(320, 1, 1), float32] */;
  %775 = add(%773, %774) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %776 = nn.simulated_quantize(%775, 6.0167e-09f /* ty=float32 */, 6.0167e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %777 = nn.relu(%776) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %778 = nn.simulated_quantize(%777, 6.0167e-09f /* ty=float32 */, 1.62613e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 320, 8, 8), float32] */;
  %779 = nn.simulated_quantize(%769, 8.75088e-09f /* ty=float32 */, 0.1469f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %780 = nn.simulated_quantize(meta[relay.Constant][172] /* ty=Tensor[(384, 2048, 1, 1), float32] */ /* ty=Tensor[(384, 2048, 1, 1), float32] */, 1f /* ty=float32 */, 0.00848985f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 2048, 1, 1), float32] */;
  %781 = nn.conv2d(%779, %780, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %782 = nn.simulated_quantize(%781, 0.00124716f /* ty=float32 */, 7.30744e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %783 = nn.simulated_quantize(meta[relay.Constant][173] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 7.30744e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %784 = add(%782, %783) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %785 = nn.simulated_quantize(%784, 7.30744e-09f /* ty=float32 */, 7.30744e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %786 = nn.relu(%785) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %787 = nn.simulated_quantize(%786, 7.30744e-09f /* ty=float32 */, 0.121899f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %788 = nn.simulated_quantize(meta[relay.Constant][174] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, 1f /* ty=float32 */, 0.0151868f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 1, 3), float32] */;
  %789 = nn.conv2d(%787, %788, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %790 = nn.simulated_quantize(%789, 0.00185125f /* ty=float32 */, 1.61283e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %791 = nn.simulated_quantize(meta[relay.Constant][175] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 1.61283e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %792 = add(%790, %791) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %793 = nn.simulated_quantize(%792, 1.61283e-08f /* ty=float32 */, 1.61283e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %794 = nn.relu(%793) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %795 = nn.simulated_quantize(%794, 1.61283e-08f /* ty=float32 */, 1.62613e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %796 = nn.simulated_quantize(%786, 7.30744e-09f /* ty=float32 */, 0.121899f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %797 = nn.simulated_quantize(meta[relay.Constant][176] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, 1f /* ty=float32 */, 0.0160716f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 3, 1), float32] */;
  %798 = nn.conv2d(%796, %797, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %799 = nn.simulated_quantize(%798, 0.0019591f /* ty=float32 */, 1.62613e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %800 = nn.simulated_quantize(meta[relay.Constant][177] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 1.62613e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %801 = add(%799, %800) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %802 = nn.simulated_quantize(%801, 1.62613e-08f /* ty=float32 */, 1.62613e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %803 = nn.relu(%802) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %804 = nn.simulated_quantize(%803, 1.62613e-08f /* ty=float32 */, 1.62613e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %805 = (%795, %804);
  %806 = concatenate(%805, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %807 = nn.simulated_quantize(%806, 1.62613e-08f /* ty=float32 */, 1.62613e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %808 = nn.simulated_quantize(%769, 8.75088e-09f /* ty=float32 */, 0.1469f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %809 = nn.simulated_quantize(meta[relay.Constant][178] /* ty=Tensor[(448, 2048, 1, 1), float32] */ /* ty=Tensor[(448, 2048, 1, 1), float32] */, 1f /* ty=float32 */, 0.00626862f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(448, 2048, 1, 1), float32] */;
  %810 = nn.conv2d(%808, %809, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1]) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %811 = nn.simulated_quantize(%810, 0.000920861f /* ty=float32 */, 5.87778e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %812 = nn.simulated_quantize(meta[relay.Constant][179] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */, 1f /* ty=float32 */, 5.87778e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(448, 1, 1), float32] */;
  %813 = add(%811, %812) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %814 = nn.simulated_quantize(%813, 5.87778e-09f /* ty=float32 */, 5.87778e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %815 = nn.relu(%814) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %816 = nn.simulated_quantize(%815, 5.87778e-09f /* ty=float32 */, 0.0818889f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 448, 8, 8), float32] */;
  %817 = nn.simulated_quantize(meta[relay.Constant][180] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, 1f /* ty=float32 */, 0.00502423f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 448, 3, 3), float32] */;
  %818 = nn.conv2d(%816, %817, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %819 = nn.simulated_quantize(%818, 0.000411429f /* ty=float32 */, 8.56509e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %820 = nn.simulated_quantize(meta[relay.Constant][181] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 8.56509e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %821 = add(%819, %820) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %822 = nn.simulated_quantize(%821, 8.56509e-09f /* ty=float32 */, 8.56509e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %823 = nn.relu(%822) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %824 = nn.simulated_quantize(%823, 8.56509e-09f /* ty=float32 */, 0.145289f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %825 = nn.simulated_quantize(meta[relay.Constant][182] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, 1f /* ty=float32 */, 0.00715138f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 1, 3), float32] */;
  %826 = nn.conv2d(%824, %825, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %827 = nn.simulated_quantize(%826, 0.00103902f /* ty=float32 */, 7.84438e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %828 = nn.simulated_quantize(meta[relay.Constant][183] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 7.84438e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %829 = add(%827, %828) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %830 = nn.simulated_quantize(%829, 7.84438e-09f /* ty=float32 */, 7.84438e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %831 = nn.relu(%830) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %832 = nn.simulated_quantize(%831, 7.84438e-09f /* ty=float32 */, 7.84438e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %833 = nn.simulated_quantize(%823, 8.56509e-09f /* ty=float32 */, 0.145289f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="int32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %834 = nn.simulated_quantize(meta[relay.Constant][184] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, 1f /* ty=float32 */, 0.00734243f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(384, 384, 3, 1), float32] */;
  %835 = nn.conv2d(%833, %834, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1]) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %836 = nn.simulated_quantize(%835, 0.00106678f /* ty=float32 */, 6.81761e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %837 = nn.simulated_quantize(meta[relay.Constant][185] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1f /* ty=float32 */, 6.81761e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(384, 1, 1), float32] */;
  %838 = add(%836, %837) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %839 = nn.simulated_quantize(%838, 6.81761e-09f /* ty=float32 */, 6.81761e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %840 = nn.relu(%839) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %841 = nn.simulated_quantize(%840, 6.81761e-09f /* ty=float32 */, 7.84438e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 384, 8, 8), float32] */;
  %842 = (%832, %841);
  %843 = concatenate(%842, axis=1) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %844 = nn.simulated_quantize(%843, 7.84438e-09f /* ty=float32 */, 1.62613e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 768, 8, 8), float32] */;
  %845 = nn.simulated_quantize(%769, 8.75088e-09f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %846 = nn.avg_pool2d(%845, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %847 = nn.simulated_quantize(%846, 1f /* ty=float32 */, 0.078022f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %848 = nn.simulated_quantize(meta[relay.Constant][186] /* ty=Tensor[(192, 2048, 1, 1), float32] */ /* ty=Tensor[(192, 2048, 1, 1), float32] */, 1f /* ty=float32 */, 0.0223402f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, in_dtype="float32", out_dtype="int8", axis=None) /* ty=Tensor[(192, 2048, 1, 1), float32] */;
  %849 = nn.conv2d(%847, %848, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1]) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %850 = nn.simulated_quantize(%849, 0.00174302f /* ty=float32 */, 4.03363e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %851 = nn.simulated_quantize(meta[relay.Constant][187] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1f /* ty=float32 */, 4.03363e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="float32", out_dtype="int32", axis=None) /* ty=Tensor[(192, 1, 1), float32] */;
  %852 = add(%850, %851) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %853 = nn.simulated_quantize(%852, 4.03363e-09f /* ty=float32 */, 4.03363e-09f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %854 = nn.relu(%853) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %855 = nn.simulated_quantize(%854, 4.03363e-09f /* ty=float32 */, 1.62613e-08f /* ty=float32 */, -2.14748e+09f /* ty=float32 */, 2.14748e+09f /* ty=float32 */, in_dtype="int32", out_dtype="int32", axis=None) /* ty=Tensor[(32, 192, 8, 8), float32] */;
  %856 = (%778, %807, %844, %855);
  %857 = concatenate(%856, axis=1) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %858 = nn.simulated_quantize(%857, 1.62613e-08f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="int32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %859 = nn.avg_pool2d(%858, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %860 = nn.simulated_quantize(%859, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %861 = nn.batch_flatten(%860) /* ty=Tensor[(32, 2048), float32] */;
  %862 = nn.simulated_quantize(%861, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 2048), float32] */;
  %863 = nn.simulated_quantize(meta[relay.Constant][188] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000, 2048), float32] */;
  %864 = nn.dense(%862, %863, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  %865 = nn.simulated_quantize(%864, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */;
  %866 = nn.simulated_quantize(meta[relay.Constant][189] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(1000), float32] */;
  %867 = add(%865, %866) /* ty=Tensor[(32, 1000), float32] */;
  nn.simulated_quantize(%867, 1f /* ty=float32 */, 1f /* ty=float32 */, nanf /* ty=float32 */, nanf /* ty=float32 */, in_dtype="float32", out_dtype="float32", axis=None) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:root:quantize graph
DEBUG:root:v0.0.4
fn (%data: Tensor[(32, 3, 299, 299), float32]) -> Tensor[(32, 1000), float32] {
  %0 = qnn.quantize(%data, 0.0205693f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 3, 299, 299), int8] */;
  %1 = qnn.quantize(meta[relay.Constant][0] /* ty=Tensor[(32, 3, 3, 3), float32] */ /* ty=Tensor[(32, 3, 3, 3), float32] */, 0.0184393f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 3, 3, 3), int8] */;
  %2 = nn.conv2d(%0, %1, strides=[2, 2], padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 32, 149, 149), int32] */;
  %3 = qnn.requantize(%2, 0.000379284f /* ty=float32 */, 0 /* ty=int32 */, 5.61859e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 32, 149, 149), int32] */;
  %4 = qnn.quantize(meta[relay.Constant][1] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */, 5.61859e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1, 1), int32] */;
  %5 = add(%3, %4) /* ty=Tensor[(32, 32, 149, 149), int32] */;
  %6 = qnn.requantize(%5, 5.61859e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.61859e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 32, 149, 149), int32] */;
  %7 = nn.relu(%6) /* ty=Tensor[(32, 32, 149, 149), int32] */;
  %8 = qnn.requantize(%7, 5.61859e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.101719f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 32, 149, 149), int8] */;
  %9 = qnn.quantize(meta[relay.Constant][2] /* ty=Tensor[(32, 32, 3, 3), float32] */ /* ty=Tensor[(32, 32, 3, 3), float32] */, 0.015331f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 32, 3, 3), int8] */;
  %10 = nn.conv2d(%8, %9, padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 32, 147, 147), int32] */;
  %11 = qnn.requantize(%10, 0.00155945f /* ty=float32 */, 0 /* ty=int32 */, 7.71852e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 32, 147, 147), int32] */;
  %12 = qnn.quantize(meta[relay.Constant][3] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */, 7.71852e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1, 1), int32] */;
  %13 = add(%11, %12) /* ty=Tensor[(32, 32, 147, 147), int32] */;
  %14 = qnn.requantize(%13, 7.71852e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.71852e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 32, 147, 147), int32] */;
  %15 = nn.relu(%14) /* ty=Tensor[(32, 32, 147, 147), int32] */;
  %16 = qnn.requantize(%15, 7.71852e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.126736f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 32, 147, 147), int8] */;
  %17 = qnn.quantize(meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */ /* ty=Tensor[(64, 32, 3, 3), float32] */, 0.00999235f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 32, 3, 3), int8] */;
  %18 = nn.conv2d(%16, %17, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 64, 147, 147), int32] */;
  %19 = qnn.requantize(%18, 0.00126639f /* ty=float32 */, 0 /* ty=int32 */, 8.47564e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 147, 147), int32] */;
  %20 = qnn.quantize(meta[relay.Constant][5] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 8.47564e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %21 = add(%19, %20) /* ty=Tensor[(32, 64, 147, 147), int32] */;
  %22 = qnn.requantize(%21, 8.47564e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.47564e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 147, 147), int32] */;
  %23 = nn.relu(%22) /* ty=Tensor[(32, 64, 147, 147), int32] */;
  %24 = qnn.requantize(%23, 8.47564e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.4983e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 147, 147), int32] */;
  %25 = nn.max_pool2d(%24, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 64, 73, 73), int32] */;
  %26 = qnn.requantize(%25, 6.4983e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.109023f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 73, 73), int8] */;
  %27 = qnn.quantize(meta[relay.Constant][6] /* ty=Tensor[(80, 64, 1, 1), float32] */ /* ty=Tensor[(80, 64, 1, 1), float32] */, 0.0102552f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(80, 64, 1, 1), int8] */;
  %28 = nn.conv2d(%26, %27, padding=[0, 0, 0, 0], channels=80, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 80, 73, 73), int32] */;
  %29 = qnn.requantize(%28, 0.00111806f /* ty=float32 */, 0 /* ty=int32 */, 7.58872e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 80, 73, 73), int32] */;
  %30 = qnn.quantize(meta[relay.Constant][7] /* ty=Tensor[(80, 1, 1), float32] */ /* ty=Tensor[(80, 1, 1), float32] */, 7.58872e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(80, 1, 1), int32] */;
  %31 = add(%29, %30) /* ty=Tensor[(32, 80, 73, 73), int32] */;
  %32 = qnn.requantize(%31, 7.58872e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.58872e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 80, 73, 73), int32] */;
  %33 = nn.relu(%32) /* ty=Tensor[(32, 80, 73, 73), int32] */;
  %34 = qnn.requantize(%33, 7.58872e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.117261f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 80, 73, 73), int8] */;
  %35 = qnn.quantize(meta[relay.Constant][8] /* ty=Tensor[(192, 80, 3, 3), float32] */ /* ty=Tensor[(192, 80, 3, 3), float32] */, 0.00574007f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 80, 3, 3), int8] */;
  %36 = nn.conv2d(%34, %35, padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 192, 71, 71), int32] */;
  %37 = qnn.requantize(%36, 0.000673084f /* ty=float32 */, 0 /* ty=int32 */, 5.89513e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 71, 71), int32] */;
  %38 = qnn.quantize(meta[relay.Constant][9] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 5.89513e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %39 = add(%37, %38) /* ty=Tensor[(32, 192, 71, 71), int32] */;
  %40 = qnn.requantize(%39, 5.89513e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.89513e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 71, 71), int32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(32, 192, 71, 71), int32] */;
  %42 = qnn.requantize(%41, 5.89513e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.84559e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 71, 71), int32] */;
  %43 = nn.max_pool2d(%42, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 192, 35, 35), int32] */;
  %44 = qnn.requantize(%43, 5.84559e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0980727f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 35, 35), int8] */;
  %45 = qnn.quantize(meta[relay.Constant][10] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, 0.00433901f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 192, 1, 1), int8] */;
  %46 = nn.conv2d(%44, %45, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %47 = qnn.requantize(%46, 0.000425539f /* ty=float32 */, 0 /* ty=int32 */, 4.94103e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %48 = qnn.quantize(meta[relay.Constant][11] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 4.94103e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %49 = add(%47, %48) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %50 = qnn.requantize(%49, 4.94103e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.94103e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %51 = nn.relu(%50) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %52 = qnn.requantize(%51, 4.94103e-09f /* ty=float32 */, 0 /* ty=int32 */, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %53 = qnn.requantize(%43, 5.84559e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0980727f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 35, 35), int8] */;
  %54 = qnn.quantize(meta[relay.Constant][12] /* ty=Tensor[(48, 192, 1, 1), float32] */ /* ty=Tensor[(48, 192, 1, 1), float32] */, 0.00401616f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(48, 192, 1, 1), int8] */;
  %55 = nn.conv2d(%53, %54, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %56 = qnn.requantize(%55, 0.000393875f /* ty=float32 */, 0 /* ty=int32 */, 5.00831e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %57 = qnn.quantize(meta[relay.Constant][13] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */, 5.00831e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(48, 1, 1), int32] */;
  %58 = add(%56, %57) /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %59 = qnn.requantize(%58, 5.00831e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.00831e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %60 = nn.relu(%59) /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %61 = qnn.requantize(%60, 5.00831e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0731393f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 48, 35, 35), int8] */;
  %62 = qnn.quantize(meta[relay.Constant][14] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, 0.00522591f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 48, 5, 5), int8] */;
  %63 = nn.conv2d(%61, %62, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %64 = qnn.requantize(%63, 0.000382219f /* ty=float32 */, 0 /* ty=int32 */, 4.36374e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %65 = qnn.quantize(meta[relay.Constant][15] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 4.36374e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %66 = add(%64, %65) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %67 = qnn.requantize(%66, 4.36374e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.36374e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %68 = nn.relu(%67) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %69 = qnn.requantize(%68, 4.36374e-09f /* ty=float32 */, 0 /* ty=int32 */, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %70 = qnn.requantize(%43, 5.84559e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0980727f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 35, 35), int8] */;
  %71 = qnn.quantize(meta[relay.Constant][16] /* ty=Tensor[(64, 192, 1, 1), float32] */ /* ty=Tensor[(64, 192, 1, 1), float32] */, 0.00706938f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 192, 1, 1), int8] */;
  %72 = nn.conv2d(%70, %71, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %73 = qnn.requantize(%72, 0.000693313f /* ty=float32 */, 0 /* ty=int32 */, 6.26184e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %74 = qnn.quantize(meta[relay.Constant][17] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 6.26184e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %75 = add(%73, %74) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %76 = qnn.requantize(%75, 6.26184e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.26184e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %77 = nn.relu(%76) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %78 = qnn.requantize(%77, 6.26184e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0858584f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 35, 35), int8] */;
  %79 = qnn.quantize(meta[relay.Constant][18] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, 0.00603217f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(96, 64, 3, 3), int8] */;
  %80 = nn.conv2d(%78, %79, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %81 = qnn.requantize(%80, 0.000517912f /* ty=float32 */, 0 /* ty=int32 */, 5.63654e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %82 = qnn.quantize(meta[relay.Constant][19] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 5.63654e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(96, 1, 1), int32] */;
  %83 = add(%81, %82) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %84 = qnn.requantize(%83, 5.63654e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.63654e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %85 = nn.relu(%84) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %86 = qnn.requantize(%85, 5.63654e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.102477f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 96, 35, 35), int8] */;
  %87 = qnn.quantize(meta[relay.Constant][20] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, 0.00584231f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(96, 96, 3, 3), int8] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %89 = qnn.requantize(%88, 0.000598702f /* ty=float32 */, 0 /* ty=int32 */, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %90 = qnn.quantize(meta[relay.Constant][21] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(96, 1, 1), int32] */;
  %91 = add(%89, %90) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %92 = qnn.requantize(%91, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %93 = nn.relu(%92) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %94 = qnn.requantize(%93, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %95 = qnn.dequantize(%43, 5.84559e-09f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %96 = nn.avg_pool2d(%95, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 192, 35, 35), float32] */;
  %97 = qnn.quantize(%96, 0.0618268f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 35, 35), int8] */;
  %98 = qnn.quantize(meta[relay.Constant][22] /* ty=Tensor[(32, 192, 1, 1), float32] */ /* ty=Tensor[(32, 192, 1, 1), float32] */, 0.011058f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 1, 1), int8] */;
  %99 = nn.conv2d(%97, %98, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 32, 35, 35), int32] */;
  %100 = qnn.requantize(%99, 0.000683679f /* ty=float32 */, 0 /* ty=int32 */, 5.49927e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 32, 35, 35), int32] */;
  %101 = qnn.quantize(meta[relay.Constant][23] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */, 5.49927e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 1, 1), int32] */;
  %102 = add(%100, %101) /* ty=Tensor[(32, 32, 35, 35), int32] */;
  %103 = qnn.requantize(%102, 5.49927e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.49927e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 32, 35, 35), int32] */;
  %104 = nn.relu(%103) /* ty=Tensor[(32, 32, 35, 35), int32] */;
  %105 = qnn.requantize(%104, 5.49927e-09f /* ty=float32 */, 0 /* ty=int32 */, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 32, 35, 35), int32] */;
  %106 = (%52, %69, %94, %105);
  %107 = concatenate(%106, axis=1) /* ty=Tensor[(32, 256, 35, 35), int32] */;
  %108 = qnn.requantize(%107, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.144319f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 35, 35), int8] */;
  %109 = qnn.quantize(meta[relay.Constant][24] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 0.00584852f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 256, 1, 1), int8] */;
  %110 = nn.conv2d(%108, %109, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %111 = qnn.requantize(%110, 0.000844052f /* ty=float32 */, 0 /* ty=int32 */, 6.7476e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %112 = qnn.quantize(meta[relay.Constant][25] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 6.7476e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %113 = add(%111, %112) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %114 = qnn.requantize(%113, 6.7476e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.7476e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %115 = nn.relu(%114) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %116 = qnn.requantize(%115, 6.7476e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %117 = qnn.requantize(%107, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.144319f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 35, 35), int8] */;
  %118 = qnn.quantize(meta[relay.Constant][26] /* ty=Tensor[(48, 256, 1, 1), float32] */ /* ty=Tensor[(48, 256, 1, 1), float32] */, 0.00465262f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(48, 256, 1, 1), int8] */;
  %119 = nn.conv2d(%117, %118, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %120 = qnn.requantize(%119, 0.000671461f /* ty=float32 */, 0 /* ty=int32 */, 6.83224e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %121 = qnn.quantize(meta[relay.Constant][27] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */, 6.83224e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(48, 1, 1), int32] */;
  %122 = add(%120, %121) /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %123 = qnn.requantize(%122, 6.83224e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.83224e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %124 = nn.relu(%123) /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %125 = qnn.requantize(%124, 6.83224e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.104371f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 48, 35, 35), int8] */;
  %126 = qnn.quantize(meta[relay.Constant][28] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, 0.00393846f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 48, 5, 5), int8] */;
  %127 = nn.conv2d(%125, %126, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %128 = qnn.requantize(%127, 0.000411061f /* ty=float32 */, 0 /* ty=int32 */, 5.49525e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %129 = qnn.quantize(meta[relay.Constant][29] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 5.49525e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %130 = add(%128, %129) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %131 = qnn.requantize(%130, 5.49525e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.49525e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %132 = nn.relu(%131) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %133 = qnn.requantize(%132, 5.49525e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %134 = qnn.requantize(%107, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.144319f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 35, 35), int8] */;
  %135 = qnn.quantize(meta[relay.Constant][30] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 0.00536366f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 256, 1, 1), int8] */;
  %136 = nn.conv2d(%134, %135, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %137 = qnn.requantize(%136, 0.000774077f /* ty=float32 */, 0 /* ty=int32 */, 5.6246e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %138 = qnn.quantize(meta[relay.Constant][31] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 5.6246e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %139 = add(%137, %138) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %140 = qnn.requantize(%139, 5.6246e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.6246e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %141 = nn.relu(%140) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %142 = qnn.requantize(%141, 5.6246e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0654255f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 35, 35), int8] */;
  %143 = qnn.quantize(meta[relay.Constant][32] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, 0.00558279f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(96, 64, 3, 3), int8] */;
  %144 = nn.conv2d(%142, %143, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %145 = qnn.requantize(%144, 0.000365257f /* ty=float32 */, 0 /* ty=int32 */, 4.72001e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %146 = qnn.quantize(meta[relay.Constant][33] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 4.72001e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(96, 1, 1), int32] */;
  %147 = add(%145, %146) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %148 = qnn.requantize(%147, 4.72001e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.72001e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %149 = nn.relu(%148) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %150 = qnn.requantize(%149, 4.72001e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0737892f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 96, 35, 35), int8] */;
  %151 = qnn.quantize(meta[relay.Constant][34] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, 0.00928544f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(96, 96, 3, 3), int8] */;
  %152 = nn.conv2d(%150, %151, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %153 = qnn.requantize(%152, 0.000685166f /* ty=float32 */, 0 /* ty=int32 */, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %154 = qnn.quantize(meta[relay.Constant][35] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(96, 1, 1), int32] */;
  %155 = add(%153, %154) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %156 = qnn.requantize(%155, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %157 = nn.relu(%156) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %158 = qnn.requantize(%157, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %159 = qnn.dequantize(%107, 9.00128e-09f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %160 = nn.avg_pool2d(%159, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 256, 35, 35), float32] */;
  %161 = qnn.quantize(%160, 0.0866785f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 256, 35, 35), int8] */;
  %162 = qnn.quantize(meta[relay.Constant][36] /* ty=Tensor[(64, 256, 1, 1), float32] */ /* ty=Tensor[(64, 256, 1, 1), float32] */, 0.00933783f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 256, 1, 1), int8] */;
  %163 = nn.conv2d(%161, %162, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %164 = qnn.requantize(%163, 0.00080939f /* ty=float32 */, 0 /* ty=int32 */, 7.18542e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %165 = qnn.quantize(meta[relay.Constant][37] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 7.18542e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %166 = add(%164, %165) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %167 = qnn.requantize(%166, 7.18542e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.18542e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %168 = nn.relu(%167) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %169 = qnn.requantize(%168, 7.18542e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %170 = (%116, %133, %158, %169);
  %171 = concatenate(%170, axis=1) /* ty=Tensor[(32, 288, 35, 35), int32] */;
  %172 = qnn.requantize(%171, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.13219f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 288, 35, 35), int8] */;
  %173 = qnn.quantize(meta[relay.Constant][38] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, 0.00838885f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 288, 1, 1), int8] */;
  %174 = nn.conv2d(%172, %173, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %175 = qnn.requantize(%174, 0.00110892f /* ty=float32 */, 0 /* ty=int32 */, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %176 = qnn.quantize(meta[relay.Constant][39] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %177 = add(%175, %176) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %178 = qnn.requantize(%177, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %179 = nn.relu(%178) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %180 = qnn.requantize(%179, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %181 = qnn.requantize(%171, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.13219f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 288, 35, 35), int8] */;
  %182 = qnn.quantize(meta[relay.Constant][40] /* ty=Tensor[(48, 288, 1, 1), float32] */ /* ty=Tensor[(48, 288, 1, 1), float32] */, 0.00501439f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(48, 288, 1, 1), int8] */;
  %183 = nn.conv2d(%181, %182, padding=[0, 0, 0, 0], channels=48, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %184 = qnn.requantize(%183, 0.000662853f /* ty=float32 */, 0 /* ty=int32 */, 5.41504e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %185 = qnn.quantize(meta[relay.Constant][41] /* ty=Tensor[(48, 1, 1), float32] */ /* ty=Tensor[(48, 1, 1), float32] */, 5.41504e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(48, 1, 1), int32] */;
  %186 = add(%184, %185) /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %187 = qnn.requantize(%186, 5.41504e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.41504e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %188 = nn.relu(%187) /* ty=Tensor[(32, 48, 35, 35), int32] */;
  %189 = qnn.requantize(%188, 5.41504e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0840957f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 48, 35, 35), int8] */;
  %190 = qnn.quantize(meta[relay.Constant][42] /* ty=Tensor[(64, 48, 5, 5), float32] */ /* ty=Tensor[(64, 48, 5, 5), float32] */, 0.00286836f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 48, 5, 5), int8] */;
  %191 = nn.conv2d(%189, %190, padding=[2, 2, 2, 2], channels=64, kernel_size=[5, 5], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %192 = qnn.requantize(%191, 0.000241217f /* ty=float32 */, 0 /* ty=int32 */, 4.46515e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %193 = qnn.quantize(meta[relay.Constant][43] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 4.46515e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %194 = add(%192, %193) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %195 = qnn.requantize(%194, 4.46515e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.46515e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %196 = nn.relu(%195) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %197 = qnn.requantize(%196, 4.46515e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %198 = qnn.requantize(%171, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.13219f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 288, 35, 35), int8] */;
  %199 = qnn.quantize(meta[relay.Constant][44] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, 0.00695815f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 288, 1, 1), int8] */;
  %200 = nn.conv2d(%198, %199, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %201 = qnn.requantize(%200, 0.0009198f /* ty=float32 */, 0 /* ty=int32 */, 6.54673e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %202 = qnn.quantize(meta[relay.Constant][45] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 6.54673e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %203 = add(%201, %202) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %204 = qnn.requantize(%203, 6.54673e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.54673e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %206 = qnn.requantize(%205, 6.54673e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.100167f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 35, 35), int8] */;
  %207 = qnn.quantize(meta[relay.Constant][46] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, 0.00469817f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(96, 64, 3, 3), int8] */;
  %208 = nn.conv2d(%206, %207, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %209 = qnn.requantize(%208, 0.0004706f /* ty=float32 */, 0 /* ty=int32 */, 5.32158e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %210 = qnn.quantize(meta[relay.Constant][47] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 5.32158e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(96, 1, 1), int32] */;
  %211 = add(%209, %210) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %212 = qnn.requantize(%211, 5.32158e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.32158e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %213 = nn.relu(%212) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %214 = qnn.requantize(%213, 5.32158e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0884136f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 96, 35, 35), int8] */;
  %215 = qnn.quantize(meta[relay.Constant][48] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, 0.00468934f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(96, 96, 3, 3), int8] */;
  %216 = nn.conv2d(%214, %215, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %217 = qnn.requantize(%216, 0.000414602f /* ty=float32 */, 0 /* ty=int32 */, 4.12971e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %218 = qnn.quantize(meta[relay.Constant][49] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 4.12971e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(96, 1, 1), int32] */;
  %219 = add(%217, %218) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %220 = qnn.requantize(%219, 4.12971e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.12971e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %221 = nn.relu(%220) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %222 = qnn.requantize(%221, 4.12971e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %223 = qnn.dequantize(%171, 8.0604e-09f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %224 = nn.avg_pool2d(%223, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 288, 35, 35), float32] */;
  %225 = qnn.quantize(%224, 0.0766314f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 288, 35, 35), int8] */;
  %226 = qnn.quantize(meta[relay.Constant][50] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, 0.0113221f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 288, 1, 1), int8] */;
  %227 = nn.conv2d(%225, %226, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %228 = qnn.requantize(%227, 0.000867631f /* ty=float32 */, 0 /* ty=int32 */, 5.5098e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %229 = qnn.quantize(meta[relay.Constant][51] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 5.5098e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %230 = add(%228, %229) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %231 = qnn.requantize(%230, 5.5098e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.5098e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %232 = nn.relu(%231) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %233 = qnn.requantize(%232, 5.5098e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %234 = (%180, %197, %222, %233);
  %235 = concatenate(%234, axis=1) /* ty=Tensor[(32, 288, 35, 35), int32] */;
  %236 = qnn.requantize(%235, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0827759f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 288, 35, 35), int8] */;
  %237 = qnn.quantize(meta[relay.Constant][52] /* ty=Tensor[(384, 288, 3, 3), float32] */ /* ty=Tensor[(384, 288, 3, 3), float32] */, 0.00398437f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 288, 3, 3), int8] */;
  %238 = nn.conv2d(%236, %237, strides=[2, 2], padding=[0, 0, 0, 0], channels=384, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 384, 17, 17), int32] */;
  %239 = qnn.requantize(%238, 0.00032981f /* ty=float32 */, 0 /* ty=int32 */, 4.90954e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 17, 17), int32] */;
  %240 = qnn.quantize(meta[relay.Constant][53] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 4.90954e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %241 = add(%239, %240) /* ty=Tensor[(32, 384, 17, 17), int32] */;
  %242 = qnn.requantize(%241, 4.90954e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.90954e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 17, 17), int32] */;
  %243 = nn.relu(%242) /* ty=Tensor[(32, 384, 17, 17), int32] */;
  %244 = qnn.requantize(%243, 4.90954e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 17, 17), int32] */;
  %245 = qnn.requantize(%235, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0827759f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 288, 35, 35), int8] */;
  %246 = qnn.quantize(meta[relay.Constant][54] /* ty=Tensor[(64, 288, 1, 1), float32] */ /* ty=Tensor[(64, 288, 1, 1), float32] */, 0.00519745f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(64, 288, 1, 1), int8] */;
  %247 = nn.conv2d(%245, %246, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %248 = qnn.requantize(%247, 0.000430224f /* ty=float32 */, 0 /* ty=int32 */, 5.60844e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %249 = qnn.quantize(meta[relay.Constant][55] /* ty=Tensor[(64, 1, 1), float32] */ /* ty=Tensor[(64, 1, 1), float32] */, 5.60844e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(64, 1, 1), int32] */;
  %250 = add(%248, %249) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %251 = qnn.requantize(%250, 5.60844e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.60844e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %252 = nn.relu(%251) /* ty=Tensor[(32, 64, 35, 35), int32] */;
  %253 = qnn.requantize(%252, 5.60844e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0798705f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 64, 35, 35), int8] */;
  %254 = qnn.quantize(meta[relay.Constant][56] /* ty=Tensor[(96, 64, 3, 3), float32] */ /* ty=Tensor[(96, 64, 3, 3), float32] */, 0.00464639f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(96, 64, 3, 3), int8] */;
  %255 = nn.conv2d(%253, %254, padding=[1, 1, 1, 1], channels=96, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %256 = qnn.requantize(%255, 0.00037111f /* ty=float32 */, 0 /* ty=int32 */, 4.99602e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %257 = qnn.quantize(meta[relay.Constant][57] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 4.99602e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(96, 1, 1), int32] */;
  %258 = add(%256, %257) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %259 = qnn.requantize(%258, 4.99602e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.99602e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %260 = nn.relu(%259) /* ty=Tensor[(32, 96, 35, 35), int32] */;
  %261 = qnn.requantize(%260, 4.99602e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0783957f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 96, 35, 35), int8] */;
  %262 = qnn.quantize(meta[relay.Constant][58] /* ty=Tensor[(96, 96, 3, 3), float32] */ /* ty=Tensor[(96, 96, 3, 3), float32] */, 0.00440968f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(96, 96, 3, 3), int8] */;
  %263 = nn.conv2d(%261, %262, strides=[2, 2], padding=[0, 0, 0, 0], channels=96, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 96, 17, 17), int32] */;
  %264 = qnn.requantize(%263, 0.0003457f /* ty=float32 */, 0 /* ty=int32 */, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 17, 17), int32] */;
  %265 = qnn.quantize(meta[relay.Constant][59] /* ty=Tensor[(96, 1, 1), float32] */ /* ty=Tensor[(96, 1, 1), float32] */, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(96, 1, 1), int32] */;
  %266 = add(%264, %265) /* ty=Tensor[(32, 96, 17, 17), int32] */;
  %267 = qnn.requantize(%266, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 17, 17), int32] */;
  %268 = nn.relu(%267) /* ty=Tensor[(32, 96, 17, 17), int32] */;
  %269 = qnn.requantize(%268, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 96, 17, 17), int32] */;
  %270 = qnn.requantize(%235, 7.61068e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.93383e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 288, 35, 35), int32] */;
  %271 = nn.max_pool2d(%270, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 288, 17, 17), int32] */;
  %272 = qnn.requantize(%271, 4.93383e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 288, 17, 17), int32] */;
  %273 = (%244, %269, %272);
  %274 = concatenate(%273, axis=1) /* ty=Tensor[(32, 768, 17, 17), int32] */;
  %275 = qnn.requantize(%274, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0958396f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %276 = qnn.quantize(meta[relay.Constant][60] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00537618f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %277 = nn.conv2d(%275, %276, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %278 = qnn.requantize(%277, 0.000515251f /* ty=float32 */, 0 /* ty=int32 */, 4.88805e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %279 = qnn.quantize(meta[relay.Constant][61] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 4.88805e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %280 = add(%278, %279) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %281 = qnn.requantize(%280, 4.88805e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.88805e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %282 = nn.relu(%281) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %283 = qnn.requantize(%282, 4.88805e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %284 = qnn.requantize(%274, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0958396f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %285 = qnn.quantize(meta[relay.Constant][62] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, 0.00610288f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 768, 1, 1), int8] */;
  %286 = nn.conv2d(%284, %285, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %287 = qnn.requantize(%286, 0.000584898f /* ty=float32 */, 0 /* ty=int32 */, 5.25398e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %288 = qnn.quantize(meta[relay.Constant][63] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 5.25398e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %289 = add(%287, %288) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %290 = qnn.requantize(%289, 5.25398e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.25398e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %291 = nn.relu(%290) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %292 = qnn.requantize(%291, 5.25398e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0796376f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 17, 17), int8] */;
  %293 = qnn.quantize(meta[relay.Constant][64] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, 0.00749939f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 1, 7), int8] */;
  %294 = nn.conv2d(%292, %293, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %295 = qnn.requantize(%294, 0.000597234f /* ty=float32 */, 0 /* ty=int32 */, 5.06826e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %296 = qnn.quantize(meta[relay.Constant][65] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 5.06826e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %297 = add(%295, %296) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %298 = qnn.requantize(%297, 5.06826e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.06826e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %299 = nn.relu(%298) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %300 = qnn.requantize(%299, 5.06826e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0838972f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 17, 17), int8] */;
  %301 = qnn.quantize(meta[relay.Constant][66] /* ty=Tensor[(192, 128, 7, 1), float32] */ /* ty=Tensor[(192, 128, 7, 1), float32] */, 0.00617447f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 128, 7, 1), int8] */;
  %302 = nn.conv2d(%300, %301, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %303 = qnn.requantize(%302, 0.000518021f /* ty=float32 */, 0 /* ty=int32 */, 4.83569e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %304 = qnn.quantize(meta[relay.Constant][67] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 4.83569e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %305 = add(%303, %304) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %306 = qnn.requantize(%305, 4.83569e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.83569e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %307 = nn.relu(%306) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %308 = qnn.requantize(%307, 4.83569e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %309 = qnn.requantize(%274, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0958396f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %310 = qnn.quantize(meta[relay.Constant][68] /* ty=Tensor[(128, 768, 1, 1), float32] */ /* ty=Tensor[(128, 768, 1, 1), float32] */, 0.00469163f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 768, 1, 1), int8] */;
  %311 = nn.conv2d(%309, %310, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %312 = qnn.requantize(%311, 0.000449644f /* ty=float32 */, 0 /* ty=int32 */, 4.69498e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %313 = qnn.quantize(meta[relay.Constant][69] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 4.69498e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %314 = add(%312, %313) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %315 = qnn.requantize(%314, 4.69498e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.69498e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %316 = nn.relu(%315) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %317 = qnn.requantize(%316, 4.69498e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0646086f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 17, 17), int8] */;
  %318 = qnn.quantize(meta[relay.Constant][70] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, 0.00493314f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 7, 1), int8] */;
  %319 = nn.conv2d(%317, %318, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %320 = qnn.requantize(%319, 0.000318723f /* ty=float32 */, 0 /* ty=int32 */, 3.92667e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %321 = qnn.quantize(meta[relay.Constant][71] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 3.92667e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %322 = add(%320, %321) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %323 = qnn.requantize(%322, 3.92667e-09f /* ty=float32 */, 0 /* ty=int32 */, 3.92667e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %324 = nn.relu(%323) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %325 = qnn.requantize(%324, 3.92667e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0552085f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 17, 17), int8] */;
  %326 = qnn.quantize(meta[relay.Constant][72] /* ty=Tensor[(128, 128, 1, 7), float32] */ /* ty=Tensor[(128, 128, 1, 7), float32] */, 0.00597663f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 1, 7), int8] */;
  %327 = nn.conv2d(%325, %326, padding=[0, 3, 0, 3], channels=128, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %328 = qnn.requantize(%327, 0.000329961f /* ty=float32 */, 0 /* ty=int32 */, 4.00599e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %329 = qnn.quantize(meta[relay.Constant][73] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 4.00599e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %330 = add(%328, %329) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %331 = qnn.requantize(%330, 4.00599e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.00599e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %332 = nn.relu(%331) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %333 = qnn.requantize(%332, 4.00599e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0631221f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 17, 17), int8] */;
  %334 = qnn.quantize(meta[relay.Constant][74] /* ty=Tensor[(128, 128, 7, 1), float32] */ /* ty=Tensor[(128, 128, 7, 1), float32] */, 0.00586756f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(128, 128, 7, 1), int8] */;
  %335 = nn.conv2d(%333, %334, padding=[3, 0, 3, 0], channels=128, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %336 = qnn.requantize(%335, 0.000370372f /* ty=float32 */, 0 /* ty=int32 */, 5.84374e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %337 = qnn.quantize(meta[relay.Constant][75] /* ty=Tensor[(128, 1, 1), float32] */ /* ty=Tensor[(128, 1, 1), float32] */, 5.84374e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(128, 1, 1), int32] */;
  %338 = add(%336, %337) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %339 = qnn.requantize(%338, 5.84374e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.84374e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %340 = nn.relu(%339) /* ty=Tensor[(32, 128, 17, 17), int32] */;
  %341 = qnn.requantize(%340, 5.84374e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0963442f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 128, 17, 17), int8] */;
  %342 = qnn.quantize(meta[relay.Constant][76] /* ty=Tensor[(192, 128, 1, 7), float32] */ /* ty=Tensor[(192, 128, 1, 7), float32] */, 0.00404097f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 128, 1, 7), int8] */;
  %343 = nn.conv2d(%341, %342, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %344 = qnn.requantize(%343, 0.000389324f /* ty=float32 */, 0 /* ty=int32 */, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %345 = qnn.quantize(meta[relay.Constant][77] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %346 = add(%344, %345) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %347 = qnn.requantize(%346, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %348 = nn.relu(%347) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %349 = qnn.requantize(%348, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %350 = qnn.dequantize(%274, 5.87421e-09f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %351 = nn.avg_pool2d(%350, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %352 = qnn.quantize(%351, 0.057475f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %353 = qnn.quantize(meta[relay.Constant][78] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.01131f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %354 = nn.conv2d(%352, %353, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %355 = qnn.requantize(%354, 0.000650043f /* ty=float32 */, 0 /* ty=int32 */, 4.57761e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %356 = qnn.quantize(meta[relay.Constant][79] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 4.57761e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %357 = add(%355, %356) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %358 = qnn.requantize(%357, 4.57761e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.57761e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %359 = nn.relu(%358) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %360 = qnn.requantize(%359, 4.57761e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %361 = (%283, %308, %349, %360);
  %362 = concatenate(%361, axis=1) /* ty=Tensor[(32, 768, 17, 17), int32] */;
  %363 = qnn.requantize(%362, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.149103f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %364 = qnn.quantize(meta[relay.Constant][80] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00530625f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %365 = nn.conv2d(%363, %364, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %366 = qnn.requantize(%365, 0.00079118f /* ty=float32 */, 0 /* ty=int32 */, 5.71017e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %367 = qnn.quantize(meta[relay.Constant][81] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 5.71017e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %368 = add(%366, %367) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %369 = qnn.requantize(%368, 5.71017e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.71017e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %370 = nn.relu(%369) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %371 = qnn.requantize(%370, 5.71017e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %372 = qnn.requantize(%362, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.149103f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %373 = qnn.quantize(meta[relay.Constant][82] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, 0.00749864f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 768, 1, 1), int8] */;
  %374 = nn.conv2d(%372, %373, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %375 = qnn.requantize(%374, 0.00111807f /* ty=float32 */, 0 /* ty=int32 */, 6.19932e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %376 = qnn.quantize(meta[relay.Constant][83] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 6.19932e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %377 = add(%375, %376) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %378 = qnn.requantize(%377, 6.19932e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.19932e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %379 = nn.relu(%378) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %380 = qnn.requantize(%379, 6.19932e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.103404f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %381 = qnn.quantize(meta[relay.Constant][84] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, 0.00484023f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 160, 1, 7), int8] */;
  %382 = nn.conv2d(%380, %381, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %383 = qnn.requantize(%382, 0.000500498f /* ty=float32 */, 0 /* ty=int32 */, 4.41672e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %384 = qnn.quantize(meta[relay.Constant][85] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 4.41672e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %385 = add(%383, %384) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %386 = qnn.requantize(%385, 4.41672e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.41672e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %387 = nn.relu(%386) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %388 = qnn.requantize(%387, 4.41672e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0772715f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %389 = qnn.quantize(meta[relay.Constant][86] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, 0.00553171f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 160, 7, 1), int8] */;
  %390 = nn.conv2d(%388, %389, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %391 = qnn.requantize(%390, 0.000427443f /* ty=float32 */, 0 /* ty=int32 */, 4.53216e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %392 = qnn.quantize(meta[relay.Constant][87] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 4.53216e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %393 = add(%391, %392) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %394 = qnn.requantize(%393, 4.53216e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.53216e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %395 = nn.relu(%394) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %396 = qnn.requantize(%395, 4.53216e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %397 = qnn.requantize(%362, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.149103f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %398 = qnn.quantize(meta[relay.Constant][88] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, 0.00691003f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 768, 1, 1), int8] */;
  %399 = nn.conv2d(%397, %398, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %400 = qnn.requantize(%399, 0.00103031f /* ty=float32 */, 0 /* ty=int32 */, 5.28767e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %401 = qnn.quantize(meta[relay.Constant][89] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 5.28767e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %402 = add(%400, %401) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %403 = qnn.requantize(%402, 5.28767e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.28767e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %404 = nn.relu(%403) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %405 = qnn.requantize(%404, 5.28767e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0807742f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %406 = qnn.quantize(meta[relay.Constant][90] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, 0.0040512f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 160, 7, 1), int8] */;
  %407 = nn.conv2d(%405, %406, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %408 = qnn.requantize(%407, 0.000327232f /* ty=float32 */, 0 /* ty=int32 */, 5.3636e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %409 = qnn.quantize(meta[relay.Constant][91] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 5.3636e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %410 = add(%408, %409) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %411 = qnn.requantize(%410, 5.3636e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.3636e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %412 = nn.relu(%411) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %413 = qnn.requantize(%412, 5.3636e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0894094f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %414 = qnn.quantize(meta[relay.Constant][92] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, 0.00555295f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 160, 1, 7), int8] */;
  %415 = nn.conv2d(%413, %414, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %416 = qnn.requantize(%415, 0.000496486f /* ty=float32 */, 0 /* ty=int32 */, 7.19698e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %417 = qnn.quantize(meta[relay.Constant][93] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 7.19698e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %418 = add(%416, %417) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %419 = qnn.requantize(%418, 7.19698e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.19698e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %420 = nn.relu(%419) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %421 = qnn.requantize(%420, 7.19698e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.111239f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %422 = qnn.quantize(meta[relay.Constant][94] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, 0.00499566f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 160, 7, 1), int8] */;
  %423 = nn.conv2d(%421, %422, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %424 = qnn.requantize(%423, 0.000555714f /* ty=float32 */, 0 /* ty=int32 */, 1.00332e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %425 = qnn.quantize(meta[relay.Constant][95] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 1.00332e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %426 = add(%424, %425) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %427 = qnn.requantize(%426, 1.00332e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.00332e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %428 = nn.relu(%427) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %429 = qnn.requantize(%428, 1.00332e-08f /* ty=float32 */, 0 /* ty=int32 */, 0.157102f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %430 = qnn.quantize(meta[relay.Constant][96] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, 0.00521494f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 160, 1, 7), int8] */;
  %431 = nn.conv2d(%429, %430, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %432 = qnn.requantize(%431, 0.000819276f /* ty=float32 */, 0 /* ty=int32 */, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %433 = qnn.quantize(meta[relay.Constant][97] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %434 = add(%432, %433) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %435 = qnn.requantize(%434, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %436 = nn.relu(%435) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %437 = qnn.requantize(%436, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %438 = qnn.dequantize(%362, 8.9195e-09f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %439 = nn.avg_pool2d(%438, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %440 = qnn.quantize(%439, 0.0451576f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %441 = qnn.quantize(meta[relay.Constant][98] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00751682f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %442 = nn.conv2d(%440, %441, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %443 = qnn.requantize(%442, 0.000339442f /* ty=float32 */, 0 /* ty=int32 */, 3.82568e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %444 = qnn.quantize(meta[relay.Constant][99] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 3.82568e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %445 = add(%443, %444) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %446 = qnn.requantize(%445, 3.82568e-09f /* ty=float32 */, 0 /* ty=int32 */, 3.82568e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %447 = nn.relu(%446) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %448 = qnn.requantize(%447, 3.82568e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %449 = (%371, %396, %437, %448);
  %450 = concatenate(%449, axis=1) /* ty=Tensor[(32, 768, 17, 17), int32] */;
  %451 = qnn.requantize(%450, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, 0.17452f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %452 = qnn.quantize(meta[relay.Constant][100] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00918641f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %453 = nn.conv2d(%451, %452, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %454 = qnn.requantize(%453, 0.00160321f /* ty=float32 */, 0 /* ty=int32 */, 6.29081e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %455 = qnn.quantize(meta[relay.Constant][101] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 6.29081e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %456 = add(%454, %455) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %457 = qnn.requantize(%456, 6.29081e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.29081e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %458 = nn.relu(%457) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %459 = qnn.requantize(%458, 6.29081e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.43986e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %460 = qnn.requantize(%450, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, 0.17452f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %461 = qnn.quantize(meta[relay.Constant][102] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, 0.00456683f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 768, 1, 1), int8] */;
  %462 = nn.conv2d(%460, %461, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %463 = qnn.requantize(%462, 0.000797002f /* ty=float32 */, 0 /* ty=int32 */, 6.89674e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %464 = qnn.quantize(meta[relay.Constant][103] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 6.89674e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %465 = add(%463, %464) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %466 = qnn.requantize(%465, 6.89674e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.89674e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %467 = nn.relu(%466) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %468 = qnn.requantize(%467, 6.89674e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.100028f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %469 = qnn.quantize(meta[relay.Constant][104] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, 0.00880511f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 160, 1, 7), int8] */;
  %470 = nn.conv2d(%468, %469, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %471 = qnn.requantize(%470, 0.000880759f /* ty=float32 */, 0 /* ty=int32 */, 7.31816e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %472 = qnn.quantize(meta[relay.Constant][105] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 7.31816e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %473 = add(%471, %472) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %474 = qnn.requantize(%473, 7.31816e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.31816e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %475 = nn.relu(%474) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %476 = qnn.requantize(%475, 7.31816e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.120732f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %477 = qnn.quantize(meta[relay.Constant][106] /* ty=Tensor[(192, 160, 7, 1), float32] */ /* ty=Tensor[(192, 160, 7, 1), float32] */, 0.00519522f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 160, 7, 1), int8] */;
  %478 = nn.conv2d(%476, %477, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %479 = qnn.requantize(%478, 0.000627232f /* ty=float32 */, 0 /* ty=int32 */, 6.16979e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %480 = qnn.quantize(meta[relay.Constant][107] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 6.16979e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %481 = add(%479, %480) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %482 = qnn.requantize(%481, 6.16979e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.16979e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %483 = nn.relu(%482) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %484 = qnn.requantize(%483, 6.16979e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.43986e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %485 = qnn.requantize(%450, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */, 0.17452f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %486 = qnn.quantize(meta[relay.Constant][108] /* ty=Tensor[(160, 768, 1, 1), float32] */ /* ty=Tensor[(160, 768, 1, 1), float32] */, 0.00663011f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 768, 1, 1), int8] */;
  %487 = nn.conv2d(%485, %486, padding=[0, 0, 0, 0], channels=160, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %488 = qnn.requantize(%487, 0.00115709f /* ty=float32 */, 0 /* ty=int32 */, 7.91671e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %489 = qnn.quantize(meta[relay.Constant][109] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 7.91671e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %490 = add(%488, %489) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %491 = qnn.requantize(%490, 7.91671e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.91671e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %492 = nn.relu(%491) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %493 = qnn.requantize(%492, 7.91671e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.12414f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %494 = qnn.quantize(meta[relay.Constant][110] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, 0.00737073f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 160, 7, 1), int8] */;
  %495 = nn.conv2d(%493, %494, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %496 = qnn.requantize(%495, 0.000915f /* ty=float32 */, 0 /* ty=int32 */, 7.64215e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %497 = qnn.quantize(meta[relay.Constant][111] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 7.64215e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %498 = add(%496, %497) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %499 = qnn.requantize(%498, 7.64215e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.64215e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %500 = nn.relu(%499) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %501 = qnn.requantize(%500, 7.64215e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.130681f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %502 = qnn.quantize(meta[relay.Constant][112] /* ty=Tensor[(160, 160, 1, 7), float32] */ /* ty=Tensor[(160, 160, 1, 7), float32] */, 0.00491007f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 160, 1, 7), int8] */;
  %503 = nn.conv2d(%501, %502, padding=[0, 3, 0, 3], channels=160, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %504 = qnn.requantize(%503, 0.00064165f /* ty=float32 */, 0 /* ty=int32 */, 5.3899e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %505 = qnn.quantize(meta[relay.Constant][113] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 5.3899e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %506 = add(%504, %505) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %507 = qnn.requantize(%506, 5.3899e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.3899e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %508 = nn.relu(%507) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %509 = qnn.requantize(%508, 5.3899e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0883848f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %510 = qnn.quantize(meta[relay.Constant][114] /* ty=Tensor[(160, 160, 7, 1), float32] */ /* ty=Tensor[(160, 160, 7, 1), float32] */, 0.00747773f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(160, 160, 7, 1), int8] */;
  %511 = nn.conv2d(%509, %510, padding=[3, 0, 3, 0], channels=160, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %512 = qnn.requantize(%511, 0.000660917f /* ty=float32 */, 0 /* ty=int32 */, 7.24282e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %513 = qnn.quantize(meta[relay.Constant][115] /* ty=Tensor[(160, 1, 1), float32] */ /* ty=Tensor[(160, 1, 1), float32] */, 7.24282e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(160, 1, 1), int32] */;
  %514 = add(%512, %513) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %515 = qnn.requantize(%514, 7.24282e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.24282e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %516 = nn.relu(%515) /* ty=Tensor[(32, 160, 17, 17), int32] */;
  %517 = qnn.requantize(%516, 7.24282e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0920394f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 160, 17, 17), int8] */;
  %518 = qnn.quantize(meta[relay.Constant][116] /* ty=Tensor[(192, 160, 1, 7), float32] */ /* ty=Tensor[(192, 160, 1, 7), float32] */, 0.00741879f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 160, 1, 7), int8] */;
  %519 = nn.conv2d(%517, %518, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %520 = qnn.requantize(%519, 0.000682821f /* ty=float32 */, 0 /* ty=int32 */, 5.33132e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %521 = qnn.quantize(meta[relay.Constant][117] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 5.33132e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %522 = add(%520, %521) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %523 = qnn.requantize(%522, 5.33132e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.33132e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %524 = nn.relu(%523) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %525 = qnn.requantize(%524, 5.33132e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.43986e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %526 = qnn.dequantize(%450, 1.05709e-08f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %527 = nn.avg_pool2d(%526, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %528 = qnn.quantize(%527, 0.0481846f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %529 = qnn.quantize(meta[relay.Constant][118] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.0139921f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %530 = nn.conv2d(%528, %529, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %531 = qnn.requantize(%530, 0.000674205f /* ty=float32 */, 0 /* ty=int32 */, 3.8614e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %532 = qnn.quantize(meta[relay.Constant][119] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 3.8614e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %533 = add(%531, %532) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %534 = qnn.requantize(%533, 3.8614e-09f /* ty=float32 */, 0 /* ty=int32 */, 3.8614e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %535 = nn.relu(%534) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %536 = qnn.requantize(%535, 3.8614e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.43986e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %537 = (%459, %484, %525, %536);
  %538 = concatenate(%537, axis=1) /* ty=Tensor[(32, 768, 17, 17), int32] */;
  %539 = qnn.requantize(%538, 6.43986e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.108856f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %540 = qnn.quantize(meta[relay.Constant][120] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00770918f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %541 = nn.conv2d(%539, %540, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %542 = qnn.requantize(%541, 0.000839188f /* ty=float32 */, 0 /* ty=int32 */, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %543 = qnn.quantize(meta[relay.Constant][121] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %544 = add(%542, %543) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %545 = qnn.requantize(%544, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %546 = nn.relu(%545) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %547 = qnn.requantize(%546, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %548 = qnn.requantize(%538, 6.43986e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.108856f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %549 = qnn.quantize(meta[relay.Constant][122] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00773977f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %550 = nn.conv2d(%548, %549, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %551 = qnn.requantize(%550, 0.000842519f /* ty=float32 */, 0 /* ty=int32 */, 5.59384e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %552 = qnn.quantize(meta[relay.Constant][123] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 5.59384e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %553 = add(%551, %552) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %554 = qnn.requantize(%553, 5.59384e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.59384e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %555 = nn.relu(%554) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %556 = qnn.requantize(%555, 5.59384e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0909263f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %557 = qnn.quantize(meta[relay.Constant][124] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, 0.00848777f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 192, 1, 7), int8] */;
  %558 = nn.conv2d(%556, %557, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %559 = qnn.requantize(%558, 0.000771762f /* ty=float32 */, 0 /* ty=int32 */, 6.45887e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %560 = qnn.quantize(meta[relay.Constant][125] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 6.45887e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %561 = add(%559, %560) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %562 = qnn.requantize(%561, 6.45887e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.45887e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %563 = nn.relu(%562) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %564 = qnn.requantize(%563, 6.45887e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.109682f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %565 = qnn.quantize(meta[relay.Constant][126] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, 0.0026531f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 192, 7, 1), int8] */;
  %566 = nn.conv2d(%564, %565, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %567 = qnn.requantize(%566, 0.000290997f /* ty=float32 */, 0 /* ty=int32 */, 4.6028e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %568 = qnn.quantize(meta[relay.Constant][127] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 4.6028e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %569 = add(%567, %568) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %570 = qnn.requantize(%569, 4.6028e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.6028e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %571 = nn.relu(%570) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %572 = qnn.requantize(%571, 4.6028e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %573 = qnn.requantize(%538, 6.43986e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.108856f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %574 = qnn.quantize(meta[relay.Constant][128] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00476074f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %575 = nn.conv2d(%573, %574, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %576 = qnn.requantize(%575, 0.000518234f /* ty=float32 */, 0 /* ty=int32 */, 6.26912e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %577 = qnn.quantize(meta[relay.Constant][129] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 6.26912e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %578 = add(%576, %577) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %579 = qnn.requantize(%578, 6.26912e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.26912e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %580 = nn.relu(%579) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %581 = qnn.requantize(%580, 6.26912e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0984178f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %582 = qnn.quantize(meta[relay.Constant][130] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, 0.00435483f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 192, 7, 1), int8] */;
  %583 = nn.conv2d(%581, %582, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %584 = qnn.requantize(%583, 0.000428593f /* ty=float32 */, 0 /* ty=int32 */, 5.721e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %585 = qnn.quantize(meta[relay.Constant][131] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 5.721e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %586 = add(%584, %585) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %587 = qnn.requantize(%586, 5.721e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.721e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %588 = nn.relu(%587) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %589 = qnn.requantize(%588, 5.721e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.095106f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %590 = qnn.quantize(meta[relay.Constant][132] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, 0.00510192f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 192, 1, 7), int8] */;
  %591 = nn.conv2d(%589, %590, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %592 = qnn.requantize(%591, 0.000485223f /* ty=float32 */, 0 /* ty=int32 */, 6.05562e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %593 = qnn.quantize(meta[relay.Constant][133] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 6.05562e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %594 = add(%592, %593) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %595 = qnn.requantize(%594, 6.05562e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.05562e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %596 = nn.relu(%595) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %597 = qnn.requantize(%596, 6.05562e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.101932f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %598 = qnn.quantize(meta[relay.Constant][134] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, 0.00319643f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 192, 7, 1), int8] */;
  %599 = nn.conv2d(%597, %598, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %600 = qnn.requantize(%599, 0.000325819f /* ty=float32 */, 0 /* ty=int32 */, 6.22569e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %601 = qnn.quantize(meta[relay.Constant][135] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 6.22569e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %602 = add(%600, %601) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %603 = qnn.requantize(%602, 6.22569e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.22569e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %604 = nn.relu(%603) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %605 = qnn.requantize(%604, 6.22569e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.108909f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %606 = qnn.quantize(meta[relay.Constant][136] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, 0.00268704f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 192, 1, 7), int8] */;
  %607 = nn.conv2d(%605, %606, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %608 = qnn.requantize(%607, 0.000292643f /* ty=float32 */, 0 /* ty=int32 */, 4.05702e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %609 = qnn.quantize(meta[relay.Constant][137] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 4.05702e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %610 = add(%608, %609) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %611 = qnn.requantize(%610, 4.05702e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.05702e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %612 = nn.relu(%611) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %613 = qnn.requantize(%612, 4.05702e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %614 = qnn.dequantize(%538, 6.43986e-09f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %615 = nn.avg_pool2d(%614, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 768, 17, 17), float32] */;
  %616 = qnn.quantize(%615, 0.0509987f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %617 = qnn.quantize(meta[relay.Constant][138] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00717943f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %618 = nn.conv2d(%616, %617, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %619 = qnn.requantize(%618, 0.000366142f /* ty=float32 */, 0 /* ty=int32 */, 3.37533e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %620 = qnn.quantize(meta[relay.Constant][139] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 3.37533e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %621 = add(%619, %620) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %622 = qnn.requantize(%621, 3.37533e-09f /* ty=float32 */, 0 /* ty=int32 */, 3.37533e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %623 = nn.relu(%622) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %624 = qnn.requantize(%623, 3.37533e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %625 = (%547, %572, %613, %624);
  %626 = concatenate(%625, axis=1) /* ty=Tensor[(32, 768, 17, 17), int32] */;
  %627 = qnn.requantize(%626, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0695813f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %628 = qnn.quantize(meta[relay.Constant][140] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00696009f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %629 = nn.conv2d(%627, %628, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %630 = qnn.requantize(%629, 0.000484292f /* ty=float32 */, 0 /* ty=int32 */, 6.74937e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %631 = qnn.quantize(meta[relay.Constant][141] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 6.74937e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %632 = add(%630, %631) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %633 = qnn.requantize(%632, 6.74937e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.74937e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %634 = nn.relu(%633) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %635 = qnn.requantize(%634, 6.74937e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0927286f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %636 = qnn.quantize(meta[relay.Constant][142] /* ty=Tensor[(320, 192, 3, 3), float32] */ /* ty=Tensor[(320, 192, 3, 3), float32] */, 0.00493863f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(320, 192, 3, 3), int8] */;
  %637 = nn.conv2d(%635, %636, strides=[2, 2], padding=[0, 0, 0, 0], channels=320, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %638 = qnn.requantize(%637, 0.000457952f /* ty=float32 */, 0 /* ty=int32 */, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %639 = qnn.quantize(meta[relay.Constant][143] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(320, 1, 1), int32] */;
  %640 = add(%638, %639) /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %641 = qnn.requantize(%640, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %642 = nn.relu(%641) /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %643 = qnn.requantize(%642, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %644 = qnn.requantize(%626, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0695813f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 768, 17, 17), int8] */;
  %645 = qnn.quantize(meta[relay.Constant][144] /* ty=Tensor[(192, 768, 1, 1), float32] */ /* ty=Tensor[(192, 768, 1, 1), float32] */, 0.00976139f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 768, 1, 1), int8] */;
  %646 = nn.conv2d(%644, %645, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %647 = qnn.requantize(%646, 0.000679211f /* ty=float32 */, 0 /* ty=int32 */, 6.75832e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %648 = qnn.quantize(meta[relay.Constant][145] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 6.75832e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %649 = add(%647, %648) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %650 = qnn.requantize(%649, 6.75832e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.75832e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %651 = nn.relu(%650) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %652 = qnn.requantize(%651, 6.75832e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0941049f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %653 = qnn.quantize(meta[relay.Constant][146] /* ty=Tensor[(192, 192, 1, 7), float32] */ /* ty=Tensor[(192, 192, 1, 7), float32] */, 0.00384012f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 192, 1, 7), int8] */;
  %654 = nn.conv2d(%652, %653, padding=[0, 3, 0, 3], channels=192, kernel_size=[1, 7], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %655 = qnn.requantize(%654, 0.000361374f /* ty=float32 */, 0 /* ty=int32 */, 6.27892e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %656 = qnn.quantize(meta[relay.Constant][147] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 6.27892e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %657 = add(%655, %656) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %658 = qnn.requantize(%657, 6.27892e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.27892e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %659 = nn.relu(%658) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %660 = qnn.requantize(%659, 6.27892e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.106464f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %661 = qnn.quantize(meta[relay.Constant][148] /* ty=Tensor[(192, 192, 7, 1), float32] */ /* ty=Tensor[(192, 192, 7, 1), float32] */, 0.003142f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 192, 7, 1), int8] */;
  %662 = nn.conv2d(%660, %661, padding=[3, 0, 3, 0], channels=192, kernel_size=[7, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %663 = qnn.requantize(%662, 0.000334511f /* ty=float32 */, 0 /* ty=int32 */, 4.70926e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %664 = qnn.quantize(meta[relay.Constant][149] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 4.70926e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %665 = add(%663, %664) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %666 = qnn.requantize(%665, 4.70926e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.70926e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %667 = nn.relu(%666) /* ty=Tensor[(32, 192, 17, 17), int32] */;
  %668 = qnn.requantize(%667, 4.70926e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0758742f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 192, 17, 17), int8] */;
  %669 = qnn.quantize(meta[relay.Constant][150] /* ty=Tensor[(192, 192, 3, 3), float32] */ /* ty=Tensor[(192, 192, 3, 3), float32] */, 0.00747768f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 192, 3, 3), int8] */;
  %670 = nn.conv2d(%668, %669, strides=[2, 2], padding=[0, 0, 0, 0], channels=192, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %671 = qnn.requantize(%670, 0.000567363f /* ty=float32 */, 0 /* ty=int32 */, 5.06169e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %672 = qnn.quantize(meta[relay.Constant][151] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 5.06169e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %673 = add(%671, %672) /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %674 = qnn.requantize(%673, 5.06169e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.06169e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %675 = nn.relu(%674) /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %676 = qnn.requantize(%675, 5.06169e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %677 = qnn.requantize(%626, 7.19665e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.14737e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 768, 17, 17), int32] */;
  %678 = nn.max_pool2d(%677, pool_size=[3, 3], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %679 = qnn.requantize(%678, 4.14737e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %680 = (%643, %676, %679);
  %681 = concatenate(%680, axis=1) /* ty=Tensor[(32, 1280, 8, 8), int32] */;
  %682 = qnn.requantize(%681, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.129485f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1280, 8, 8), int8] */;
  %683 = qnn.quantize(meta[relay.Constant][152] /* ty=Tensor[(320, 1280, 1, 1), float32] */ /* ty=Tensor[(320, 1280, 1, 1), float32] */, 0.0073916f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(320, 1280, 1, 1), int8] */;
  %684 = nn.conv2d(%682, %683, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %685 = qnn.requantize(%684, 0.000957103f /* ty=float32 */, 0 /* ty=int32 */, 4.63508e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %686 = qnn.quantize(meta[relay.Constant][153] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */, 4.63508e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(320, 1, 1), int32] */;
  %687 = add(%685, %686) /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %688 = qnn.requantize(%687, 4.63508e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.63508e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %689 = nn.relu(%688) /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %690 = qnn.requantize(%689, 4.63508e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %691 = qnn.requantize(%681, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.129485f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1280, 8, 8), int8] */;
  %692 = qnn.quantize(meta[relay.Constant][154] /* ty=Tensor[(384, 1280, 1, 1), float32] */ /* ty=Tensor[(384, 1280, 1, 1), float32] */, 0.00610927f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 1280, 1, 1), int8] */;
  %693 = nn.conv2d(%691, %692, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %694 = qnn.requantize(%693, 0.00079106f /* ty=float32 */, 0 /* ty=int32 */, 4.72628e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %695 = qnn.quantize(meta[relay.Constant][155] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 4.72628e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %696 = add(%694, %695) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %697 = qnn.requantize(%696, 4.72628e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.72628e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %698 = nn.relu(%697) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %699 = qnn.requantize(%698, 4.72628e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0767814f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 384, 8, 8), int8] */;
  %700 = qnn.quantize(meta[relay.Constant][156] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, 0.00507655f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 384, 1, 3), int8] */;
  %701 = nn.conv2d(%699, %700, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %702 = qnn.requantize(%701, 0.000389785f /* ty=float32 */, 0 /* ty=int32 */, 4.80139e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %703 = qnn.quantize(meta[relay.Constant][157] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 4.80139e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %704 = add(%702, %703) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %705 = qnn.requantize(%704, 4.80139e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.80139e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %706 = nn.relu(%705) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %707 = qnn.requantize(%706, 4.80139e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %708 = qnn.requantize(%698, 4.72628e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0767814f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 384, 8, 8), int8] */;
  %709 = qnn.quantize(meta[relay.Constant][158] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, 0.00815033f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 384, 3, 1), int8] */;
  %710 = nn.conv2d(%708, %709, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %711 = qnn.requantize(%710, 0.000625794f /* ty=float32 */, 0 /* ty=int32 */, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %712 = qnn.quantize(meta[relay.Constant][159] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %713 = add(%711, %712) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %714 = qnn.requantize(%713, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %715 = nn.relu(%714) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %716 = qnn.requantize(%715, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %717 = (%707, %716);
  %718 = concatenate(%717, axis=1) /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %719 = qnn.requantize(%718, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %720 = qnn.requantize(%681, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.129485f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1280, 8, 8), int8] */;
  %721 = qnn.quantize(meta[relay.Constant][160] /* ty=Tensor[(448, 1280, 1, 1), float32] */ /* ty=Tensor[(448, 1280, 1, 1), float32] */, 0.00571918f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(448, 1280, 1, 1), int8] */;
  %722 = nn.conv2d(%720, %721, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %723 = qnn.requantize(%722, 0.000740549f /* ty=float32 */, 0 /* ty=int32 */, 4.25375e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %724 = qnn.quantize(meta[relay.Constant][161] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */, 4.25375e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(448, 1, 1), int32] */;
  %725 = add(%723, %724) /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %726 = qnn.requantize(%725, 4.25375e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.25375e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %727 = nn.relu(%726) /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %728 = qnn.requantize(%727, 4.25375e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0580821f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 448, 8, 8), int8] */;
  %729 = qnn.quantize(meta[relay.Constant][162] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, 0.00254263f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 448, 3, 3), int8] */;
  %730 = nn.conv2d(%728, %729, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %731 = qnn.requantize(%730, 0.000147681f /* ty=float32 */, 0 /* ty=int32 */, 3.88767e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %732 = qnn.quantize(meta[relay.Constant][163] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 3.88767e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %733 = add(%731, %732) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %734 = qnn.requantize(%733, 3.88767e-09f /* ty=float32 */, 0 /* ty=int32 */, 3.88767e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %735 = nn.relu(%734) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %736 = qnn.requantize(%735, 3.88767e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0542847f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 384, 8, 8), int8] */;
  %737 = qnn.quantize(meta[relay.Constant][164] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, 0.00525039f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 384, 1, 3), int8] */;
  %738 = nn.conv2d(%736, %737, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %739 = qnn.requantize(%738, 0.000285016f /* ty=float32 */, 0 /* ty=int32 */, 4.25846e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %740 = qnn.quantize(meta[relay.Constant][165] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 4.25846e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %741 = add(%739, %740) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %742 = qnn.requantize(%741, 4.25846e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.25846e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %743 = nn.relu(%742) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %744 = qnn.requantize(%743, 4.25846e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.41786e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %745 = qnn.requantize(%735, 3.88767e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0542847f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 384, 8, 8), int8] */;
  %746 = qnn.quantize(meta[relay.Constant][166] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, 0.00360351f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 384, 3, 1), int8] */;
  %747 = nn.conv2d(%745, %746, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %748 = qnn.requantize(%747, 0.000195615f /* ty=float32 */, 0 /* ty=int32 */, 4.41786e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %749 = qnn.quantize(meta[relay.Constant][167] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 4.41786e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %750 = add(%748, %749) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %751 = qnn.requantize(%750, 4.41786e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.41786e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %752 = nn.relu(%751) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %753 = qnn.requantize(%752, 4.41786e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.41786e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %754 = (%744, %753);
  %755 = concatenate(%754, axis=1) /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %756 = qnn.requantize(%755, 4.41786e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %757 = qnn.dequantize(%681, 7.76225e-09f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %758 = nn.avg_pool2d(%757, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 1280, 8, 8), float32] */;
  %759 = qnn.quantize(%758, 0.0628236f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 1280, 8, 8), int8] */;
  %760 = qnn.quantize(meta[relay.Constant][168] /* ty=Tensor[(192, 1280, 1, 1), float32] */ /* ty=Tensor[(192, 1280, 1, 1), float32] */, 0.00902899f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 1280, 1, 1), int8] */;
  %761 = nn.conv2d(%759, %760, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %762 = qnn.requantize(%761, 0.000567233f /* ty=float32 */, 0 /* ty=int32 */, 3.7089e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %763 = qnn.quantize(meta[relay.Constant][169] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 3.7089e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %764 = add(%762, %763) /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %765 = qnn.requantize(%764, 3.7089e-09f /* ty=float32 */, 0 /* ty=int32 */, 3.7089e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %766 = nn.relu(%765) /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %767 = qnn.requantize(%766, 3.7089e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %768 = (%690, %719, %756, %767);
  %769 = concatenate(%768, axis=1) /* ty=Tensor[(32, 2048, 8, 8), int32] */;
  %770 = qnn.requantize(%769, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.1469f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 2048, 8, 8), int8] */;
  %771 = qnn.quantize(meta[relay.Constant][170] /* ty=Tensor[(320, 2048, 1, 1), float32] */ /* ty=Tensor[(320, 2048, 1, 1), float32] */, 0.0190236f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(320, 2048, 1, 1), int8] */;
  %772 = nn.conv2d(%770, %771, padding=[0, 0, 0, 0], channels=320, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %773 = qnn.requantize(%772, 0.00279457f /* ty=float32 */, 0 /* ty=int32 */, 6.0167e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %774 = qnn.quantize(meta[relay.Constant][171] /* ty=Tensor[(320, 1, 1), float32] */ /* ty=Tensor[(320, 1, 1), float32] */, 6.0167e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(320, 1, 1), int32] */;
  %775 = add(%773, %774) /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %776 = qnn.requantize(%775, 6.0167e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.0167e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %777 = nn.relu(%776) /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %778 = qnn.requantize(%777, 6.0167e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 320, 8, 8), int32] */;
  %779 = qnn.requantize(%769, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.1469f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 2048, 8, 8), int8] */;
  %780 = qnn.quantize(meta[relay.Constant][172] /* ty=Tensor[(384, 2048, 1, 1), float32] */ /* ty=Tensor[(384, 2048, 1, 1), float32] */, 0.00848985f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 2048, 1, 1), int8] */;
  %781 = nn.conv2d(%779, %780, padding=[0, 0, 0, 0], channels=384, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %782 = qnn.requantize(%781, 0.00124716f /* ty=float32 */, 0 /* ty=int32 */, 7.30744e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %783 = qnn.quantize(meta[relay.Constant][173] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 7.30744e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %784 = add(%782, %783) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %785 = qnn.requantize(%784, 7.30744e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.30744e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %786 = nn.relu(%785) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %787 = qnn.requantize(%786, 7.30744e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.121899f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 384, 8, 8), int8] */;
  %788 = qnn.quantize(meta[relay.Constant][174] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, 0.0151868f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 384, 1, 3), int8] */;
  %789 = nn.conv2d(%787, %788, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %790 = qnn.requantize(%789, 0.00185125f /* ty=float32 */, 0 /* ty=int32 */, 1.61283e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %791 = qnn.quantize(meta[relay.Constant][175] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1.61283e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %792 = add(%790, %791) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %793 = qnn.requantize(%792, 1.61283e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.61283e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %794 = nn.relu(%793) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %795 = qnn.requantize(%794, 1.61283e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %796 = qnn.requantize(%786, 7.30744e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.121899f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 384, 8, 8), int8] */;
  %797 = qnn.quantize(meta[relay.Constant][176] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, 0.0160716f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 384, 3, 1), int8] */;
  %798 = nn.conv2d(%796, %797, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %799 = qnn.requantize(%798, 0.0019591f /* ty=float32 */, 0 /* ty=int32 */, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %800 = qnn.quantize(meta[relay.Constant][177] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %801 = add(%799, %800) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %802 = qnn.requantize(%801, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %803 = nn.relu(%802) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %804 = qnn.requantize(%803, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %805 = (%795, %804);
  %806 = concatenate(%805, axis=1) /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %807 = qnn.requantize(%806, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %808 = qnn.requantize(%769, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.1469f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 2048, 8, 8), int8] */;
  %809 = qnn.quantize(meta[relay.Constant][178] /* ty=Tensor[(448, 2048, 1, 1), float32] */ /* ty=Tensor[(448, 2048, 1, 1), float32] */, 0.00626862f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(448, 2048, 1, 1), int8] */;
  %810 = nn.conv2d(%808, %809, padding=[0, 0, 0, 0], channels=448, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %811 = qnn.requantize(%810, 0.000920861f /* ty=float32 */, 0 /* ty=int32 */, 5.87778e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %812 = qnn.quantize(meta[relay.Constant][179] /* ty=Tensor[(448, 1, 1), float32] */ /* ty=Tensor[(448, 1, 1), float32] */, 5.87778e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(448, 1, 1), int32] */;
  %813 = add(%811, %812) /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %814 = qnn.requantize(%813, 5.87778e-09f /* ty=float32 */, 0 /* ty=int32 */, 5.87778e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %815 = nn.relu(%814) /* ty=Tensor[(32, 448, 8, 8), int32] */;
  %816 = qnn.requantize(%815, 5.87778e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.0818889f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 448, 8, 8), int8] */;
  %817 = qnn.quantize(meta[relay.Constant][180] /* ty=Tensor[(384, 448, 3, 3), float32] */ /* ty=Tensor[(384, 448, 3, 3), float32] */, 0.00502423f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 448, 3, 3), int8] */;
  %818 = nn.conv2d(%816, %817, padding=[1, 1, 1, 1], channels=384, kernel_size=[3, 3], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %819 = qnn.requantize(%818, 0.000411429f /* ty=float32 */, 0 /* ty=int32 */, 8.56509e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %820 = qnn.quantize(meta[relay.Constant][181] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 8.56509e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %821 = add(%819, %820) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %822 = qnn.requantize(%821, 8.56509e-09f /* ty=float32 */, 0 /* ty=int32 */, 8.56509e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %823 = nn.relu(%822) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %824 = qnn.requantize(%823, 8.56509e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.145289f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 384, 8, 8), int8] */;
  %825 = qnn.quantize(meta[relay.Constant][182] /* ty=Tensor[(384, 384, 1, 3), float32] */ /* ty=Tensor[(384, 384, 1, 3), float32] */, 0.00715138f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 384, 1, 3), int8] */;
  %826 = nn.conv2d(%824, %825, padding=[0, 1, 0, 1], channels=384, kernel_size=[1, 3], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %827 = qnn.requantize(%826, 0.00103902f /* ty=float32 */, 0 /* ty=int32 */, 7.84438e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %828 = qnn.quantize(meta[relay.Constant][183] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 7.84438e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %829 = add(%827, %828) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %830 = qnn.requantize(%829, 7.84438e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.84438e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %831 = nn.relu(%830) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %832 = qnn.requantize(%831, 7.84438e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.84438e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %833 = qnn.requantize(%823, 8.56509e-09f /* ty=float32 */, 0 /* ty=int32 */, 0.145289f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 384, 8, 8), int8] */;
  %834 = qnn.quantize(meta[relay.Constant][184] /* ty=Tensor[(384, 384, 3, 1), float32] */ /* ty=Tensor[(384, 384, 3, 1), float32] */, 0.00734243f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(384, 384, 3, 1), int8] */;
  %835 = nn.conv2d(%833, %834, padding=[1, 0, 1, 0], channels=384, kernel_size=[3, 1], out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %836 = qnn.requantize(%835, 0.00106678f /* ty=float32 */, 0 /* ty=int32 */, 6.81761e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %837 = qnn.quantize(meta[relay.Constant][185] /* ty=Tensor[(384, 1, 1), float32] */ /* ty=Tensor[(384, 1, 1), float32] */, 6.81761e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(384, 1, 1), int32] */;
  %838 = add(%836, %837) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %839 = qnn.requantize(%838, 6.81761e-09f /* ty=float32 */, 0 /* ty=int32 */, 6.81761e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %840 = nn.relu(%839) /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %841 = qnn.requantize(%840, 6.81761e-09f /* ty=float32 */, 0 /* ty=int32 */, 7.84438e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 384, 8, 8), int32] */;
  %842 = (%832, %841);
  %843 = concatenate(%842, axis=1) /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %844 = qnn.requantize(%843, 7.84438e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 768, 8, 8), int32] */;
  %845 = qnn.dequantize(%769, 8.75088e-09f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %846 = nn.avg_pool2d(%845, pool_size=[3, 3], padding=[1, 1, 1, 1], count_include_pad=True) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %847 = qnn.quantize(%846, 0.078022f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(32, 2048, 8, 8), int8] */;
  %848 = qnn.quantize(meta[relay.Constant][186] /* ty=Tensor[(192, 2048, 1, 1), float32] */ /* ty=Tensor[(192, 2048, 1, 1), float32] */, 0.0223402f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int8") /* ty=Tensor[(192, 2048, 1, 1), int8] */;
  %849 = nn.conv2d(%847, %848, padding=[0, 0, 0, 0], channels=192, kernel_size=[1, 1], out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %850 = qnn.requantize(%849, 0.00174302f /* ty=float32 */, 0 /* ty=int32 */, 4.03363e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %851 = qnn.quantize(meta[relay.Constant][187] /* ty=Tensor[(192, 1, 1), float32] */ /* ty=Tensor[(192, 1, 1), float32] */, 4.03363e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(192, 1, 1), int32] */;
  %852 = add(%850, %851) /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %853 = qnn.requantize(%852, 4.03363e-09f /* ty=float32 */, 0 /* ty=int32 */, 4.03363e-09f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %854 = nn.relu(%853) /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %855 = qnn.requantize(%854, 4.03363e-09f /* ty=float32 */, 0 /* ty=int32 */, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */, out_dtype="int32") /* ty=Tensor[(32, 192, 8, 8), int32] */;
  %856 = (%778, %807, %844, %855);
  %857 = concatenate(%856, axis=1) /* ty=Tensor[(32, 2048, 8, 8), int32] */;
  %858 = qnn.dequantize(%857, 1.62613e-08f /* ty=float32 */, 0 /* ty=int32 */) /* ty=Tensor[(32, 2048, 8, 8), float32] */;
  %859 = nn.avg_pool2d(%858, pool_size=[8, 8], strides=[8, 8], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(32, 2048, 1, 1), float32] */;
  %860 = nn.batch_flatten(%859) /* ty=Tensor[(32, 2048), float32] */;
  %861 = nn.dense(%860, meta[relay.Constant][188] /* ty=Tensor[(1000, 2048), float32] */ /* ty=Tensor[(1000, 2048), float32] */, units=1000) /* ty=Tensor[(32, 1000), float32] */;
  add(%861, meta[relay.Constant][189] /* ty=Tensor[(1000), float32] */ /* ty=Tensor[(1000), float32] */) /* ty=Tensor[(32, 1000), float32] */
}
// meta data omitted. you can use show_meta_data=True to include meta data
DEBUG:autotvm:Finish loading 688 records
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op nn.pad
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op divide
INFO:compile_engine:Use implementation injective.cpu for op add
INFO:compile_engine:Use implementation injective.cpu for op round
INFO:compile_engine:Use implementation injective.cpu for op cast
INFO:compile_engine:Use implementation injective.cpu for op clip
INFO:compile_engine:Use implementation injective.cpu for op cast
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 4, 299, 299), 'int8'), ('TENSOR', (32, 4, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 32, 149, 149), 'int8'), ('TENSOR', (32, 32, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 32, 147, 147), 'int8'), ('TENSOR', (64, 32, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 64, 73, 73), 'int8'), ('TENSOR', (80, 64, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 80, 73, 73), 'int8'), ('TENSOR', (192, 80, 3, 3), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 192, 35, 35), 'int8'), ('TENSOR', (64, 192, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 192, 35, 35), 'int8'), ('TENSOR', (48, 192, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 48, 35, 35), 'int8'), ('TENSOR', (64, 48, 5, 5), 'int8'), (1, 1), (2, 2, 2, 2), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 64, 35, 35), 'int8'), ('TENSOR', (96, 64, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 96, 35, 35), 'int8'), ('TENSOR', (96, 96, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 192, 35, 35), 'int8'), ('TENSOR', (32, 192, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 256, 35, 35), 'int8'), ('TENSOR', (64, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 256, 35, 35), 'int8'), ('TENSOR', (48, 256, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 288, 35, 35), 'int8'), ('TENSOR', (64, 288, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 288, 35, 35), 'int8'), ('TENSOR', (48, 288, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 288, 35, 35), 'int8'), ('TENSOR', (384, 288, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 96, 35, 35), 'int8'), ('TENSOR', (96, 96, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 768, 17, 17), 'int8'), ('TENSOR', (192, 768, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 768, 17, 17), 'int8'), ('TENSOR', (128, 768, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 128, 17, 17), 'int8'), ('TENSOR', (128, 128, 1, 7), 'int8'), (1, 1), (0, 3, 0, 3), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 128, 17, 17), 'int8'), ('TENSOR', (192, 128, 7, 1), 'int8'), (1, 1), (3, 0, 3, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 128, 17, 17), 'int8'), ('TENSOR', (128, 128, 7, 1), 'int8'), (1, 1), (3, 0, 3, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 128, 17, 17), 'int8'), ('TENSOR', (192, 128, 1, 7), 'int8'), (1, 1), (0, 3, 0, 3), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 768, 17, 17), 'int8'), ('TENSOR', (160, 768, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 160, 17, 17), 'int8'), ('TENSOR', (160, 160, 1, 7), 'int8'), (1, 1), (0, 3, 0, 3), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 160, 17, 17), 'int8'), ('TENSOR', (192, 160, 7, 1), 'int8'), (1, 1), (3, 0, 3, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 160, 17, 17), 'int8'), ('TENSOR', (160, 160, 7, 1), 'int8'), (1, 1), (3, 0, 3, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 160, 17, 17), 'int8'), ('TENSOR', (192, 160, 1, 7), 'int8'), (1, 1), (0, 3, 0, 3), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 192, 17, 17), 'int8'), ('TENSOR', (192, 192, 1, 7), 'int8'), (1, 1), (0, 3, 0, 3), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 192, 17, 17), 'int8'), ('TENSOR', (192, 192, 7, 1), 'int8'), (1, 1), (3, 0, 3, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 192, 17, 17), 'int8'), ('TENSOR', (320, 192, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 192, 17, 17), 'int8'), ('TENSOR', (192, 192, 3, 3), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 1280, 8, 8), 'int8'), ('TENSOR', (320, 1280, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 1280, 8, 8), 'int8'), ('TENSOR', (384, 1280, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 384, 8, 8), 'int8'), ('TENSOR', (384, 384, 1, 3), 'int8'), (1, 1), (0, 1, 0, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 384, 8, 8), 'int8'), ('TENSOR', (384, 384, 3, 1), 'int8'), (1, 1), (1, 0, 1, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 1280, 8, 8), 'int8'), ('TENSOR', (448, 1280, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 448, 8, 8), 'int8'), ('TENSOR', (384, 448, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 1280, 8, 8), 'int8'), ('TENSOR', (192, 1280, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 2048, 8, 8), 'int8'), ('TENSOR', (320, 2048, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 2048, 8, 8), 'int8'), ('TENSOR', (384, 2048, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 2048, 8, 8), 'int8'), ('TENSOR', (448, 2048, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -thread_warp_size=32, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (32, 2048, 8, 8), 'int8'), ('TENSOR', (192, 2048, 1, 1), 'int8'), (1, 1), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation injective.cpu for op expand_dims
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation concatenate.cuda for op concatenate
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op right_shift
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op right_shift
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.pad
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation pool.cuda for op nn.max_pool2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation conv2d_nchw_int8.cuda for op nn.conv2d
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op nn.relu
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op fixed_point_multiply
INFO:compile_engine:Use implementation pool.cuda for op nn.avg_pool2d
INFO:compile_engine:Use implementation injective.cuda for op divide
INFO:compile_engine:Use implementation injective.cuda for op add
INFO:compile_engine:Use implementation injective.cuda for op round
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op clip
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op subtract
INFO:compile_engine:Use implementation injective.cuda for op cast
INFO:compile_engine:Use implementation injective.cuda for op multiply
learning_based_quantize.py:100: DeprecationWarning: legacy graph runtime behaviour of producing json / lib / params will be removed in the next release 
  graph, lib, params = relay.build(mod, target)
^CTraceback (most recent call last):
  File "learning_based_quantize.py", line 169, in <module>
    acc = test_quantize_acc(config, rec_val)
  File "learning_based_quantize.py", line 150, in test_quantize_acc
    acc = eval_acc(mod, val_data, batch_fn, target='cuda', ctx=tvm.gpu(2))
  File "learning_based_quantize.py", line 117, in eval_acc
    acc_top1.update(label, [mx.nd.array(out_arr.asnumpy())])
  File "/home/woongkyu/hago/tvm/python/tvm/runtime/ndarray.py", line 174, in asnumpy
    check_call(_LIB.TVMArrayCopyToBytes(self.handle, data, nbytes))
KeyboardInterrupt

(tvm-build) ]0;woongkyu@watson: ~/hago/tvm/tests/python/nightly/quantizationwoongkyu@watson:~/hago/tvm/tests/python/nightly/quantization$ exit
exit

Script done on 2021-03-15 21:22:24+0900
